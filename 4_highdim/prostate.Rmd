---
title: "Ridge, Best Subsets and LASSO: Prostate Cancer Data"
output: html_document
---

## Training and test set

```{r, message=FALSE, warning=FALSE}
rm(list=ls())
# load data and divide in train/test
require(ElemStatLearn)
train <- prostate[prostate$train,-10]
test <- prostate[!prostate$train,-10]

X = as.matrix(train[,-9])
y = train$lpsa
n<-nrow(X)
p<-ncol(X)
X.star = as.matrix(test[,-9])
y.star = test$lpsa
m = nrow(X.star)
```

## Correlation matrix

```{r, message=FALSE, warning=FALSE}
library("corrplot")
corrplot(cor(train), method="number")
```

## Full model fit

```{r, message=FALSE, warning=FALSE}
# full model fit
fit.full = lm(lpsa ~ ., train)
summary(fit.full)
# MSE test
mean( (predict(fit.full, newdata=test) - test$lpsa )^2 )
```

## Ridge regression: R package glmnet

```{r, message=FALSE, warning=FALSE}
require(glmnet)

# ridge fit
fit.ridge = glmnet(X,y,family="gaussian",alpha=0)
# solution path
plot(fit.ridge, xvar="lambda")

# K-fold cross validation
K <- n
ridge.cv<-cv.glmnet(X,y,alpha=0, nfolds = K, grouped=FALSE)
plot(ridge.cv)

# lambda selected by K fold cross validation
hatlambda <-ridge.cv$lambda.min
hatlambda

predict(fit.ridge, s=hatlambda, type ="coefficients")
plot(fit.ridge, xvar="lambda")
abline(v=log(hatlambda))

# predicted values
yhat.ridge = predict(fit.ridge, s=hatlambda, newx=X.star, exact=T)

# MSE test
mean( (yhat.ridge - test$lpsa)^2 )
```


## Ridge regression: R package penalized

```{r, message=FALSE, warning=FALSE}
library(penalized)

# plot cross-validated likelihood
CVlik = profL2(y, penalized=X, minlambda2= 0.08, maxlambda2 = 800, plot=T, trace=F, fold=1:n, standardize = T)

# estimated penalty parameter
hatlambda = optL2(y,penalized=X, standardize = T, trace=F)$lambda
hatlambda

# ridge fit
fit.ridge = penalized(y,penalized=X,lambda2=hatlambda, standardize = T, trace=F)
yhat.ridge = predict(fit.ridge, penalized=X.star)[,1]
mean( (yhat.ridge - test$lpsa)^2 )
```



## Best Subsets Selection: information criteria

```{r, message=FALSE, warning=FALSE}
require(leaps)

fit.bests <- regsubsets(lpsa~.,train, nbest=1, nvmax=(p-1))
summary.bests<-summary(fit.bests)
summary.bests
names(summary.bests)

# function predict for regsubsets
predict.regsubsets =function(object ,newdata ,id ,...){
 form=as.formula(object$call[[2]])
 mat=model.matrix(form, newdata)
 coefi =coef(object, id=id)
 xvars =names(coefi)
 mat[,xvars]%*%coefi
}

# best Cp
plot(summary.bests$cp, xlab="k", ylab="log Cp", type="b", log="y" )
kbest=which(summary.bests$cp==min(summary.bests$cp))
coef(fit.bests,kbest)

yhat.bestCp = predict.regsubsets(fit.bests, newdata=test, id=7)
mean( (yhat.bestCp - test$lpsa)^2 )


# best BIC
plot(fit.bests, scale="bic")

yhat.bestBIC = predict.regsubsets(fit.bests, newdata=test, id=2)
mean( (yhat.bestBIC - test$lpsa)^2 )


# Forward with AIC stopping rule

# null model fit
fit.null = lm(lpsa ~ 1, train)

fit.fwdAIC = step(fit.null, scope=list(upper=fit.full), direction="forward", k=2, trace=0)
summary(fit.fwdAIC)

# AIC penalty = n*log(RSS/n)+2*p
n*log(deviance(fit.fwdAIC)/n)+2*ncol(model.matrix(fit.fwdAIC))

yhat.fwdAIC = predict(fit.fwdAIC, newdata=test)
mean( (yhat.fwdAIC - test$lpsa)^2 )
```

## Best Subsets Selection: cross-validation

```{r, message=FALSE, warning=FALSE}
set.seed(123)
K = 5
folds = sample(1:K, n, replace =TRUE)
KCV = matrix(NA, K, p)
for (k in 1:K){
fit_k = regsubsets(lpsa ~.,data=train[folds!=k,])
for (j in 1:p){
yhat_k=predict(fit_k, train[folds==k,], id=j)
KCV[k,j]=mean( (train$lpsa[folds==k]-yhat_k)^2 )
}
}
plot(1:p,apply(KCV,2,mean), type="b")
```

## Lasso

```{r, message=FALSE, warning=FALSE}

# lasso fit
fit.lasso <- glmnet(X, y, alpha=1)
plot(fit.lasso, xvar="lambda")

# K fold cross validation
set.seed(123)
K<-5
cv.lasso <-cv.glmnet(X, y, alpha=1, nfolds = K)
plot(cv.lasso)

# lambda selected by K fold cross validation with the one-standard error rule
hatlambda<-cv.lasso$lambda.1se
hatlambda

predict(fit.lasso ,s=hatlambda, type ="coefficients")

plot(fit.lasso, xvar="lambda")
abline(v=log(hatlambda))

# predicted values
yhat.lasso = predict(fit.lasso, s=hatlambda, newx=X.star, exact=T)

# MSE test
mean( (yhat.lasso - test$lpsa)^2 )
```

