---
title: 'Ridge Regression: Prostate Cancer Data'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data

```{r,  warning=FALSE, message=FALSE}
rm(list=ls())
# training and test data
require(ElemStatLearn)
train <- prostate[prostate$train,-10]
test <- prostate[!prostate$train,-10]
# n.ro of observations
n <- nrow(train)
m <- nrow(test)

# design matrix (without the intercept)
X<-as.matrix(
  scale(train[,-9]) # standardized 
  )
X.star <- as.matrix(
  scale(test[,-9]) # standardized 
  )
p = ncol(X)
# response
y = scale(train[,"lpsa"]) # standardized
y.star = scale(test[,"lpsa"]) # standardized
```

## Exercize

The training and test data are 
$$\underset{n\times 1}{\mathbf{y}}, \underset{n\times p}{\mathbf{X}}, \quad \underset{m\times 1}{\mathbf{y}^*}, \underset{m\times p}{\mathbf{X}^*}$$
with $n=67$, $m=30$ and $p=8$ for the Prostate Cancer data set, where the response and each predictor of the design matrix are standardized to have mean 0 and variance 1, separately for the training and test set. 

The response variable is \texttt{lpsa}, and the predictor variables are \texttt{lcavol, lweight, age, lbph, svi, lcp, gleason, pgg45}. 

1. Compute the rigde estimate
$$\hat{\mathbf{\beta}}(\lambda) = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_{p\times p})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}$$
for $\lambda=10$.

2. For $\lambda = 0, 1, 2,\ldots ,9,10$, plot the solution path of the estimated coeffients $\hat{\beta}_1(\lambda),\ldots,\hat{\beta}_p(\lambda)$ and the corresponding variances $\mathbb{V}\mathrm{ar}(\hat{\beta}_1(\lambda)), \ldots, \mathbb{V}\mathrm{ar}(\hat{\beta}_p(\lambda))$, which are the elements in the diagonal of the variance/covariance matrix
$$\mathbb{V}\mathrm{ar}(\hat{\mathbf{\beta}}(\lambda) ) = \sigma^2 \mathbf{W}_\lambda (\mathbf{X}^\mathsf{T}\mathbf{X} )^{-1}\mathbf{W}^\mathsf{T}_\lambda$$ where $\mathbf{W}_\lambda = [\mathbf{I}_{p\times p} + \lambda(\mathbf{Z}^\mathsf{T}\mathbf{X})^{-1}]^{-1}$ and is set to $\sigma^2=1$.

3. Compare the LOOCV estimate and the test MSE for the linear model and the ridge regression model with $\lambda=5$. 

\newpage
 
```{r, echo=FALSE}
#1.
lambda <- 10
hatbeta <- solve(t(X) %*% X + lambda*diag(p)) %*% t(X) %*% y
print(hatbeta)

#2.
lambdas = 0:10
nlambda = length(lambdas) 
hatbetas <- matrix(NA,ncol=p, nrow=nlambda)
Vars <- matrix(NA,ncol=p, nrow=nlambda)
for (l in 1:nlambda){
  lambda = lambdas[l]
  hatbetas[l,] = solve(t(X) %*% X + lambda*diag(p)) %*% t(X) %*% y
  W = solve(diag(p) + lambda*solve(t(X) %*% X))
  Vars[l,] = diag( W%*% solve(t(X) %*% X) %*% t(W)  )
}
matplot(lambdas, hatbetas, type="l")
matplot(lambdas, Vars, type="l")

#3. 
lambda = 5
LOOCV <- vector()
for (i in 1:n){
  X_i <- X[-i,]
  hatbeta_i = solve(t(X_i) %*% X_i + lambda*diag(p)) %*% t(X_i) %*% y[-i] 
  LOOCV[i] = ( X[i,] %*% hatbeta_i - y[i])^2
}
cat("LOOCV ridge: ", mean(LOOCV))
hatbeta <- solve(t(X) %*% X + lambda*diag(p)) %*% t(X) %*% y
yhat.ridge = X.star %*% hatbeta
cat("MSE ridge: ", mean((yhat.ridge - y.star)^2))

library(boot)
train.std = data.frame(y,X)
fit.lm = glm(y ~ 0 + ., train.std, family = gaussian) 
cat("LOOCV lm: ", cv.glm(train.std, fit.lm)$delta[1])
yhat.lm = predict(fit.lm, newdata=data.frame(X.star))
cat("MSE lm: ", mean((yhat.lm - y.star)^2))
```
