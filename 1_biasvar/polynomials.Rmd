---
title: "The Bias-Variance Trade-Off"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/aldosolari/Desktop/DM_off/1_biasvar/code")
```

## Polynomial regression fit

```{r, message=FALSE, warning=FALSE}
# import data
load("poly.Rdata")

# plot training data
plot( y ~ x , train)

# 2nd-degree polynomial regression fit
fit <- lm( y ~ poly(x, degree=2), train)
yhat <- predict(fit, newdata=test)
lines( yhat ~ x, train)

# MSE.tr
MSE.tr <- mean( (train$y - yhat)^2 )
```

## Training MSE

```{r, message=FALSE, warning=FALSE}
n <- nrow(train)
ds = 0:20

fun <- function(d) if (d==0) lm(y~1, train) else lm(y~poly(x,degree=d), train)
fits <- sapply(ds, fun)

MSEs.tr <- unlist( lapply(fits, deviance) )/n
plot(ds, MSEs.tr, type="b")
```

## Test MSE

```{r, message=FALSE, warning=FALSE}
yhats <- lapply(fits, predict)
MSEs.te <- unlist( lapply(yhats, 
           function(yhat) mean((test$y - yhat)^2)) )

plot(ds, MSEs.tr, type="b", col=4)
lines(ds, MSEs.te, type="b", col=2)
```

## Overfitting

```{r, message=FALSE, warning=FALSE}
fit <- lm( y ~ poly(x, degree=14), train)
yhat <- predict(fit, newdata=test)
op <- par(mfrow = c(1, 2))
plot( y ~ x , train, col=4)
lines(yhat ~ x, train)
plot( y ~ x , test, col=2)
lines(yhat ~ x, test)
par(op)
```

## True regression function

```{r, message=FALSE, warning=FALSE}
plot(y~x, truef, type="l", col=3)
points(y~x,train, col=4)
points(y~x,test, col=2)
```

## AIC and BIC

```{r, message=FALSE, warning=FALSE}
AICs <- unlist( lapply(fits, AIC) )
BICs <- unlist( lapply(fits, BIC) )

plot(ds, AICs, type="b", col=5)
lines(ds, BICs, type="b", col=6)
```



## K-fold CV

By hand

```{r}
d = 5

# how many folds
K<-5
set.seed(123)
# create folds
folds <- sample( rep(1:K,length=n) )
# initialize vector
KfoldCV <- vector()

for (k in 1:K){
  fit <- lm(y~poly(x,degree=d), train, subset=which(folds!=k))
  x.out <- train$x[which(folds==k)]
  yhat <- predict(fit, newdata=list(x=x.out))
  y.out <- train$y[which(folds==k)]
  KfoldCV[k]<- mean( ( y.out - yhat )^2 )
}

# KfoldCV estimate of EMSE.te
mean(KfoldCV)
```

By function

```{r, message=FALSE, warning=FALSE}
library(boot)

set.seed(123)
KCV = sapply(1:10, function(d) 
     cv.glm(train, glm(y~poly(x,degree=d), 
     train, family = gaussian), K=5 )$delta[1] )

plot(1:10, KCV, type="b", log="y")
```

## LOOCV and GCV

By hand

```{r}
d = 5
# initialize vector
oneout <- vector()

for (i in 1:n){
fit_i <- lm( y~poly(x,degree=d), data=train[-i,])
yhat_i <- predict(fit_i, newdata=data.frame(x=train$x[i]) )
oneout[i] <- ( train$y[i] -  yhat_i )^2
}

# LOOCV estimate of MSE.te
mean(oneout)
```

By matrix

```{r}
d = 5
fit <-  lm(y~poly(x,degree=d), train)

X <- model.matrix(fit)
H <- X %*% solve(t(X)%*% X) %*% t(X)

LOOCV = mean(
   ( (train$y - predict(fit)) / (1-diag(H))  )^2 
) 
LOOCV
```


By function

```{r, message=FALSE, warning=FALSE}
library(boot)

LOOCV = sapply(1:10, function(d) 
     cv.glm(train, glm(y~poly(x,degree=d), 
     train, family = gaussian) )$delta[1] )

GCV = MSEs.tr/(1-(ds+1)/n )^2

plot(1:10, LOOCV, type="b", log="y", col=5)
lines(ds, GCV, col=6)
```