---
title: "Bagging and Random Forests: spam data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/aldosolari/Desktop/DM_off/3_ensemble/code")
```


## Import data

```{r}
#Load the data and split into training and test sets.
rm(list=ls())
spam <- read.csv("SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test = spam[spam$testid, -2]
n = nrow(train)
m = nrow(test)
```

## Model's formula

```{r}
# Use all the predictors 
preds <- setdiff(colnames(train),"spam")
fml <- as.formula(paste("spam",
                  paste(preds,collapse=' + '),sep=' ~ '))
fml
```

## Accuracy measures

```{r}
# A function to calculate and return various measures on the model: prediction accuracy and specificity (the proportion of correctly predicted emails to all true emails)
score <- function(pred, truth, name="model") {
  ctable <- table(truth=truth,
                  pred=(pred>0.5))
  accuracy <- sum(diag(ctable))/sum(ctable)
  specificity <- ctable[1,1]/sum(ctable[1,])
  data.frame(model=name, accuracy=accuracy, specificity=specificity)
}
```

## Fit a classification tree

```{r}
# Load the rpart library and fit a classification tree model
library(rpart)
fit.tree <- rpart(fml, train)
library(rpart.plot)
rpart.plot(fit.tree)

#Confusion matrix
phat = predict(fit.tree, newdata=test)[,"spam"]
yhat = ifelse(phat>.5,"spam","email")
table(pred=yhat, truth=test$spam)

#Evaluate the bagged classification trees in test set
score(predict(fit.tree, newdata=test)[,"spam"], test$spam, name="tree, test")
```

## Bagging classification trees

```{r}
# Use B bootstrap samples of the training set
set.seed(123)
B <- 50

samples <- sapply(1:B,
                  FUN = function(iter)
                  {sample(1:n, size=n, replace=T)})

# Train the individual classification trees and return them in a list.
# Note: this step can take a few minutes.
treelist <-lapply(1:B,
                  FUN=function(iter)
                  {samp <- samples[,iter];
                  rpart(fml, train[samp,])})

# predict.bag assumes the underlying classifier returns decision probabilities, not decisions
predict.bag <- function(treelist, newdata) {
  preds <- sapply(1:length(treelist),
                  FUN=function(iter) {
                  predict(treelist[[iter]], newdata=newdata)[,"spam"]})
  predsums <- rowSums(preds)
  predsums/length(treelist)
}

#Evaluate the bagged classification trees in the test set
score(predict.bag(treelist, newdata=test),
                 test$spam,
                 name="bagging, test")

```

## Random forests

```{r}
library(randomForest)
set.seed(123)

p = ncol(test)
fit.rf <- randomForest(fml,
          data = train,
          ntree=B,
          mtry = sqrt(p),
          importance=T)

# out-of-bag estimate of error
plot(fit.rf)
# variable importance 
importance(fit.rf)[1:5,]
# variable importance plot
varImpPlot(fit.rf, type=1)

score(predict(fit.rf,
newdata=test[,preds],type='prob')[,"spam"],
test$spam, name="random forest, test")
```

