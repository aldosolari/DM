---
title: "**Data Mining**"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---

Teacher: [Aldo Solari](https://aldosolari.github.io/)

This course aims to provide statistical and computational tools for *data mining* and *supervised learning* by using the R software environment for statistical computing. Special emphasis is given to applied predictive modelling. 

The goal of *predictive modeling* is to construct models that generate accurate predictions for future, yet-to-be-seen data. The process of developing an effective model is both iterative and heuristic. This framework includes exploratory data analysis, splitting the data into training and testing sets, building models, selecting an approach for identifying optimal tuning parameters, and estimating predictive performance.

Participating in predictive modelling *competitions* can help you gain practical experience and  improve your data modelling skills in various domains such as credit, insurance, marketing, sales' forecasting etc.  At the same time you get to do it in a competitive context against dozens of participants where each one tries to build the most predictive algorithm. Since 2015, this course utilises [BeeViva](http://www.bee-viva.com/competitions), one of the first Italian public data platforms for hosting [#Competitions](#competitions). 

See the **[course syllabus](https://aldosolari.github.io/DM/syllabus/syllabus.html)** for more information. 

| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 12:30-15:30 |
| Thursday | Lab713 | 13:30-15:30|
| Friday | Lab713 | 12:30-15:30|

# **Latest announcements**

* October 29: Suggested reading: Clark (2018) [Introduction to Machine Learning]( https://m-clark.github.io/introduction-to-machine-learning/)

# **Course Material**

Most of the course material can be found in the notes linked to below. The notes are supplemented by readings which are listed in the [#Reading](#reading) section.

| N | Date |  Lecture    | Notes | Slides|
|--|--|--------------|--------|-------|
| 1 | October 1 | Introduction | [Polynomials](docs/notes/1_Polynomials/1_Polynomials.html) | [Slides](docs/slides/1_Intro/1_Intro.html) |
| 2 | October 4 | Bias-Variance Decomposition | [BiasVar](docs/notes/2_BiasVar/2_BiasVar.html) | [Slides](docs/slides/2_BiasVar/2_BiasVar_slides.html) |
| 3 | October 5  | Optimism, IC and CV | [ICCV](docs/notes/3_ICCV/3_ICCV.html) | [Slides](docs/slides/3_ICCV/3_ICCV_slides.html) |
|  |  |  |  | | |
| 4 | October 8 | Non-parametric regression  | [NPR](docs/notes/4_NPR/4_NPR.html) | [Slides](docs/slides/4_NPR/4_NPR_slides.html) |
| 5 | October 11| Random-X prediction error | [RPE](docs/notes/5_RPE/5_RPE.html)  | [Slides](docs/slides/5_RPE/5_RPE_slides.html) |
| 6 | October 12 | EDA and the modelling process | - | [Slides](docs/slides/6_EDA/6_EDA_slides.html) |
|  |  |  |  | | |
| 7 | October 15 | Titanic data | -  | [Slides](docs/slides/7_Titanic/7_Titanic_slides.html)  |
| 8 | October 18 | Bagging | [BAG](docs/notes/8_BAG/8_BAG.html) | [Slides](docs/slides/8_BAG/8_BAG_slides.html) |
| 9 | October 19 | Random Forests | - | [Slides](docs/slides/9_RF/9_RF_slides.html) |
|  |  |  |  | | |
| 10 | October 22 | Spam and Boston data | - | - |
| 11 | October 25 | Class imbalance | - | [Slides](docs/slides/11_CImb/11_CImb_slides.html) | 
| 12 | October 26 | Ridge regression | - | [Slides](docs/slides/12_Ridge/12_Ridge_slides.html) |
|  |  |  |  | | |
| 13 | October 29 | Lasso and best subset selection | - | [Slides](docs/slides/13_Lasso/13_Lasso_slides.html)  |
|  |  |  |  | | |
| 14 | November 12 |  | |
| 15 | November 15 |  | |
|  |  |  |  | | |
| 16 | November 16 | | |

The lecture notes and course webpage were done with [R Markdown](https://rmarkdown.rstudio.com/).
All of the course material is on the [github repo](https://github.com/aldosolari/DM) including the [.Rmd files](https://github.com/aldosolari/DM/tree/master/docs/notes) for the [notes](https://github.com/aldosolari/DM/tree/master/docs/notes). The course material is in the lecture notes and reading. The [slides](https://github.com/aldosolari/DM/tree/master/docs/slides) are visual aids for the lectures.

 

__Download the .Rmd files, modify as you like and decide the output format (PDF, HTML or Word)__.

To extract the R code from a .Rmd file use
```{r, eval=F}
knitr::purl("../filename.Rmd")
```


We will use R Markdown quite a bit in the class, and its use is __required__ for the Test (in lab) where the Programming exercises must be solved with R Markdown by generating an HTML file containing the answers, which will be uploaded to the "CONSEGNA" folder.

You can read more about R Markdown in [Chapter 27 of r4ds](http://r4ds.had.co.nz/r-markdown.html#r-markdown-basics). This [document](https://m-clark.github.io/Introduction-to-Rmarkdown/) may be helpful to get started with R Markdown.
R Markdown is an example of [literate programming](https://en.wikipedia.org/wiki/Literate_programming), a concept introduced by Donald Knuth saying _you should write code that communicates primarily to humans, not computers_.


# **Reading**

Readings should be complete by the following class. There are three primary references:

- [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) (ISLR)
- [Analisi dei dati e data mining](http://azzalini.stat.unipd.it/Book-DM/index.html) (AS)
- [R for Data Science](http://r4ds.had.co.nz/) (r4ds)

As a supplement to the textbook ISLR, you may also want to watch the excellent course lecture videos (linked below)

- [Video](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)

With regard to the exercises at the end of each chapter of ISLR, there are solutions provided by students you can use to check your work

- [Solutions](https://blog.princehonest.com/stat-learning/)

**October 1** Introduction to the course

- ISLR 1, 2.1, 2.2.1, 7.1 
- AS 3.1, 3.2
- [Breiman (2001)](http://www2.math.uu.se/~thulin/mm/breiman.pdf), [Carmichael and Marron (2018)](https://arxiv.org/pdf/1801.00371.pdf) (optional)


**October 4** Bias-variance decomposition

- ISLR  2.2.2, 6.1.3
- AS 3.3, 3.5.3

**October 5** Optimism, information criteria and cross-validation

- ISLR 5.1.1-5.1.4
- AS  3.5.1, 3.5.2

**October 8** Nonparametric regression

- the [caret](http://topepo.github.io/caret/index.html) package

**October 11** Random-X prediction error

- [Rosset and Tibshirani (2018) From Fixed-X to Random-X Regression](https://arxiv.org/pdf/1704.08160.pdf) (optional)

**October 12** EDA and the modeling process

- AS 2
- r4ds 1, 2, 3

**October 12** The Titanic dataset

- ISLR 2.2.3, 5.1.5

**October 18** Bagging

- ISLR 8.2.1
- AS 5.8.1

**October 19** Random Forests

- ISLR 8.2.2
- AS 5.8.3

**October 22** Spam and Boston data

- [Ehrlinger (2015) ggRandomForests: Visually Exploring a Random Forest for Regression](https://arxiv.org/abs/1501.07196) (optional)

**October 25** Class imbalance

**October 26** Ridge regression

- ISLR 6.2.1, 6.4
- AS 3.6.3

**October 29** Lasso and best subset selection

- ISLR 6.1, 6.2.2, 6.2.3, 6.5.3



# **Competitions**

The [competition platform](http://www.bee-viva.com/competitions) is provided by BeeViva s.r.l. (University of Padua spin-off). I would like to thank BeeViva and in particular [Livio Finos](http://www.liviofinos.net/), [Dario Solari](https://www.linkedin.com/in/dariosolari/?originalSubdomain=it) and [Tomaso "Minni" Minelli](https://github.com/minni). The competitions offered in this course would not have been possible without their constant and precious support.


If you click on a specific Competition in the listing, you will go to the Competition's homepage. The following information is provided in the tabs:

1. *Description* gives a description of the problem
2. *Data* is where you can download the data splitted into training set and test set
3. *Evaluation* describes how to format your submission file and how your submissions will be evaluated. Each competition employs a metric (RMSE, Accuracy, etc.) that serves as the objective measure for how competitors are ranked on the leaderboard.
4. *Timeline* for joining the competition: an entry deadline and a submission deadline (after which no new submissions will be accepted). After the deadline passes, partecipants get a final score.
5. *Leaderboard*. Submissions can be made through through manual upload in the competition platform. Submissions are scored immediately and summarized on a live leaderboard.


| Competition | 1st: From-To | 2nd: From-To | 3rd: From-To | 
|----|----|----|----|
| [Houses prices](docs/competitions/Houseprices/Houseprices.html) | 8/10/18 - 16/11/18 | ?/1/19 - ?/1/19 | ?/6/19 - ?/6/19  |
| [OkCupid](docs/competitions/OkCupid/OkCupid.html)  | 16/10/18 - 16/11/18 | ?/1/19 - ?/1/19 | ?/6/19 - ?/6/19  |
| [Bikesharing](docs/competitions/Bikesharing/Bikesharing.html) | 16/10/18 - 16/11/18 | ?/1/19 - ?/1/19  | ?/6/19 - ?/6/19  |

- [List of participants (1st round)](docs/competitions/Participants.html)

There are two references that use the `caret` R package that you can use for improving your predictive modeling skills:

- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) (APM)
- [Feature Engineering and Selection](http://www.feat.engineering/) (FEV)


All students must follow a __code of conduct__, which covers issues such as plagiarism, falsification, unauthorized assistance, cheating, and other grievous acts of academic dishonesty. Violations of the code of conduct will not be taken lightly. For the competitions,

* you are encouraged to work in a team. The competition score will be adjusted based on __peer ratings__ (see [peer ratings in coorperative learning teams](http://www4.ncsu.edu/unity/lockers/users/f/felder/public/Papers/kaufman-asee.PDF)). This multiplier will range from 1.05 (for people who go above and beyond) down to 0 (for people who donâ€™t participate). As a last resort you may fire a team member who refuses to participate. Please contact the instructor well before it comes to this.

* you must __publish on-line__ the code used to get the final prediction.

* you can join the competitions __only once__ per academic year. The score will expire at the end of the academic year

* you are encouraged to use online resources (unseen models, other people analyses, etc.) as long as you __cite__ a resource you borrow code from. Full copy/paste from other people code will have a multiplier of 0  

* you are expected to be able to __explain your code by your self__ if required by the teacher


**ACKNOWLEDGEMENTS**

Most of the ideas and materials for this course were collected from several sources. I'll make a bibliography at the earliest. 

This page was last updated on `r Sys.time()` Eastern Time.

