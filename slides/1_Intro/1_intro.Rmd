---
title: "Introduction to the course"
author: Aldo Solari
output:
  xaringan::moon_reader:
    css: ["mtheme_max.css", "fonts_mtheme_max.css"]  
    self_contained: false
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
# This is good for getting the ggplot background consistent with
# the html background color
library(ggplot2)
thm <- theme_bw() + 
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA), 
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

# Outline

* Course overview
* The two cultures
* Data Science
* Your turn


---
layout: false
class: inverse, middle, center

# Course overview

---

# Teacher

Aldo Solari

**E-mail** aldo.solari@unimib.it

**Web page** https://aldosolari.github.io/

**Office hours** Wednesday, 17:00-18:00, room 2030, building U7, II floor

**Course page** https://aldosolari.github.io/DM/


| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 12:30-15:30 |
| Thursday | Lab713 | 13:30-15:30|
| Friday | Lab713 | 12:30-15:30|

.center[**Time table**]

---

# The course


* This course aims to provide statistical and computational tools for __data mining__ and __supervised learning__ by using the R software environment for statistical computing

* In particular, you will learn how to build __predictive/machine learning models__ that generate accurate predictions for future, yet-to-be-seen data

* This framework includes 
    - data visualization
    - exploratory data analysis
    - feature engineering
    - splitting the data into training and testing sets
    - building models
    - selecting an approach for identifying optimal tuning parameters
    - estimating predictive performance

---

# Competitions

* Participating in predictive modelling __competitions__ can help you gain practical experience and improve your data modelling skills in various domains such as credit, insurance, marketing, sales' forecasting etc. 

* At the same time you get to do it in a competitive context against dozens of participants where each one tries to build the most predictive algorithm

* Inspired by [kaggle](https://www.kaggle.com/), the world's largest community of data scientists and machine learners

* Since 2015, this course utilises [BeeViva](http://www.bee-viva.com/competitions), an Italian public data platform for hosting competitions

* Beeviva is also used for the event [Stats Under the Stars](http://sus.stat.unipd.it/)

---

# Stats Under the Stars 

.pull-left[

![](images/SUS.JPG)
.center[CaffÃ¨ Pedrocchi, Padua]

]

.pull-left[
* [SUS](http://sus.stat.unipd.it/) Padua (8/9/15)
* [SUS^2](http://www.labeconomia.unisa.it/sus2/) Salerno (7/6/16)
* [SUS^3](http://local.disia.unifi.it/sus3/) Florence (27/6/17)
* [SUS^4](https://www.unipa.it/dipartimenti/seas/sus4/) Palermo (19/6/18)
* SUS^5 Milan ! 
]


---

# Exam

The exams consists of two parts:

* Test (in lab)
    - Multiple-choice and open questions about theory 
    - Programming exercises (selected R scripts available)

* Competitions (homework)
    - Attending students can form a team (max. 3 persons)

* The final grade is a weighted average of test (2/3) and competitions (1/3)

* See the [course syllabus](https://aldosolari.github.io/DM/syllabus/syllabus.html) for more information

---

# This year competitions

1. **House prices** (40%)

2. **OkCupid** (30%)

3. **TBD** (30%)

| Competition | Problem |  Entry | Submission | 
|---|---|----|----|
| House prices | Regression | October 8 | November 16  |
| OkCupid | Classification | October 15 | November 16  |
| TBD | TBD | October 15 | November 16  |

.center[Competitions timeline]


---
layout: false
class: inverse, middle, center

# The two cultures

---

# The two cultures

Breiman (2001) distinguished between

**Statistical modeling**
* emphasis on probability models
* the goal is explanation (by assessing the uncertainty of the
estimates)

**Machine learning**
* emphasis on algorithms
* the goal is prediction accuracy

What is the difference today?

---

# Rock-Paper-Scissor algorithm

![](images/rps.jpg)

.center[http://www.nytimes.com/interactive/science/rock-paper-scissors.html]

---

# What the computer is thinking

* Four last throws

    - **PAPER, SCISSORS, SCISSORS, ROCK** <- HUMAN 

    - **SCISSORS, PAPER, PAPER, SCISSORS** <- COMPUTER

* COMPUTER is going to search over 200,000 rounds of Rock-Paper-Scissors data and find
all the times that HUMAN played **P, S, S, R**  when COMPUTER played **S, P, P, S**

* Of all the times HUMANS played **P, S, S, R**  when COMPUTER played **S, P, P, S**, HUMANS played **PAPER** at their next throw the most. 

* COMPUTER thinks that you will do the same this time

---

# Is this machine learning or statistics?

* **Theorem** 
A truly random game would result in a tie

* **Hypothesis** 
A human is not truly random

* **Algorithm** 
Learn humans non-random patterns from the data

* **Counter-attack?** 

---

# Same thing, different name?

| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| target variable | $Y$ | response variable |
| attribute, feature | $X$ | predictor, explanatory variable |
| supervised learning | model $Y$ as a function of $X$ | regression |
| hypothesis | $Y= f(X) + \varepsilon$ | model, regression function |
| instances, examples | $(Y_1,X_1),\ldots,(Y_n,X_n)$ | samples, observations |
| learning | $\displaystyle \hat{f} = \underset{f\in \mathcal{F}}{\arg\min} \sum_{i=1}^{n} \mathrm{loss}(Y_i,f(X_i))$ | estimation, fitting |
| classification | $\hat{Y} = \hat{f}(X)$ | prediction |
| generalization error | $\mathbb{E} [\, \mathrm{loss}(Y,\hat{f}(X))\,]$ | risk |



---

# So, what's the difference? 


| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| | FOCUS | |
|prediction |  |significance |
| | CULTURE | |
|algorithmic/optimization |  | modeling |
| | METHODS |  |
|decision trees |  | linear/logistic regression |
|k-nearest-neighbors | | discriminant analysis |
|neural networks | | mixed models |
|support vector machines || ridge/lasso regression |
|adaboost | | GAM |
|random forests | | random forests |

---

# Working together

* Leo Breiman's fundamental contributions:
    - Bagging
    - Random Forests
    - Boosting

* Breiman's work helped to bridge the gap between statistical
modeling and machine learning

* __Statistical Learning__ = __Statistical__ Modeling + Machine __Learning__

---

# Statistical Learning

* __Unsupervised learning__
    - the data consists of $p$ variables $X_1, \ldots, X_p$; no variable has a special status
    - the goal is clustering, dimensionality reduction, etc.

* __Supervised learning__
    - the data consists of both response $Y$ and predictors $X_1, \ldots, X_p$
    - the problem is called supervised learning since the response
`supervises' the learning process
    - the goal is (usually) prediction of the response
    - $Y$ continuous : __regression__ problem 
    - $Y$ binary/multi-class : __classification__ problem 

---

# Convergence

__Machine learning__ constructs algorithms that can learn from data

__Statistical learning__ is a brach of Statistics that was born in response to Machine
learning, emphasizing statistical models and assessment of
uncertainty

__Data mining__ finding patterns in data

__. . .__

all converge into __Data Science__



