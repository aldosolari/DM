---
title: "Orange benchmark"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/aldosolari/Desktop/DM_off/5_otop/code")
```

## Import data

```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
n = nrow(train)
m = nrow(test)
test$churn=NA
combi = rbind(train,test)
train = combi[1:n,]
test = combi[(n+1):(n+m),]
```

## Missing values

```{r}
pMiss <- function(x){sum(is.na(x))/length(x)*100}

pMiss2 = apply(train,2,pMiss)
hist(pMiss2)

pMiss1 = apply(train,1,pMiss)
hist(pMiss1)

vars_miss = which(pMiss2==100)
vars_miss
```

## Zero- and Near Zero-Variance Predictors

In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable.

To identify these types of predictors, the following two metrics can be calculated:

* the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data 

* the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases

If the frequency ratio is greater than a pre-specified threshold freqCut = 95/5 and the unique value percentage is less than a threshold uniqueCut = 10, we might consider a predictor to be near zero-variance.

```{r}
library(caret)
vars_zv = nearZeroVar(train)
setdiff(vars_zv, vars_miss)
```


## Type of predictors

```{r}
vars <- setdiff(names(train),c('churn', 
                               names(train)[vars_zv]
                               ))
table(sapply(train[,vars],class))
# categorical and numerical
vars_cat <- vars[sapply(train[,vars],class) %in% c('factor','logical')]
vars_cat
vars_num<- vars[sapply(train[,vars],class) %in% c('numeric','integer')]
vars_num
length(c(vars_cat, vars_num))
```


## Single categorical predictor

To create a table comparing the levels of variable
218 against the labeled churn outcome

```{r}
# Tabulate levels of Var218.
table218 <- table(
   Var218=train[,'Var218'],  
   churn=train[,'churn'], 	
   useNA='ifany') 	
print(table218)
print(table218[,2]/(table218[,1]+table218[,2]))
```

This summary tells us that when variable 218 takes on a value of cJvF, around 6% of
the customers churn; when it's UYBR, 8% of the customers churn; and when it’s not
recorded (NA), 27% of the customers churn. The utility of any variable level is a combination
of how often the level occurs (rare levels aren't very useful) and how extreme
the distribution of the outcome is for records matching a given level. Variable 218
seems like a predictor that's easy to use and helpful with prediction.


## Calibration set

Split the data into three sets: training, calibration, and test.

The intent of the three-way split is this: we'll use the training set for most of our work, and
we'll never look at the test set (we'll reserve it for our final report of model performance).

The calibration set is used to simulate the unseen test set during modeling - we'll look at performance on the calibration set to estimate if we’re overfitting. 

```{r}
set.seed(123)
train.all = train
is.calib <- rbinom(n=nrow(train.all),size=1,prob=0.25)>0
# further split training data into training and calibration.
train = train.all[!is.calib,]
calib = train.all[is.calib,]
```


## Scoring categorical predictors

We also need to design a strategy for what to do if a new level not seen during training were to occur during model use. We'll build a function that converts NA to a level (as it seems to
be pretty informative) and also treats novel values as uninformative.

```{r, warning=FALSE, message=FALSE}
y = train$churn
x=train[ ,"Var218"]
xstar = calib[,'Var218']

# how often (%) y is positive for the training data
pPos <- sum(y==1)/length(y)
pPos

# how often (%) y is positive for NA values for the training data
pPosNA <- prop.table(table(as.factor(y[is.na(x)])))["1"]
pPosNA

# how often (%) y is positive for the levels of the categorical predictor for the training data
tab <- table(as.factor(y),x)
tab
pPosLev <- (tab["1",]+1.0e-3*pPos)/(colSums(tab)+1.0e-3)
pPosLev

# compute predictons
pred <- pPosLev[xstar]
pred[1:10]

# compute predictons for NA levels of xstar
pred[is.na(xstar)] <- pPosNA

# compute predictions for levels of xstar that weren't seen during training
pred[is.na(pred)] <- pPos
pred[1:10]
```


We calculate the score (the predicted probability of churning) for each categorical variable

```{r, warning=FALSE, message=FALSE}
# Given a vector of training response (y), a categorical training predictor (x), and a calibration categorical training predictor (xstar), use y and x to build a single-variable model and then apply the model to xstart to get new predictions.
score_cat <- function(y,x,xstar){
  pPos <- sum(y==1)/length(y)
  pPosNA <- prop.table(table(as.factor(y[is.na(x)])))["1"]
  tab <- table(as.factor(y),x)
  pPosLev <- (tab["1",]+1.0e-3*pPos)/(colSums(tab)+1.0e-3)
  pred <- pPosLev[xstar]
  pred[is.na(xstar)] <- pPosNA
  pred[is.na(pred)] <- pPos
  pred
}



library('ROCR')
calcAUC <- function(phat,truth) {
  perf <- performance(prediction(phat,truth=="1"),'auc')
  as.numeric(perf@y.values)
}

# Once we have the predictions, we can find the categorical variables that have a good AUC both on the training data and on the calibration data
for(v in vars_cat) {
  pi <- paste('score', v, sep='')
  train[,pi] <- score_cat(train$churn,train[,v],train[,v])
  calib[,pi] <- score_cat(train$churn,train[,v],calib[,v])
  test[,pi] <- score_cat(train$churn,train[,v],test[,v])
  train.auc <- calcAUC(train[,pi],train$churn)
  if(train.auc >= 0.8) {
    calib.auc <- calcAUC(calib[,pi],calib$churn)
    print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
                  pi,train.auc,calib.auc))
  }
}
```

Many categorical variables that have a very large number of
levels and are subject to overfitting.
Note how, as expected, each predictor's training AUC is inflated compared to its calibration AUC. This is because many of these predictors have thousands of levels. For example

```{r}
length(unique(train[,"Var217"]))
nlevels(train[,"Var217"])
```

A good trick to work around this is to sort the variables by their AUC score on the calibration set (not seen during training), which is a better estimate of the variable's true utility. 

In our case, the most promising variable is variable 206

```{r}
length(unique(train[,"Var206"]))
nlevels(train[,"Var206"])

tr206 <- score_cat(train$churn,train[,"Var206"],train[,"Var206"])
calcAUC(tr206,train$churn)

ca206 <- score_cat(train$churn,train[,"Var206"],calib[,"Var206"])
calcAUC(ca206,calib$churn)
```


## Scoring numerical predictors

A common method is to bin the numeric predictor into a number of intervals and then use the intervals as a new categorical variable.

```{r, warning=FALSE, message=FALSE}
score_num <- function(y,x,xtilde){
  cuts <- unique(as.numeric(quantile(x,probs=seq(0, 1, 0.1),na.rm=T)))
  x.cut <- cut(x,cuts)
  xtilde.cut <- cut(xtilde,cuts)
  score_cat(y,x.cut,xtilde.cut)
}

for(v in vars_num) {
  pi <- paste('score',v,sep='')
  train[,pi] <- score_num(train$churn,train[,v],train[,v])
  calib[,pi] <- score_num(train$churn,train[,v],calib[,v])
  test[,pi] <- score_num(train$churn,train[,v],test[,v])
  train.auc <- calcAUC(train[,pi],train$churn)
  if(train.auc >= 0.55) {
    calib.auc <- calcAUC(calib[,pi],calib$churn)
    print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
                  pi,train.auc,calib.auc))
   }
}
```


Notice in this case the numeric predictors behave similarly on the training and calibration data. This is because our prediction method converts numeric variables into categorical
variables with around 10 well-distributed levels, so our training estimate tends to be good and not overfit.

## Plotting predictor performance

What the next figure is showing is the conditional distribution of predVar126 for churning accounts (red line) and the distribution of predVar126 for nonchurning
accounts (blue line). 

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
ggplot(data=calib) +
  geom_density(aes(x=scoreVar126,color=as.factor(churn)))
```

We can deduce that low values of
predVar126 are rare for churning accounts and not as rare for non-churning
accounts (the graph is read by comparing areas under the curves).

## Variable selection


A key part of building many variable models is selecting what predictors to use and how the predictors are to be transformed or treated. 

An important evaluation of an estimated probability is the log likelihood. The log likelihood is the logarithm of the product of the probability the model assigned to each observation.

For an observation with churn=1 and an estimated probability of 0.9 of being churn, the log likelihood is log(0.9); for an observation with churn=-1, the same score of 0.9 is a log likelihood of log(1-0.9). 


The null model has probability of churn = (number of churn clients)/(the total number of clients).

The score is a bit ad hoc, but tends to work well in
selecting variables. Notice we're using performance on the calibration set (not the training set) to select predictors.

```{r, warning=FALSE, message=FALSE}
vars_score <- paste('score',c(vars_cat,vars_num),sep='')

# Define a convenience function to compute log likelihood.
loglik <- function(y,x) {
    sum(ifelse(y==1,log(x),log(1-x)))
}

vars_sel <- c()
cutoff <- 5
loglik0 <- loglik(calib$churn,
                  sum(calib$churn==1)/length(calib$churn)
                  )

# Run through categorical predictor and pick based on a deviance improvement (related to difference in log likelihoods)
for(v in vars_cat) {
  pi <- paste('score',v,sep='')
  deviance <- 2*( (loglik(calib$churn,calib[,pi]) - loglik0))
  if(deviance>cutoff) {
    print(sprintf("%s, calibrationScore: %g",
                  pi,deviance))
    vars_sel <- c(vars_sel,pi)
  }
}

# Run through numerical predictor and pick based on a deviance improvement (related to difference in log likelihoods)
for(v in vars_num) {
  pi <- paste('score',v,sep='')
  deviance <- 2*( (loglik(calib$churn,calib[,pi]) - loglik0))
  if(deviance>cutoff) {
    print(sprintf("%s, calibrationScore: %g",
                  pi,deviance))
    vars_sel <- c(vars_sel,pi)
  }
}
```

## Submission

```{r}
library(rpart)
fml <- paste('churn == 1 ~ ',paste(vars_sel,collapse=' + '),sep='')
fml
fit = rpart(fml, data=train)
phat = predict(fit, newdata=test)

write.table(file="mySubmission.txt", phat, row.names = FALSE, col.names = FALSE)

```
