---
title: Question 1.6 from Lecture notes on ridge regression
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Download the \texttt{multtest} R package from BioConductor. 
  
```{r, eval=FALSE}
source("http://www.bioconductor.org/biocLite.R")
biocLite("multtest")
```

Activate the library and load leukemia data from the package:

```{r, eval=FALSE}
library(multtest)
data(golub)
```

The objects \texttt{golub} and \texttt{golub.cl} are now available. The matrix-object golub contains the expression
profiles of 38 leukemia patients. Each profile comprises expression levels of 3051 genes. The numeric-object \texttt{golub.cl} is an indicator variable for the leukemia type (AML or ALL) of the patient.


(a) Relate the leukemia subtype and the gene expression levels by a logistic regression model. Fit this model
by means of penalized maximum likelihood, employing the ridge penalty with penalty parameter $\lambda=1$. This is implemented in the \texttt{penalized} package available from CRAN. Note: center (gene-wise) the expression levels around zero.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(multtest)
library(penalized)
data(golub)
y = golub.cl
X = t(golub)
n = length(y)
fit <- penalized(y,penalized=X,lambda2=1, standardize = T, trace=F, model = "logistic")
```

(b) Obtain the fits from the regression model. The fit is almost perfect. Could this be due to overfitting
the data? Alternatively, could it be that the biological information in the gene expression levels indeed
determines the leukemia subtype almost perfectly?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
yhat = predict(fit, penalized=X)
rbind(y,f_x=round(yhat,3))
```


(c) To discern between the two explanations for the almost perfect fit, randomly shuffle the subtypes. Refit
the logistic regression model and obtain the fits. On the basis of this and the previous fit, which explanation is more plausible?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
y.p <- sample(y)
fit.p <- penalized(y.p,penalized=X,lambda2=1, standardize = T, trace=F, model = "logistic")
yhat.p = predict(fit.p, penalized=X)
rbind(y.p,f_x=round(yhat.p,3))
```

(d) Compare the fit of the logistic model with different penalty parameters, say $\lambda=1$ and $\lambda=1000$. How
does $\lambda$ influence the possibility of overfitting the data?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
fit.1000 <- penalized(y,penalized=X,lambda2=1000, standardize = T, trace=F, model = "logistic")
yhat.1000 = predict(fit.1000, penalized=X)
rbind(y,f_x=round(yhat.1000,3))
```

(e) Describe what you would do to prevent overfitting.

