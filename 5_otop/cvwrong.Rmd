---
title: The Wrong and Right Way to Do Cross-validation
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**From ESL, Chapter 7.10.2**

Consider a classification problem with a large number of predictors, as may arise, for example, in genomic or proteomic applications. A typical strategy for analysis might be as follows:

1. Screen the predictors: find a subset of ``good'' predictors that show
fairly strong (univariate) correlation with the class labels

2. Using just this subset of predictors, build a multivariate classifier.

3. Use cross-validation to estimate the unknown tuning parameters and
to estimate the prediction error of the final model.

Is this a correct application of cross-validation? 


**Simulation** Consider a scenario with $n = 50$ samples in two equal-sized classes, and $p = 5000$ quantitative
predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%. 

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use $K$-fold CV to estimate the test error of the final model

Over 50 simulations from this
setting, the average CV error rate was 3%. This is far lower than the true
error rate of 50%. What has happened?



**Exercize** Perform the simulation with $K=5$


```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)

n = 50
p = 5000

# binary response
y = c(rep(0,n/2),rep(1,n/2))
# predictors independent of the response
X = matrix(rnorm(p*n), ncol=p)

# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]

# 5-fold CV
require(class)

K<-5
set.seed(123)
# create folds
folds <- sample( rep(1:K,length=n) )

Err.CV = vector()

for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred <- knn(train = Xbest[ -out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)

    # % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
cat("5-fold CV:", mean(Err.CV))
```


The problem is that the predictors have an unfair
advantage, as they were chosen in step 1. on the basis of all of the observations.
Leaving observations out after the variables have been selected does not correctly
mimic the application of the classifier to a completely independent
test set, since these predictors *have already seen* the left out observations.

Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into $K$ cross-validation folds at random.
2. For each fold $k = 1,\ldots,K$

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold $k$.
    
    b. Using just this subset of predictors,fit a 1-nearest neighbor classifier, using all of the observations except those in fold $k$
    
    c. Use the classifier to predict the class labels for the observations in fold $k$
    
The error estimates from step 2.c are then accumulated over all $K$ folds, to produce the cross-validation estimate of prediction error.

**Exercize** Try with $K=5$ 
    
```{r, echo=FALSE}
set.seed(123)

n = 50
p = 5000

# binary response
y = c(rep(0,n/2),rep(1,n/2))
# predictors independent of the response
X = matrix(rnorm(p*n), ncol=p)


# 5-fold CV
require(class)

K<-5
set.seed(123)
# create folds
folds <- sample( rep(1:K,length=n) )

Err.CV = vector()

for (k in 1:K){
out = which(folds==k)

# compute p correlations between each predictor and the response
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]


# predict the held-out samples using k nearest neighbors
pred <- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)

    # % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
cat("5-fold CV:", mean(Err.CV))

```


In general, with a multistep modeling procedure, cross-validation must
be applied to the entire sequence of modeling steps. In particular, observations
must be “left out” before any selection or filtering steps are applied.