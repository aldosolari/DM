---
title: "Data Mining"
subtitle: "Distorsione e Varianza"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true     
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Il segnale e il rumore

Le risposte $y_1, \ldots, y_n$ sono realizzazioni delle v.c.
$$Y_i = f(x_i)+ \varepsilon_i, \quad i=1,\ldots,n$$
dove

* $f$ è la **funzione di regressione** (il segnale)

* $\varepsilon$  è il termine di **errore** (il rumore) 
dove
$\varepsilon_1,\ldots,\varepsilon_n$ sono i.i.d. con $\mathbb{E}(\varepsilon_i)=0$ e $\mathbb{V}\mathrm{ar}(\varepsilon_i)=\sigma^2$. 


---

# Il segnale

```{r, echo=F}
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
plot(x,ftrue, type="l", col=4, ylab="f(x)", lwd=2)
```

---

# Il rumore

```{r, echo=F}
plot(x,ftrue, type="l", col=4, ylab="f(x)", lwd=2)
set.seed(12)
x0 = x[15]
y0 = ftrue[15]+rnorm(1,0,sd=0.01)
points(x0,y0)
segments(x0,y0,x0,ftrue[15], col=2)
```

---

# Sovra-adattamento (overfitting)

Confondere il rumore per il segnale

```{r, echo=F}
rm(list=ls())
n = 10
p = 9
set.seed(1793)
X = matrix(rnorm(n*p),nrow=n, ncol=p)
y = X[,1] + rnorm(n,0,0.5)
fit = lm(y~ 0 + X)
yhat = predict(fit)
plot(X[,1],y, xlab="x1")
ix = sort(X[,1], index.return=T)$ix
lines(X[ix,1], yhat[ix])
abline(a=0,b=1, col=4, lwd=2)
```


---

# Errore di previsione

* Funzione di perdita: errore quadratico medio (MSE)

* Obiettivo: minimizzare l'errore di previsione atteso (nel setting Fixed-X)
$$\mathrm{ErrF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}})=\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}( Y^*_i - \hat{f}(x_i))^2\right]= \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[( Y^*_i - \hat{f}(x_i))^2\right]$$
dove il valore atteso è rispetto alle v.c. $Y_1,\ldots,Y_n$ e $Y^*_1,\ldots,Y^*_n$


---
layout: false
class: inverse, middle, center

# La scomposizione distorsione-varianza 

---

# Fonti di errore

1. **Errore irriducibile** <br>
Possiamo fare previsioni senza commettere errori?  <br>
No, neppure se conoscessimo la vera $f$, per la presenza del termine di errore $\varepsilon$

2. **Distorsione** <br>
Quanto è lontano (in media) lo stimatore $\hat{f}$ dalla vera $f$? <br>
Ad esempio, se stimiamo una retta di regressione quando la vera relazione è quadratica

3. **Varianza** <br>
Quanto è variabile lo stimatore $\hat{f}$? <br>
In altre parole, quanto variano sono le nostre stime se le calcoliamo su training set diversi?

---

# Errore riducibile ed irriducibile 

* Supponiamo di prevedere $y^*_i$ con $\hat{y}_i^* = \hat{f}(x_i)$ dove $\hat{f}$ è la stima di $f$ ottenuta con il training set

* L'accuratezza di $\hat{y}^*_i$ dipende da due quantità, l'errore riducibile ed l'errore irriducibile 

* Abbiamo
$$
\begin{aligned}
\mathbb{E}[(Y^*_{i} - \hat{Y}^*_i)^2] & = \mathbb{E}[(f(x_i) + \varepsilon^*_i -  \hat{f}(x_i))^2] \\
& = \mathbb{E}[  \{f(x_i) -  \hat{f}(x_i) \}^2 + \{\varepsilon^*_i\}^2 +2  \{[f(x_i) -\hat{f}(x_i)]\varepsilon^*_i\} ] \\
 & = \underbrace{\mathbb{E}[\{f(x_i)  -  \hat{f}(x_i)\}^2]}_{\mathrm{RIDUCIBILE}} + \underbrace{\mathbb{V}\mathrm{ar}(\varepsilon^*_i)}_{\mathrm{IRRIDUCIBILE}}
\end{aligned}
$$
dove $\mathbb{V}\mathrm{ar}(\varepsilon^*_i) = \sigma^2$


---

# L'errore riducibile

* L'errore riducibile può essere ulteriormente scomposto in __distorsione__ (al quadrato) e __varianza__ dello stimatore $\hat{f}$

\begin{aligned}
\mathbb{E}[\{f(x_i)  -  \hat{f}(x_i))^2] & = \mathbb{E}[(f(x_i)  - \mathbb{E}\hat{f}(x_i) + \mathbb{E}\hat{f}(x_i) - \hat{f}(x_i)\}^2]\\
& =
\underbrace{ [\mathbb{E}\hat{f}(x_i) - f(x_i) ]^2}_{[\mathrm{Distorsione}(\hat{f}(x_i))]^2} + \underbrace{\mathbb{V}\mathrm{ar}[\hat{f}(x_i)]}_{\mathrm{Varianza}(\hat{f}(x_i))}\\
\end{aligned}

* Riassumendo, la scomposizione dell'errore di previsione è data da

$$\mathrm{ErrF} = \sigma^2 + \underbrace{\frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2}_{\mathrm{Distorsione}^2} + \underbrace{\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}(\hat{f}(x_i))}_{\mathrm{Varianza}}$$

* Distorsione e varianza sono quantità in conflitto e non possiamo
minimizzare entrambe contemporaneamente. Dobbiamo quindi scegliere un __compromesso__ (trade-off) tra distorsione e varianza.


---
layout: false
class: inverse, middle, center

# Se conoscessimo la vera $f$ ... 

---

# La vera $f$

```{r, echo=F}
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
plot(x,ftrue, type="l", col=4)
```

---

```{r}
sigmatrue=0.01

x = seq(.5,3,length=30)

n = length(x)

ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
```

---

Per la regressione polinomiale di grado $d$ , la stima del vettore $\underset{p\times 1}{\beta}$ (con $p=d+1$) risulta pari a
$\hat{\beta} = (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}$
dove $\underset{n \times p}{\mathbf{X}}$ è la matrice del disegno. La previsione per $i$-sima osservazione è $\hat{f}(x_i)=x_i^\mathsf{T}\hat{\beta}$, dove $x_i^\mathsf{T}$ è l' $i$-sima riga di $\mathbf{X}$. 

La distorsione risulta
$$
\begin{aligned}
\mathbb{E}[\hat{f}(x_i)] - f(x_i) &= \mathbb{E}[ x_i^\mathsf{T}(\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} \mathbf{X}^\mathsf{T}\mathbf{y} ] - f(x_i)\\
&= x_i^\mathsf{T}(\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbb{E}[\mathbf{y}] - f(x_i) \\
&= x_i^\mathsf{T}(\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{f} - f(x_i)
\end{aligned}
$$
dove $\mathbf{f} = (f(x_1),\ldots, f(x_n))^\mathsf{T}$. 

Abbiamo inoltre
$\mathbb{V}\mathrm{ar}(\hat{\beta})=\sigma^2 (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1}$,
quindi la varianza della previsione $\hat{f}(x_i)$  risulta
$$\mathbb{V}\mathrm{ar}(\hat{f}(x_i))=x_i \mathbb{V}\mathrm{ar}(\hat{\beta}) x_i^\mathsf{T}$$
e sommando rispetto alle $n$ osservazioni
$$\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}(\hat{f}(x_i))=\mathrm{tr}(\mathbf{X} \mathbb{V}\mathrm{ar}(\hat{\beta}) \mathbf{X}^\mathsf{T}) =\sigma^2 \mathrm{tr}(\mathbf{X}^\mathsf{T}\mathbf{X} (\mathbf{X}^\mathsf{T} \mathbf{X})^{-1} ) = \sigma^2 p$$

---

```{r}
# grado del polinomio
d = 5

# matrice del disegno
X <- model.matrix(lm(ftrue ~ poly(x,degree=d)))
invXtX <- solve(crossprod(X))

# distorsione
Bias2 <- ( apply(X, 1, function(x) 
                 x %*% invXtX %*% t(X) %*% ftrue) - ftrue )^2


# varianza
Var = apply(X, 1, function(x) 
  sigmatrue^2 * t(x) %*% invXtX %*% x
)
```

---

```{r}
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```

---


```{r}
# errore di previsione
sigmatrue^2 + mean( Bias2 ) + mean( Var )

# verifichiamo via simulazione l'errore di previsione
ErrF = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
y_new = ftrue + rnorm(n,0,sigmatrue)
MSE.te = mean( (yhat - y_new)^2 )
}
B = 1000
mean(replicate(B, ErrF(d=5)))
```

---


# Polinomio di grado 3

.pull-left[

```{r, echo=F}
B = 100
# simulation function
sim = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
}
# 3rd degree polynomial
d = 3
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1, ylim=c(.45,.55))
lines(x,ftrue, col=4)
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r, echo=F}
# matrice del disegno
X <- model.matrix(lm(ftrue ~ poly(x,degree=d)))
invXtX <- solve(crossprod(X))

# distorsione
Bias2 <- ( apply(X, 1, function(x) 
  x %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

# varianza
Var = apply(X, 1, function(x) 
  sigmatrue^2 * t(x) %*% invXtX %*% x
)

barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1), ylim=c(0,0.00012))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]

---

# Polinomio di grado 12

.pull-left[

```{r, echo=F}
# 12th degree polynomial
d = 12
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1, ylim=c(.45,.55))
lines(x,ftrue, col=4)
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r, echo=F}
# matrice del disegno
X <- model.matrix(lm(ftrue ~ poly(x,degree=d)))
invXtX <- solve(crossprod(X))

# distorsione
Bias2 <- ( apply(X, 1, function(x) 
  x %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

# varianza
Var = apply(X, 1, function(x) 
  sigmatrue^2 * t(x) %*% invXtX %*% x
)

barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1), ylim=c(0,0.00012))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]


```{r, echo=F}
d = 3
# expected value 
Ehatf = fitted(lm(ftrue ~ poly(x,degree=d)))
Bias2 = mean( (ftrue - Ehatf)^2 )
```


```{r, echo=F}
# true variance
p = d+1
Var = (sigmatrue^2)*p/n
```


---

# Errore riducibile


```{r, echo=F}
ds = 1:20
ps = ds+1
Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
  )
Vars = ps*(sigmatrue^2)/n
Reds = Bias2s+Vars 
barplot(Reds, ylab="Errore riducibile", names.arg=ds)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```

---

# Errore di previsione

```{r, echo=F}
Irr = rep(sigmatrue^2,length(ps))
ErrFs = Reds + Irr
barplot(ErrFs, ylab="Errore di previsione", names.arg=ds)
barplot(Irr, add=T, col=1, names.arg=" ")
legend("topright", c("Riducibile","Irriducibile"), col=c("gray",1), pch=c(19,19))
```

---

# Il miglior modello

.pull-left[

```{r, echo=F}
# best model!
d = 5 
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1)
lines(x,ftrue, col=4)
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r, echo=F}
Bias2 = (ftrue - Ehatf)^2
Var = apply(yhats,1,var)
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]

---

# Il compromesso distorsione-varianza

* Errore riducibile = Distorsione $^2$ + Varianza

* Modelli con poca distorsione tendono ad avere elevatà variabilità

* Modelli con poca variabilità tendono ad avere elevatà distorsione

* Da un lato, anche se il nostro modello è non distorto( $\mathbb{E}\hat{f}(x_i)=f(x_i)$ ), l'errore di previsione può essere elevato se il modello è molto variabile

* Dall'altro lato, un modello che prevede una costante (e.g. $\hat{f}(x_i)=0$) ha varianza nulla ma elevata distorsione

* Per prevedere bene, dobbiamo bilanciare la distorsione e la varianza

---

![](images/tiroasegno.png)

---

# Riepilogo

* **Dati**: training set / test set

* **Segnale/rumore**: funzione di regressione / errore

* **Sovra-adattamento**: confondere il rumore per il segnale

* **Errore di previsione**: riducibile + irriducibile

* **Errore riducibile**: distorsione $^2$ + varianza

*  **Compromesso**: introdurre un pò di distorsione se in cambio riduco di molto la variabilità 

---

layout: false
class: inverse, middle, center

# Metodi nonparametrici

---

# Metodi nonparametrici

* I metodi nonparametrici non fanno assunzioni specifiche a proposito della forma funzionale della $f$ (ad esempio, " $f$ è un polinomio "). Si lascia che i dati "parlino da soli"

* Vantaggio: evitando assunzioni sulla forma di $f$, permettiamo qualsiasi forma funzionale per $f$ (anche le più strane e irregolari)

* Svantaggio: poichè il problema della stima di $f$ non si riduce più al problema di stimare $p$ parametri (se $p \ll n$), risulta necessario avere tante osservazioni ( $n$ elevato )

---

# Il metodo dei $k$ vicini più vicini 

* Il metodo dei $k$ vicini più vicini (*k-nearest neighbors*, abbreviato k-NN) è un metodo molto semplice ma efficace. Considereremo questo approccio nel caso di un problema di regressione

* Supponiamo di voler prevedere la risposta $y^*_1$ in corrispondenza ad un certo punto $x^*_1$. Definiamo il "vicinato" (*neighbourhood*) di questo punto con $N_k(x^*_1)$: è l'insieme dei $k$ punti del training set  più "vicini" a $x^*_1$

* Dobbiamo quindi definire una distanza: si può ad esempio considerare la distanza Euclidea tra $x_i$ e $x^*_1$ 
$$\| x_i - x^*_1 \|_2=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)} = \sqrt{\sum_{j=1}^{p}(x_{ij} - x^*_{1j})}$$ dove 
$\|\cdot \|_2$ indica la norma Euclidea

* La previsione è definita dalla media delle $k$ risposte $y_i$ del training set che appartengono al vicinato di $x^*_1$, i.e. le $x_i \in N_k(x^*_1)$
$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$

---

# Parametro di regolazione

* $k$ può assumere valori da $1$ a $n$, e rappresenta il **parametro di regolazione**  (*tuning parameter*) 

* Un $k$ piccolo corrisponde ad una stima più flessibile, vicerversa un $k$ grande ad una stima meno flessibile

* Il caso estremo $k=n$ corrisponde alla media delle risposte del training: $\hat{f}(x^*_1) = \bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$, mentre $k=1$ corrisponde a $\hat{f}(x^*_1) = y_i$ per un certo indice $i$ tale che $x_i$ è il punto più vicino a $x^*_1$

* Operativamente, poichè il metodo utilizza una distanza, spesso i valori dei predittori vengono standardizzati

---

```{r}
library(readr)
df <- read_table2("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat")[-31,]
train <- data.frame(x=df$x, y=df$y.yesterday)
test <- data.frame(x=df$x, y=df$y.tomorrow)
library(kknn)
my_k = 4
fit = kknn(y ~ x, train, test, distance = 2, kernel = "rectangular", k = my_k)
yhat = fit$fitted.values
```

---

```{r}
plot(y ~ x, train, main=paste("k = ", my_k))
lines(test$x, yhat, col=4, type="s")
```

---

# Errore di previsione per kNN

Nel Fixed-X setting, la distorsione di kNN risulta

$$
\begin{aligned}
\mathbb{E}[\hat{f}(x_i)] - f(x_i) &= \frac{1}{k}\sum_{i \in N_k(x_i)}\mathbb{E}[  y_{i}] - f(x_i)= \frac{1}{k}\sum_{i \in N_k(x_i)}f(x_i) - f(x_i)\\
\end{aligned}
$$
e la varianza di kNN risulta
$$
\begin{aligned}
\mathbb{V}\mathrm{ar}(\hat{f}(x_i)) &= \mathbb{E}\{(\hat{f}(x_i) - \mathbb{E}[\hat{f}(x_i)] )^2 \} = \frac{\sigma^2}{k}
\end{aligned}
$$
quindi l'errore di previsione di kNN si può calcolare come
$$\mathrm{ErrF} = \sigma^2 + \frac{1}{n}\sum_{i=1}^{n}(\frac{1}{k}\sum_{i \in N_k(x_i)}f(x_i) - f(x_i))^2 + \frac{\sigma^2}{k}$$

---

```{r}
ks = 1:n
Vars = sigmatrue^2/ks
Bias2s = sapply(ks, function(k) 
  mean( 
    ( ftrue - kknn(ftrue ~ x, train, test, kernel = "rectangular", k = k)$fitted.values )^2 
      )
  )
```

---


```{r}
barplot(Bias2s+Vars, ylab="Errore riducibile", names.arg=ks)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```

---

```{r}
# errore di previsione
sigmatrue^2 + Bias2s[4] +Vars[4]
# verifichiamo via simulazione l'errore di previsione
ErrF = function(k){
y = ftrue + rnorm(n,0,sigmatrue)
yhat =  kknn(y ~ x, train, test, distance = 2, kernel = "rectangular", k = k)$fitted.values
y_new = ftrue + rnorm(n,0,sigmatrue)
MSE.te = mean( (yhat - y_new)^2 )
}
B = 1000
mean(replicate(B, ErrF(k=4)))

```

