---
title: "Data Mining"
subtitle: "Boston housing data"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo=T, eval=T, message=F, warning=F, error=F, comment=NA)
```

# Model stacking

Stacking is a general method to combine models 

Consider a library of $L$ models $\hat{f}_1,\ldots, \hat{f}_L$ fitted on the training data
$$(x_1,y_1),\ldots, (x_n,y_n)$$


Perhaps by combining their respective efforts, we could get even better prediction than using any
particular one

The issue then how we might combine them for predicting the test set $y^*_1,\ldots,y^*_m$

A linear combination 
$$\hat{y}^{*}_i = \sum_{l=1}^{L}w_l \hat{f}_l(x^*_i)$$
requires to define the the weights $w_1,\ldots,w_L$

---

# Least squares

The method of least squares provides the weights 
$$\hat{w}_1,\ldots,\hat{w}_L=  \underset{w_1,\ldots,w_L}{\arg \min\,\,} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}_l(x_i) \right]^2$$

However, in this way we fail to take into account for model complexity: models with higher complexity get higher weights

For example, consider $L$ predictors and let $\hat{f}_{l}$ be the linear model formed the best subset of predictors of size $l$, $l=1,\ldots,L$, where best is defined as having the smallest $\mathrm{MSE}_{\mathrm{Tr}}$

Then all the weight goes on the largest model, that is, $\hat{w}_L = 1$ and $\hat{w}_l = 0$ for $l< L$

---

# Stacked regression

Wolpert (1992) presented an interesting idea, called *stacked generalizations*.  This proposal was translated in statistical language by Breiman, in 1993.

If we exclude $y_i$ in the fitting procedure of the models, then 
 $\hat{f}^{-i}_1(x_i),\ldots, \hat{f}^{-i}_L(x_i)$ do not depend on $y_i$ 

*Stacked regression* is an ensemble method with $\hat{f}_{\mathrm{stack}}=$ linear model and cross-validation with $K=n$

---

# Staked regression algorithm

1. Let $\hat{f}^{-i}_l(x_i)$ be the prediction at $x_i$ using model $l$ fitted to the training data with the
$i$th training observation $(x_i,y_i)$ removed

2. Obtain the weights by least squares
$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$

3. Compute the predictions for the test data as
$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$

---

layout: false
class: inverse, middle, center

# 20. Ensembles of models

---

# Libraries

```{r}
library(tidymodels)
tidymodels_prefer()
library(baguette)
library(stacks)
library(MASS)
```

---

# Boston data

```{r}
set.seed(123)
Boston <- Boston %>% mutate(medv = log(medv))
Boston_split <- initial_split(Boston, strata = medv)
Boston_train <- training(Boston_split)
Boston_test  <- testing(Boston_split)

Boston_folds <- vfold_cv(Boston_train, strata = medv)
```

The training and test data are 
$$(x_1,y_1),\ldots,(x_n,y_n),\quad (x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$$
with $n=378$ and $m=128$ for the Boston data set. 

The response variable is `medv`, and the
covariates are `crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat` 


---

# Models specification

```{r}
linear_reg_spec <- 
  linear_reg() %>% 
  set_engine("lm")

knn_spec <- 
  nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")

cart_spec <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

bag_cart_spec <- 
  bag_tree() %>% 
  set_engine("rpart", times = 20L) %>% 
  set_mode("regression")
```

---

# Workflow set

```{r}
# no_pre_proc
model_vars <- 
  workflow_variables(outcomes = medv, 
                     predictors = everything())

no_pre_proc <- 
  workflow_set(
    preproc = list(simple = model_vars), 
    models = list(linear_reg = linear_reg_spec, 
                  CART = cart_spec, 
                  CART_bagged = bag_cart_spec)
  )
```

---

```{r}
# normalized
normalized_rec <- 
  recipe(medv ~ ., data = Boston_train) %>% 
  step_normalize(all_predictors()) 

normalized <- 
  workflow_set(
    preproc = list(normalized = normalized_rec), 
    models = list(KNN = knn_spec)
  )

# poly
poly_recipe <- 
  normalized_rec %>% 
  step_poly(lstat, rm, crim) 

with_poly <- 
  workflow_set(
    preproc = list(poly = poly_recipe), 
    models = list(linear_reg = linear_reg_spec)
  )
```

---

```{r}
# all
all_workflows <- 
  bind_rows(no_pre_proc, normalized, with_poly) %>% 
  # Make the workflow ID's a little more simple: 
  mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

---

```{r}
grid_ctrl <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

library(tictoc)
tic()
grid_results <- 
    all_workflows %>% 
    workflow_map(seed = 123, 
                 resamples = Boston_folds, 
                 grid = 5, 
                 control = grid_ctrl, 
                 verbose = TRUE)
toc()
```

---

# Evaluating the models

```{r}
grid_results %>% 
  rank_results() %>% 
  filter(.metric == "rmse") %>% 
  select(model, .config, rmse = mean, rank)
```

---

# Stacking models

```{r}
Boston_stack <- stacks() %>% 
  add_candidates(grid_results)

Boston_stack
```

---

```{r}
ens <- blend_predictions(Boston_stack)
ens
```

---

```{r}
autoplot(ens)
```

---

```{r}
autoplot(ens, "weights")
```

---

```{r}
ens <- fit_members(ens)

reg_metrics <- metric_set(rmse, rsq)

ens_test_pred <- 
  predict(ens, Boston_test) %>% 
  bind_cols(Boston_test)

ens_test_pred %>% 
  reg_metrics(medv, .pred)
```

---

```{r}
member_preds <- 
  Boston_test %>%
  select(medv) %>%
  bind_cols(predict(ens, Boston_test, members = TRUE))

map_dfr(member_preds, rmse, truth = medv, data = member_preds) %>%
  mutate(member = colnames(member_preds))
```

---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

See Introduction to Statistical Learning, section 6.5.3.

Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error.

If the full data set is used to perform the feature selection step, the cross-validation errors that we obtain will not be accurate estimates of the test error.

To choose among the models of different sizes using cross-validation, we perform feature selection within each iteration of the cross-validation.

Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation.

---

# ESL Chapter 7.10.2

Consider a scenario with $n = 50$ samples in two equal-sized classes, and $p = 5000$ quantitative
predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%. Try the following approach:

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use $K$-fold CV to estimate the test error of the final model


---

```{r}
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K<-5
set.seed(123)
folds <- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred <- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

---

Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations. Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into $K$ cross-validation folds at random
2. For each fold $k = 1,\ldots,K$

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold $k$
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold $k$
    
    c. Use the classifier to predict the class labels for the observations in fold $k$
    
---

```{r}
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K<-5
set.seed(123)
folds <- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred <- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```


