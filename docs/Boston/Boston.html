<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Boston housing data
### Aldo Solari

---




# Model stacking

Stacking is a general method to combine models 

Consider a library of `\(L\)` models `\(\hat{f}_1,\ldots, \hat{f}_L\)` fitted on the training data
`$$(x_1,y_1),\ldots, (x_n,y_n)$$`


Perhaps by combining their respective efforts, we could get even better prediction than using any
particular one

The issue then how we might combine them for predicting the test set `\(y^*_1,\ldots,y^*_m\)`

A linear combination 
`$$\hat{y}^{*}_i = \sum_{l=1}^{L}w_l \hat{f}_l(x^*_i)$$`
requires to define the the weights `\(w_1,\ldots,w_L\)`

---

# Least squares

The method of least squares provides the weights 
`$$\hat{w}_1,\ldots,\hat{w}_L=  \underset{w_1,\ldots,w_L}{\arg \min\,\,} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}_l(x_i) \right]^2$$`

However, in this way we fail to take into account for model complexity: models with higher complexity get higher weights

For example, consider `\(L\)` predictors and let `\(\hat{f}_{l}\)` be the linear model formed the best subset of predictors of size `\(l\)`, `\(l=1,\ldots,L\)`, where best is defined as having the smallest `\(\mathrm{MSE}_{\mathrm{Tr}}\)`

Then all the weight goes on the largest model, that is, `\(\hat{w}_L = 1\)` and `\(\hat{w}_l = 0\)` for `\(l&lt; L\)`

---

# Stacked regression

Wolpert (1992) presented an interesting idea, called *stacked generalizations*.  This proposal was translated in statistical language by Breiman, in 1993.

If we exclude `\(y_i\)` in the fitting procedure of the models, then 
 `\(\hat{f}^{-i}_1(x_i),\ldots, \hat{f}^{-i}_L(x_i)\)` do not depend on `\(y_i\)` 

*Stacked regression* is an ensemble method with `\(\hat{f}_{\mathrm{stack}}=\)` linear model and cross-validation with `\(K=n\)`

---

# Staked regression algorithm

1. Let `\(\hat{f}^{-i}_l(x_i)\)` be the prediction at `\(x_i\)` using model `\(l\)` fitted to the training data with the
`\(i\)`th training observation `\((x_i,y_i)\)` removed

2. Obtain the weights by least squares
`$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$`

3. Compute the predictions for the test data as
`$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$`

---

layout: false
class: inverse, middle, center

# 20. Ensembles of models

---

# Libraries


```r
library(tidymodels)
tidymodels_prefer()
library(baguette)
library(stacks)
library(MASS)
```

---

# Boston data


```r
set.seed(123)
Boston &lt;- Boston %&gt;% mutate(medv = log(medv))
Boston_split &lt;- initial_split(Boston, strata = medv)
Boston_train &lt;- training(Boston_split)
Boston_test  &lt;- testing(Boston_split)

Boston_folds &lt;- vfold_cv(Boston_train, strata = medv)
```

The training and test data are 
`$$(x_1,y_1),\ldots,(x_n,y_n),\quad (x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$$`
with `\(n=378\)` and `\(m=128\)` for the Boston data set. 

The response variable is `medv`, and the
covariates are `crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat` 


---

# Models specification


```r
linear_reg_spec &lt;- 
  linear_reg() %&gt;% 
  set_engine("lm")

knn_spec &lt;- 
  nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;% 
  set_engine("kknn") %&gt;% 
  set_mode("regression")

cart_spec &lt;- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% 
  set_engine("rpart") %&gt;% 
  set_mode("regression")

bag_cart_spec &lt;- 
  bag_tree() %&gt;% 
  set_engine("rpart", times = 20L) %&gt;% 
  set_mode("regression")
```

---

# Workflow set


```r
# no_pre_proc
model_vars &lt;- 
  workflow_variables(outcomes = medv, 
                     predictors = everything())

no_pre_proc &lt;- 
  workflow_set(
    preproc = list(simple = model_vars), 
    models = list(linear_reg = linear_reg_spec, 
                  CART = cart_spec, 
                  CART_bagged = bag_cart_spec)
  )
```

---


```r
# normalized
normalized_rec &lt;- 
  recipe(medv ~ ., data = Boston_train) %&gt;% 
  step_normalize(all_predictors()) 

normalized &lt;- 
  workflow_set(
    preproc = list(normalized = normalized_rec), 
    models = list(KNN = knn_spec)
  )

# poly
poly_recipe &lt;- 
  normalized_rec %&gt;% 
  step_poly(lstat, rm, crim) 

with_poly &lt;- 
  workflow_set(
    preproc = list(poly = poly_recipe), 
    models = list(linear_reg = linear_reg_spec)
  )
```

---


```r
# all
all_workflows &lt;- 
  bind_rows(no_pre_proc, normalized, with_poly) %&gt;% 
  # Make the workflow ID's a little more simple: 
  mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

```
# A workflow set/tibble: 5 × 4
  wflow_id        info             option    result    
  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    
1 linear_reg      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;
2 CART            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;
3 CART_bagged     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;
4 KNN             &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;
5 poly_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;
```

---


```r
grid_ctrl &lt;-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )

library(tictoc)
tic()
grid_results &lt;- 
    all_workflows %&gt;% 
    workflow_map(seed = 123, 
                 resamples = Boston_folds, 
                 grid = 5, 
                 control = grid_ctrl, 
                 verbose = TRUE)
toc()
```

```
85.529 sec elapsed
```

---

# Evaluating the models


```r
grid_results %&gt;% 
  rank_results() %&gt;% 
  filter(.metric == "rmse") %&gt;% 
  select(model, .config, rmse = mean, rank)
```

```
# A tibble: 13 × 4
   model            .config               rmse  rank
   &lt;chr&gt;            &lt;chr&gt;                &lt;dbl&gt; &lt;int&gt;
 1 bag_tree         Preprocessor1_Model1 0.154     1
 2 nearest_neighbor Preprocessor1_Model5 0.175     2
 3 nearest_neighbor Preprocessor1_Model3 0.175     3
 4 nearest_neighbor Preprocessor1_Model1 0.175     4
 5 linear_reg       Preprocessor1_Model1 0.177     5
 6 nearest_neighbor Preprocessor1_Model2 0.183     6
 7 linear_reg       Preprocessor1_Model1 0.189     7
 8 decision_tree    Preprocessor1_Model2 0.197     8
 9 decision_tree    Preprocessor1_Model4 0.197     9
10 decision_tree    Preprocessor1_Model3 0.197    10
11 decision_tree    Preprocessor1_Model1 0.198    11
12 nearest_neighbor Preprocessor1_Model4 0.204    12
13 decision_tree    Preprocessor1_Model5 0.228    13
```

---

# Stacking models


```r
Boston_stack &lt;- stacks() %&gt;% 
  add_candidates(grid_results)

Boston_stack
```

```
# A data stack with 5 model definitions and 13 candidate members:
#   linear_reg: 1 model configuration
#   CART: 5 model configurations
#   CART_bagged: 1 model configuration
#   KNN: 5 model configurations
#   poly_linear_reg: 1 model configuration
# Outcome: medv (numeric)
```

---


```r
ens &lt;- blend_predictions(Boston_stack)
ens
```

```
# A tibble: 7 × 3
  member              type              weight
  &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;
1 CART_bagged_1_1     bag_tree         0.581  
2 KNN_1_2             nearest_neighbor 0.176  
3 KNN_1_5             nearest_neighbor 0.118  
4 poly_linear_reg_1_1 linear_reg       0.0618 
5 CART_1_4            decision_tree    0.0538 
6 KNN_1_4             nearest_neighbor 0.0313 
7 linear_reg_1_1      linear_reg       0.00442
```

---


```r
autoplot(ens)
```

![](Boston_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---


```r
autoplot(ens, "weights")
```

![](Boston_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---


```r
ens &lt;- fit_members(ens)

reg_metrics &lt;- metric_set(rmse, rsq)

ens_test_pred &lt;- 
  predict(ens, Boston_test) %&gt;% 
  bind_cols(Boston_test)

ens_test_pred %&gt;% 
  reg_metrics(medv, .pred)
```

```
# A tibble: 2 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard       0.150
2 rsq     standard       0.892
```

---


```r
member_preds &lt;- 
  Boston_test %&gt;%
  select(medv) %&gt;%
  bind_cols(predict(ens, Boston_test, members = TRUE))

map_dfr(member_preds, rmse, truth = medv, data = member_preds) %&gt;%
  mutate(member = colnames(member_preds))
```

```
# A tibble: 9 × 4
  .metric .estimator .estimate member             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              
1 rmse    standard       0     medv               
2 rmse    standard       0.150 .pred              
3 rmse    standard       0.207 linear_reg_1_1     
4 rmse    standard       0.229 CART_1_4           
5 rmse    standard       0.171 CART_bagged_1_1    
6 rmse    standard       0.206 KNN_1_4            
7 rmse    standard       0.179 KNN_1_2            
8 rmse    standard       0.183 KNN_1_5            
9 rmse    standard       0.179 poly_linear_reg_1_1
```

---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

See Introduction to Statistical Learning, section 6.5.3.

Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error.

If the full data set is used to perform the feature selection step, the cross-validation errors that we obtain will not be accurate estimates of the test error.

To choose among the models of different sizes using cross-validation, we perform feature selection within each iteration of the cross-validation.

Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation.

---

# ESL Chapter 7.10.2

Consider a scenario with `\(n = 50\)` samples in two equal-sized classes, and `\(p = 5000\)` quantitative
predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%. Try the following approach:

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use `\(K\)`-fold CV to estimate the test error of the final model


---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred &lt;- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.02
```

---

Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations. Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into `\(K\)` cross-validation folds at random
2. For each fold `\(k = 1,\ldots,K\)`

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold `\(k\)`
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold `\(k\)`
    
    c. Use the classifier to predict the class labels for the observations in fold `\(k\)`
    
---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred &lt;- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.56
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
