---
title: '**Random-X prediction error**'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval= T, message=F, warning=F, error=F, comment=NA, cache=T, R.options=list(width=220))
```



# Random-X setting

Earlier we have seen the Fixed-X setting, where predictor values are assumed to be nonrandom.
In most modern predictive modeling applications, it is more reasonable to take a Random-X view, where the predictor values (both those used in training and for future predictions) are random.

We assume 

* Response $Y$
* Predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$
* $(X,Y)$ have some unknown joint distribution
* Given $X=x$, we have
$$(Y|X=x) = f(x) + \varepsilon$$
where the conditional expectation $f(x)=\mathbb{E}(Y|X=x)$ is called the __regression function__ and $\varepsilon$ is the error term independent from $X$ with $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2$. Note that the conditional variance 
$\mathbb{V}\mathrm{ar}(Y|X=x) = \sigma^2$ is constant (__homoscedasticity__ assumption). 
* Training set: $n$ observations $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$
* Test set: $m$ observations $(x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$ i.i.d. from $(X,Y)$

Now consider a model building process that uses the training data to build a prediction function $\hat{f}$. The __Random-X prediction error__ is given by
$$\mathrm{ErrR}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}(Y^*_i - \hat{f}(X^*_i))^2\right] = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2]$$
where the last equality follows by exchangeability and the expectation of the last term is with respect to the training set $(X_1,Y_1),\ldots,(X_n,Y_n)$ and to the new test point $(X^*_1,Y^*_1)$. 

Recall that in the Fixed-X we have made the following two assumptions about the problem:

1. The predictor values $x_1,\ldots,x_n$ used in training are not random (e.g., designed), and the only randomness in training is due to the responses $y_1,\ldots,y_n$.

2. $x^*_1,\ldots,x^*_n$ exactly match $x_1,\ldots,x_n$

The Random-X setting drops both assumption. Compare it with the Fixed-X prediction error
$$\mathrm{ErrF}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}(Y^*_i - \hat{f}(x_i))^2\right]$$
where the expectation is with respect  $Y_1,\ldots,Y_n$ and $Y^*_1,\ldots,Y^*_n$. 

In the Random-X setting, the bias-variance decomposition is given by
$$\mathrm{ErrR}= \sigma^2 + \mathbb{E}\left\{ \left[\mathbb{E}(\hat{f}(x^*_1)|x_1,\ldots,x_n,x^*_1) - f(x_1^*)\right]^2 \right\} + \mathbb{E}\left\{\mathbb{V}\mathrm{ar}(\hat{f}(x^*_1)|x_1,\ldots,x_n,x^*_1)\right\} = \mathrm{Irreducible} + \mathrm{Bias}^2 + \mathrm{Variance}$$



# Random-X Mallows' Cp 

In the Random-X setting, the optimism is given by
$$\mathrm{OptR}= \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}})- \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2 - (Y_1 - \hat{f}(X_1))^2]$$
where the last expectation is with respect to to the training set $(X_1,Y_1),\ldots,(X_n,Y_n)$ and to the new test point $(X^*_1,Y^*_1)$.

Assume that $X = (X_1,\ldots,X_p)^\mathsf{T} \sim N(0,\Sigma)$ where $\Sigma$ is invertible and $p<n-1$. Assume also that $f(x)= x^\mathsf{T}\beta$, a linear function of $x$. Then, for the least squares regression estimator,
$$\mathrm{OptR}= \mathrm{OptF} + \frac{\sigma^2p}{n}\left( \frac{p+1}{n-p-1}\right)=  \frac{2\sigma^2p}{n} + \frac{\sigma^2p}{n}\left( \frac{p+1}{n-p-1}\right) = \frac{\sigma^2p}{n}\left(2 + \frac{p+1}{n-p-1}\right)$$
Note that the above conditions hold if we assume that $(X,Y)$ is jointly Gaussian, which implies the linear model to be unbiased.  


Under the these assumptions, we can estimate ErrR by
$$\widehat{\mathrm{ErrR}} = \mathrm{MSE}_{\mathrm{Tr}} + \frac{\sigma^2p}{n}\left(2 + \frac{p+1}{n-p-1}\right)$$

If we use  $\hat{\sigma}^2 = \mathrm{RSS}/(n-p)$ in place of $\sigma^2$, we obtain a Random-X version of Mallows' Cp 
$$\mathrm{RCp} = \mathrm{Cp} + \frac{\hat{\sigma}^2p}{n}\left( \frac{p+1}{n-p-1}\right) = \frac{\mathrm{RSS}(n-1)}{(n-p)(n-p-1)}$$

# Exercise

For example, suppose that 
$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
\mu_y \\
\mu_x \\
\end{array}\right), \left(\begin{array}{cc} 
\sigma^2_y & \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  & \sigma^2_x \\
\end{array}\right)\right)$$
so that the conditional distribution of $Y$ given $X=x$ is
$$(Y|X=x) \sim N\Big(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x-\mu_x), \sigma^2_y (1-\rho^2)\Big)$$
where $f(x)= \mathbb{E}(Y|X=x) = \left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right) + \left(\rho\frac{\sigma_y}{\sigma_x}\right)x = \alpha + \beta x$. This corresponds to generate the training data as follows:

For $i=1,\ldots,n$:

1. $x_i$ is the realization of $X_i \sim N(\mu_x,\sigma^2_x)$, the marginal distribution of $X$
2. $y_i$ is the realization of $(Y_i|X_i=x_i) = f(x_i) + \varepsilon_i$, the conditional distribution of $Y$ given $X=x$, where 
    - $f(x_i)=\alpha + \beta x_i$ with $\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)$ and $\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)$
    - $\varepsilon_i \sim N(0,\sigma^2)$ with $\sigma^2=\sigma^2_y (1-\rho^2)$

Analogously, the test data:

For $i=1,\ldots,m$:

1. $x^*_i$ is the realization of $X^*_i \sim N(\mu_x,\sigma^2_x)$ 
2. $y^*_i$ is the realization of $(Y^*_i|X^*_i=x^*_i) = f(x^*_i) + \varepsilon^*_i$ where 
    - $f(x^*_i)=\alpha + \beta x^*_i$
    - $\varepsilon^*_i \sim N(0,\sigma^2)$


Suppose that $\mu_x=\mu_y=0$, $\rho=0.5$, $\sigma_x=1$, $\sigma_y=2$, i.e. 
$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
0 \\
0 \\
\end{array}\right), \left(\begin{array}{cc} 
2^2 & 1 \\
1  & 1^2 \\
\end{array}\right)\right)$$
thus $X\sim N(0,1)$, $Y|X=x \sim N(\beta x,\sigma^2)$ with $\sigma^2=3$ and $\beta=1$.


Consider the estimator  
$$\hat{y^*_1} = \hat{f}(x^*_1) =  \hat{\beta} x_1^*$$
where the estimate of $\beta$ is given by
$$\displaystyle \hat{\beta} = \frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2 }$$
where $(x_1,y_1),\ldots,(x_n,y_n)$ is the training set with $n>2$. 
Note that $\mathbb{E}(\hat{\beta})=\beta$, thus theestimator $\mathbb{E}(\hat{f}(x))= f(x)$ is unbiased. 

Show that

1. ErrR conditional to the training set is given by
$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | (X_1,Y_1) = (x_1,y_1), \ldots, (X_n,Y_n) = (x_n,y_n)] = \sigma^2 + (\beta-\hat{\beta})^2$$

2. ErrR conditional to $X_1^*,X_1,\ldots,X_n$ is given by
$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | X^*_1 = x^*_1, X_1 = x_1, \ldots, X_n = x_n] = \sigma^2 + \sigma^2\left(\frac{(x_1^*)^2}{\sum_{i=1}^{n}x^2_i}\right)$$

3. ErrR is given by
$$\mathrm{ErrR} = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2] =  \sigma^2 + \frac{\sigma^2}{n-2}$$

Check 1., 2. and 3. by simulation with $n=20$.

```{r}
n = 20
beta = 1
sigma = sqrt(3)
set.seed(123)
# observed training set
x = rnorm(n)
y = beta*x + rnorm(n,0,sigma)
# estimate
hatbeta <- sum(x*y)/sum(x^2)
# observed test point
x1star = rnorm(1)
y1star = beta*x1star + rnorm(1,0,sigma)
# error
(y1star - hatbeta*x1star)^2
```

1st comparison

```{r}
# 1.
B = 5000
set.seed(123)
x1stars = rnorm(B,0,1)
y1stars = beta*x1stars + rnorm(B,0,sigma)
# empirical
mean( ( y1stars - hatbeta*x1stars )^2 )
# theoretical
sigma^2 + (beta - hatbeta)^2
```

2nd comparison

```{r}
# 2.
set.seed(123)
sim2 = function(n){
  xs = c(x,x1star)
  ys = xs*beta + rnorm(n+1,0,sigma)
  hatbeta <- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim2(n)))
# theoretical
sigma^2 + (sigma^2)*(x1star^2/sum(x^2))
```


3rd comparison

```{r}
# 3.
set.seed(123)
sim3 = function(n){
  xs = rnorm(n+1,0,1)
  ys = beta*xs + rnorm(n+1,0,sigma)
  hatbeta <- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim3(n)))
# theoretical
sigma^2 + (sigma^2)/(n-2)
```
