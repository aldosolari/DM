---
title: '**Bagging**'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval= T, message=F, warning=F, error=F, comment=NA, cache=T, R.options=list(width=220))
```

# Ensemble methods

Ensemble methods are techniques that create multiple
models and then combine them to produce improved results. 
These models, when used as inputs of ensemble methods,
are often called __base learners__ or __weak learners__. 
Ensemble learning is appealing because that it is able to
boost weak learners which are slightly better than random
guess to strong learners which can make very accurate
predictions. However, ensemble methods increases __computation time__ and
reduces __interpretability__

For example, classification and regression trees are simple and useful for interpretation. However they  are typically not competitive with other approaches in terms of prediction accuracy. We will see that ensemble methods such as __bagging__ and __random forests__ grow multiple trees which are then combined to yield a single prediction. Combining a large number of trees can often result in
dramatic improvements in prediction accuracy, at the
expense of some interpretation loss.


The primary disadvantage of trees is that they are rather
unstable (high variance). In other words, a small change in the data often results in a completely different tree. One major reason for this instability is that if a split changes, all the splits under it change as well, thereby propagating the variability. We will learn how to  control the variance, or __stabilize__ the predictions made by trees. In  doing so, we can greatly improve prediction accuracy but we suffer in terms of interpretability

### The boostrap 

The __bootstrap__ is an useful resampling tool in statistics. 
A bootstrap sample of size $n$ from the training data is
$$(\tilde{x}_1, \tilde{y}_1), (\tilde{x}_2, \tilde{y}_2), \ldots, (\tilde{x}_n, \tilde{y}_n)$$
where each $(\tilde{x}_i, \tilde{y}_i)$ are drawn from uniformly at random from $$(x_1, y_1), (x_2, y_2), \ldots, (x_n, x_n)$$ with __replacement__

Not all of the training points are represented in a bootstrap
sample, and some are represented more than once. The probability for one observation not to be drawn in one draw is $1 - \frac{1}{n}$

For large $n$, the probability for one observation not to be drawn in any of the $n$ draws is
$$\lim_{n\rightarrow \infty} \left(1- \frac{1}{n}\right)^n = \frac{1}{e} \approx 0.368$$

We can expect $\approx 1/3$ of the $n$ original observations to be __out-of-bag__ (OOB)


# Bagging trees 

Bootstrap AGGregation, or __bagging__, is a general procedure for reducing the variance of a model and it is particularly useful in the case of regression or classification trees

1. Generate $B$ different bootstrapped training sets
$$(\tilde{x}_1^b, \tilde{y}_1^b), (\tilde{x}_2^b, \tilde{y}_2^b), \ldots, (\tilde{x}_n^b, \tilde{y}_n^b), \qquad b=1,\ldots,B$$

2. Fit a regression tree $\hat{f}^{b}$ or a classification tree $\hat{c}^{b}$ for each bootstrapped training set (simple strategy: grow fairly large trees on each bootstrapped training set, with no pruning)

3. Average all the predictions:
$$\bar{f}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{b}(x)$$
for regression trees and
$$\bar{c}(x) = \mathrm{Mode}\{\hat{c}^{b}(x),b=1,\ldots,B\}$$
for classification trees (consensus)

## Bagging estimates of class probabilities

For a classification problem with $K$ classes, we may want to estimate the class probabilities out of our bagging procedure. What about using the proportion of votes that were for each class? This is generally not a good estimate.

Classification trees already gives us a set of
predicted class probabilities at $x$: $\hat{p}_k^b(x), k=1,\ldots,K$. These are simply the proportion of points in the appropriate region that
are in each class. 

The bagging estimates of class probabilities are given by
$$\bar{p}_k(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{p}_k^{b}(x), \quad k=1,\ldots,K$$
The final bagged classifier just chooses the class with the highest
probability. This form of bagging is preferred if it is desired to get estimates of
the class probabilities. Also, it can sometimes help the overall
prediction accuracy.


## Breiman (1996) Bagging predictors

Example from the original Breiman paper on bagging: comparing
the misclassification error of the CART tree (pruning performed by
cross-validation) and of the bagging classifier (with $B = 50$):

| Data set | CART | Bagging | Decrease |
|---|---|----|---|
| waveform | 29.1 | 19.3 | 34% |
| heart | 4.9 | 2.8 | 43% |
| breast cancer | 5.9 | 3.7 | 37% |
| ionosphere | 11.2 | 7.9 | 29% |
| diabetes | 25.3 | 23.9 | 6% |
| glass | 30.4 | 23.6 | 22% |
| soybean | 8.6 | 6.8 | 21% |

## Why is bagging working?

Why is bagging working? Here is a simplified setup with $K = 2$
classes "1" and "2" to help understand the basic phenomenon. 
Suppose that for a given $x$, we have an odd number $B$ of independent classifiers $\hat{c}^b(x), b=1,\ldots,B$, and each classifier has a misclassification rate of 40%. Assume without loss of generality that the true class at $x$ is "1", so
$$\mathrm{Pr}(\hat{c}^b(x) = 2 ) = 0.4$$
Let $Z = \sum_{b=1}^{B}I\{\hat{c}^b(x) = 2\}$ be the number of votes for class "2", and note that 
$$Z \sim \mathrm{Binomial}(B,0.4)$$
Therefore the misclassification rate of the bagged classifier is
$$\mathrm{Pr}(\bar{c}^b(x) = 2 ) = \mathrm{Pr}(Z \geq (B+1)/2)$$
which goes $\rightarrow 0$ as $B\rightarrow \infty$. In other words, the
bagged classifier has perfect predictive accuracy as the number of
sampled data sets $B\rightarrow \infty$

Of course, the caveat here is __independence__. The classifiers that we
use in practice, $\hat{c}^b$ are clearly not independent, because they
are fit on very similar data sets (bootstrap samples from the same
training set).

## Wisdom of crowds

The wisdom of crowds is a concept popularized outside of statistics
to describe the same phenomenon. It is the idea that the collection
of knowledge of an __independent__ group of people can exceed the
knowledge of any one person individually

Assume that we have an odd number $n>1$ of independent individuals, and each individual has probability $\pi$ of making a correct guess. 
The number of correct guesses is 
$$Z \sim \mathrm{Binomial}(n, \pi)$$
The majority vote makes the correct guess when $Z \geq z$ with $z=(n+1)/2$, i.e.
$$\mathrm{Pr}(Z \geq z) = \sum_{k=z}^{n}{n\choose k}\pi^{k}(1-\pi)^{n-k}$$
e.g. if $n=3$ and $\pi = 70\%$, then the majority vote makes the correct guess with probability $78.4\%$

---

```{r}
pi = seq(0,1,by=0.01)
n = 3
z = (n+1)/2
Pr = sapply(pi, function(p) sum(dbinom(z:n, size=n, prob=p) ))
plot(pi, Pr, type="l", xlab="Probability of being correct (individual)", ylab="Probability of being correct (group)")
```

## When will bagging fail?

Now suppose that we consider the same simplified setup as before
(independent classifiers), but each classifier has a misclassification
rate:
$$\mathrm{Pr}(\hat{c}^b(x) = 2 ) = 0.6$$

Then by the same arguments,  
$$\lim_{B\rightarrow \infty }\mathrm{Pr}(\bar{c}^b(x) = 2 )  \rightarrow 1$$
In other words, the bagged classifier is perfectly inaccurate as the number of bootstrapped data sets.

Again, the independence assumption doesnâ€™t hold with trees, but
the take-away message is clear: bagging a good classifier can
improve predictive accuracy, but bagging a bad one can seriously
degrade predictive accuracy.

## Disadvantages

It is important to discuss some disadvantages of bagging:

* __Loss of interpretability__: the final bagged classifier is not a
tree, and so we forfeit the clear interpretative ability of a
classification tree

* __Computational complexity__: we are essentially multiplying the
work of growing a single tree by $B$ (especially if we are using
the more involved implementation that prunes and validates
on the original training data)


## Out-of-bag error

Each bagged tree makes use of $\approx 2/3$ of the original observations. We can predict the response for the $i$th observation using
each of the bagged trees in which that observation was OOB. 

This yields $\approx B/3$ predictions for the $i$th observation, which
we average (probability or consensus). This estimate is essentially the LOOCV error for bagging, if $B$ is large


# Spam data

```{r}
#Load the data and split into training and test sets.
rm(list=ls())
spam <- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test = spam[spam$testid, -2]
n = nrow(train)
m = nrow(test)
```



## Accuracy and specificity

```{r}
# A function to calculate prediction accuracy and specificity 
score <- function(phat, truth, name="model") {
  ctable <- table(truth=truth,
                  yhat=(phat>0.5))
  accuracy <- sum(diag(ctable))/sum(ctable)
  specificity <- ctable[1,1]/sum(ctable[1,])
  data.frame(model=name, accuracy=accuracy, specificity=specificity)
}
```

## Null model

```{r}
# null model
phat0 = rep(mean(train$spam=="email"),m)
yhat0 = rep("email",m)
# confusion matrix
table(Predicted=yhat0, True=test$spam)
# score
score(phat0, test$spam, name="null model")
```

## Fit a classification tree

```{r}
library(rpart)
library(rpart.plot)
pnames <- setdiff(colnames(train),"spam")
fml <- as.formula(paste("spam", paste(pnames,collapse=' + '),sep=' ~ '))
fit1 <- rpart(fml, train); rpart.plot(fit1)
phat1 = predict(fit1, newdata=test)[,"spam"]
yhat1 = ifelse(phat1>.5,"spam","email")
# confusion matrix
table(pred=yhat1, truth=test$spam)
# score 
score(phat1, test$spam, name="tree")
```

## Bagging classification trees

```{r}
# Obtain B bootstrap samples
set.seed(123)
B <- 50
obs <- sapply(1:B, 
              function(b)
              sample(1:n, size=n, replace=T)
              )
# Train classification trees and return them in a list.
treelist <-lapply(1:B,
                  function(b)
                  rpart(fml, train[obs[,b],])
                  )

# predict.bag (probability)
predict.bag <- function(treelist, newdata) {
  phats <- sapply(1:length(treelist),
                  function(b)
                  predict(treelist[[b]], newdata=newdata)[,"spam"]
                  )
  pbar <- rowMeans(phats)
}
phat2 = predict.bag(treelist, newdata=test)
yhat2 = ifelse(phat2>.5,"spam","email")
# confusion matrix
table(pred=yhat2, truth=test$spam)
# score
score(phat2, test$spam, name="bagging")
```

## Out-of-bag estimate of error

```{r}
library(randomForest)
p = ncol(test)
fit <- randomForest(fml, data = train, ntree = B, mtry = p)
# out-of-bag estimate of error
plot(fit)
```