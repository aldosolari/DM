---
title: '**Bias-variance decomposition**'
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval= T, message=F, warning=F, error=F, 
                      comment=NA, cache=T, R.options=list(width=220))
```

# Notation

* Response $Y$
* Predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$
* $(X,Y)$ have some unknown joint distribution
* Model
$$Y = f(X) + \varepsilon$$
where $f(X)=\mathbb{E}(Y|X)$ is the regression function and $\varepsilon$ is the error term independent from $X$ with $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2$
* Observed training set: $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$
* Observed test set: $(x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$ i.i.d. from $(X,Y)$
* Vector of response (training set): 
$$\underset{n\times 1}{\mathbf{y}} = 
\left[
\begin{array}{c}
y_1   \\
\cdots\\
y_i  \\
\cdots\\
y_n \\
\end{array}\right]$$

* Design matrix (training set):
$$\underset{n\times p}{\mathbf{X}} = \left[
\begin{array}{cccccc}
x_{11}  & x_{12}  & \cdots   &  x_{1j}  & \cdots   &   x_{1p}  \\
x_{21}  & x_{22} & \cdots   &  x_{2j}  & \cdots   &   x_{2p}  \\
\cdots   & \cdots   &  \cdots & \cdots   &  \cdots  \\
x_{i1}  & x_{i2} & \cdots   &  x_{ij}& \cdots   & x_{ip}    \\
\cdots   & \cdots   &  \cdots  &  \cdots   &  \cdots\\
x_{n1}   & x_{n2} & \cdots   & x_{nj}    &  \cdots   &   x_{np}\\
\end{array}\right]$$







# Fixed-X setting

A statistical regression model seeks to describe the relationship between a response $Y$ and a
predictor vector $X=(X_1,\ldots,X_p)^\mathsf{T}$, based on training data comprised of paired observations $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$.
Many modern regression models are ultimately aimed at prediction: given a new predictor value $x^*$, we apply the model to predict the corresponding response value $y^*$.

A common assumption that is used in the estimation of prediction error is what we call a fixed-X assumption, where the training predictor values $x_1,\ldots,x_n$ are treated as fixed (i.e. non-random), as are the test predictor values $x^*_1,\ldots,x^*_n$,
which are also assumed to equal the training values, i.e. $x_i=x^*_i$.
This setting combines the following two assumptions about the problem:

1. The predictor values $x_1,\ldots,x_n$ used in training are not random (e.g., designed), and the only
randomness in training is due to the responses $y_1,\ldots,y_n$.

2. $x^*_1,\ldots,x^*_n$ exactly match $x_1,\ldots,x_n$

We assume that for the training data
$$Y_{i}=f(x_i)+\varepsilon_i, \quad i=1,\ldots,n$$
and for the test data
$$Y^*_{i}=f(x_i)+\varepsilon^*_i, \quad i=1,\ldots,n$$
where $f(x)=\mathbb{E}(Y|x)$ is some unknown regression function, 
the predictors $x_1,\ldots,x_n$ are fixed and the errors 
$\varepsilon_1,\ldots,\varepsilon_n$ and $\varepsilon^*_1,\ldots,\varepsilon^*_n$ are i.i.d. $N(0,\sigma^2)$. 


In the fixed-X setting, __prediction error__ $\mathrm{ErrF}$ is defined by
$$\mathrm{ErrF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}})=\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2\right]= \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[( Y^*_i - \hat{f}(x_i))^2\right]$$
 
 
# Bias-variance decomposition 

We start by considering a specific value $x_i$ for $x$. 
Suppose we want to predict $y^*_i$ by $\hat{y}_i^* = \hat{f}(x_i)$ where $\hat{f}$ is an estimate of $f$ based on the training data.
The accuracy of $\hat{y}^*_i$ as a prediction for $y^*_i$ depends on two quantities, which we call the __reducible error__ and the __irreducible error__. 
We have 
$$\mathbb{E}[(Y^*_{i} - \hat{Y}^*_i)^2] = \mathbb{E}[(f(x_i) + \varepsilon^*_i -  \hat{f}(x_i))^2] = \underbrace{\mathbb{E}[(f(x_i)  -  \hat{f}(x_i))^2]}_{\mathrm{Reducible}} + \underbrace{\mathbb{V}\mathrm{ar}(\varepsilon^*_i)}_{\mathrm{Irreducible}}$$

where $\mathbb{V}\mathrm{ar}(\varepsilon^*_i) = \sigma^2$. 
Note that, even if it were possible to form a perfect estimate for
$f$, so that our estimated response took the form $\hat{y}^*_i = f(x_i)$, our prediction would still have some error in it. This is because $Y_i^* = f(x_i) + \varepsilon_i^*$. 

The reducible error can be decomposed into (squared) __bias__ and __variance__ of the estimator $\hat{f}$, respectively:
$$\mathbb{E}[(f(x_i)  -  \hat{f}(x_i))^2] = \mathbb{E}[(f(x_i)  - \mathbb{E}\hat{f}(x_i) + \mathbb{E}\hat{f}(x_i) - \hat{f}(x_i))^2] =
\underbrace{ [\mathbb{E}\hat{f}(x_i) - f(x_i) ]^2}_{[\mathrm{Bias}(\hat{f}(x_i))]^2} + \underbrace{\mathbb{V}\mathrm{ar}[\hat{f}(x_i)]}_{\mathrm{Variance}(\hat{f}(x_i))}$$


Now the decomposition of fixed-X prediction error is simply

$$\mathrm{ErrF} =  \sigma^2 + \underbrace{\frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2}_{\mathrm{Bias}^2} + \underbrace{\frac{1}{n}\sum_{i=1}^{n} \mathbb{V}\mathrm{ar}(\hat{f}(x_i))}_{\mathrm{Variance}} = [\mathrm{Bias}(\hat{f})]^2+\mathrm{Variance}(\hat{f})$$

Bias and variance are conflicting entities, and we cannot
minimize both simultaneously. We must therefore choose a __trade-off__ between bias and variance.

# Simulation

For the yesterday-tomorrow data, the true $f(x)$ is

```{r}
# true regression function 
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
plot(x,ftrue, type="l", col=4)
```

Consider the polynomial regression of degree $d$.
We will perform a simulation to compute squared bias $[\mathbb{E}\hat{f}(x_i) - f(x_i) ]^2$ and variance $\mathbb{V}\mathrm{ar}[\hat{f}(x_i)]$. 
 

```{r}
# setting
sigmatrue=0.01
n = length(x)
d = 3
p = d+1
B = 100
# simulation function
sim = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
}
# results
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1)
lines(x,ftrue, col=4)
# expected value
Ave = apply(yhats,1,mean)
lines(x,Ave)
# squared bias
Bias2 = (ftrue - Ave)^2
# variance
Var = apply(yhats,1,var)
```

The bias-variance decomposition gives

```{r}
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```

Note that the expected value $\mathbb{E}\hat{f}(x_i)$ can be obtained by fitting the polynomial model using $y_i = f(x_i)$. This allows to compute the squared bias $[\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2$.

```{r}
# expected value 
Ehatf = fitted(lm(ftrue ~ poly(x,degree=d)))
plot(x,ftrue, type="l", col=4)
lines(x,Ehatf)
Bias2 = mean( (ftrue - Ehatf)^2 )
```


Theoretical results show that for the linear model $$\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2 p}{n}$$
Importantly, in the linear model the variance is unaffected by the form of $f(x)$. 

```{r}
# empirical variance
mean(Var)
# true variance
(sigmatrue^2)*p/n
```

Now we easily compute the decomposition of $\mathrm{ErrF}$ into squared bias and variance of the polynomial regression for all possible degrees from 0 to $n-1$ without using simulations. 

```{r}
ds = 1:10
ps = ds+1

Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
  )
Vars = ps*(sigmatrue^2)/n
Reds = Bias2s+Vars 
barplot(Reds, ylab="Reducible error", names.arg=ps)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
# best model size
ps[which.min(Reds)]
```

The decomposition of the prediction error into irredubile and reducible error:
```{r}
# prediction error
Irr = rep(sigmatrue^2,length(ps))
ErrFs = Reds + Irr
barplot(ErrFs, ylab="Prediction error", names.arg=ps)
barplot(Irr, add=T, col=1, names.arg=" ")
legend("topright", c("Reducible","Irreducible"), col=c("gray",1), pch=c(19,19))
```

Now we chack the correctness of our results by comparing the empirical prediction error (calculated by simulation) and the theoretical prediction error (calculated by formulas).

```{r}
sigmatrue=0.01
n = length(x)
B = 1000
# simulation function
sim2 = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
ystar = ftrue + rnorm(n,0,sigmatrue)
MSE.te = mean((ystar - yhat)^2)
} 
# MSE.te empirical
d = 5
mean(replicate(B,sim2(d)))
# MSE.te theoretical
Ehatf = fitted(lm(ftrue ~ poly(x,degree=d)))
Bias2 = mean( (ftrue - Ehatf)^2 )
p = d+1
Var = (sigmatrue^2)*p/n
Bias2 + Var + sigmatrue^2
```

