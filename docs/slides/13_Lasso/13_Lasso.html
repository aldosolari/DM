<!DOCTYPE html>
<html>
  <head>
    <title>Lasso and best subset selection</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lasso and best subset selection
### Aldo Solari

---





# Outline

* The three norms
* Lasso
* Best subset selection
* Prostate data
* Variable selection and cross-validation

---

# Three norms

* Let's consider three canonical choices: the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms
`$$\|\boldsymbol{\beta} \|_0 = \sum_{j=1}^{p}1\{\beta_j\neq 0\}, \quad \|\boldsymbol{\beta} \|_1 = \sum_{j=1}^{p} |\beta_j |, \quad \|\boldsymbol{\beta} \|_2 =\Big( \sum_{j=1}^{p} \beta_j^2 \Big)^{1/2}$$`

* Strictly speacking, "the `\(\ell_0\)` norm" is not a norm : it does not satisfy positive homogeneity, i.e. `\(\|a\boldsymbol{\beta} \|_0 \neq a \|\boldsymbol{\beta} \|_0\)`

* The three norms are special cases of the `\(\ell_q\)` norm:
`$$\| \boldsymbol{\beta} \|_{\ell_q} = (|\beta_1|^q + \ldots + |\beta_p|^q)^{1/q} = (\sum_{j=1}^{p}|\beta_j|^q)^{1/q}$$`

---

![](images/four.jpg)

---

# Penalized problems

In penalized form, the use of the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms gives rise to the problems

* Best subset selection
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_0$$`
* Lasso regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$`
*  Ridge regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_0$$`

with `\(\lambda\geq 0\)` the tuning parameter

---

# Constrained problems

In constrained form, the use of the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms gives rise to the problems

* Best subset selection
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$`
* Lasso regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$`
*  Ridge regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t$$`

where `\(k\)` and `\(t\)` are the tuning parameters

Note that it makes sense to restrict `\(k\)` to be an integer: in best subset selection, we want to find the "best" subset of predictors of size `\(k\)`, where "best" is in terms of the achieved training error

---

# Penalized and constrained problems

* The penalized and constrained problems with the `\(\ell_1\)` and `\(\ell_2\)` norms (lasso and ridge) are equivalent: for any `\(t\geq 0\)` and solution `\(\hat{\boldsymbol{\beta}}\)` of the constrained problem, there is a value `\(\lambda\geq 0\)` such that  `\(\hat{\boldsymbol{\beta}}\)` also solves the penalized problem, and vice versa

* The penalized and constrained problems with the  `\(\ell_0\)` norm (best subset selection) are not equivalent: for every value of `\(\lambda \geq 0\)` and solution `\(\hat{\boldsymbol{\beta}}\)` of the penalized problem, there is a value of `\(k\)` such that `\(\hat{\boldsymbol{\beta}}\)` solves the constrained problem, but the converse is not true

---

# Convexity

* The lasso and ridge regression problems have a very important property: they are __convex optimization problems__

* It is convexity that allows to equate the penalized and constrained problems

* It is also convexity that allows to efficiently obtain the lasso and ridge regression solutions

* Why is a convex optimization problem so special? The short answer: because any local minimizer is a global minimizer

* Best subset selection is not convex. It is known to be __NP-hard__, in some sense the worst kind of nonconvex problem

---


# Sparsity

* Sparsity is the assumption that only a "small" number of predictors have an effect, i.e. have `\(\beta_j \neq 0\)`

*  We would like our estimator `\(\hat{\boldsymbol{\beta}}\)` to be __sparse__, meaning that most `\(\hat{\beta}_j\)` are zero

* The ridge regression estimator is not sparse

* The best subset selection estimator and the lasso estimator are sparse

* Why does the `\(\ell_1\)` norm induces sparsity and not the `\(\ell_2\)` norm? Look at the lasso and ridge constraint sets (ISLR, Figure 6.7)

---

![](images/constraints.jpg)

---

layout: false
class: inverse, middle, center

# Lasso

---


# Lasso regression

* Solve the penalized problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$`
or, equivalently, the constrained problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$`

* The solution is known as LASSO: Least Absolute Shrinkage and
Selection Operator (Tibshirani, 1996)

* Shrinkage: `\(\hat{\boldsymbol{\beta}}^\lambda\)` is the lasso shrunken estimate

* Selection: `\(\hat{S}^{\lambda} = \{X_j: \hat{\beta}^{\lambda}_j \neq 0\}\)` is the set of selected predictors

---

Figure 2.1 in Hastie, Tibshirani, Wainwright (2015) [Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity/) 

![](images/lassoridge.jpg)

---

# The one standard error rule

* Usually we choose the tuning parameter `\(\lambda\)` that minimize the CV error

* This rule often ends up selecting models that are larger than desiderable for interpretation purposes

* We can achieve smaller, simpler models with comparable predictive performance by using a simple device called the __one standard error rule__

* We can compute cross-validation "standard errors" as
`$$\mathrm{SE}(\mathrm{CVErr}) = \frac{1}{\sqrt{K}}\mathrm{sd}(\mathrm{Err}^{-1},\ldots,\mathrm{Err}^{-K})$$`
where `\(\mathrm{Err}^{-k}\)` denotes the error incurred in predicting the observations in the `\(k\)` hold-out fold, `\(k=1,\ldots,K\)`

* The one standard error rule choooses the tuning parameter value corresponding to the simplest model whose CV error is within one standard error of the minimum

---

![](images/oneserule.jpg)

---

layout: false
class: inverse, middle, center

# Best subset selection

---

# Best subset selection

* Solve the constrained problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$`

* The solution is the best subset of predictors of size `\(k\)`, known as __best subset selection__

* The problem is nonconvex (NP-hard): it requires
searching through all `\({p \choose k}\)` subsets of predictors

---

# Algorithm

Set `\(B_0\)` as the null model (only intercept)

For `\(k=1,\ldots,p\)`:


1. Fit all `\({ p \choose k }\)` models that contain exactly `\(k\)` predictors

2. Pick the "best" among these `\({ p \choose k }\)` models, and call it `\(B_k\)`, where "best" is defined having the smallest residual sum of squares RSS = `\(n \mathrm{MSE}_{\mathrm{Tr}}\)`

Select a single best model from among `\(B_0,B_1,\ldots,B_p\)` using AIC, BIC, Cross-Validation, etc.


---

![](images/bestsubset.jpg)

---

# Backward stepwise selection


* __Greedy algorithm__ sub-optimal to best subset selection but computationally efficient
* Applicable only when `\(n&gt;p\)`

__Algorithm__

Set `\(S_p\)` as the full model (all `\(p\)` predictors)

For `\(k=p,p-1,\ldots,1\)`:

1. Consider all `\(k\)` models that contain all but one of the predictors in `\(S_k\)`, for a total of `\(k-1\)` predictors

2. Choose the "best" among these `\(k\)` models and call it `\(S_{k-1}\)`, where "best" is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,\ldots,S_p\)` using AIC, BIC, cross-validation, etc.

---

# Forward stepwise selection


* Greedy algorithm sub-optimal to best subset selection but computationally efficient
* Applicable also when `\(p&gt;n\)` to construct the sequence `\(S_0, S_1,\ldots,S_{n-1}\)`

__Algorithm__

Set `\(S_0\)` as the null model (only intercept)

For `\(k=0,\ldots,\min(n-1,p-1)\)`:


1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor

2. Choose the \emph{best} among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where \emph{best} is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,S_2, \ldots\)` using AIC, BIC, cross-validation, etc.


---

# Forward with AIC-based stopping rule

Set `\(S_0\)` as the null model and `\(k=0\)`

1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor

2. Choose the "best" among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where "best" is defined having the smallest AIC

3. If AIC( `\(S_{k+1}\)` ) `\(&lt;\)` AIC( `\(S_k\)` ), set `\(k=k+1\)` and go to 1., otherwise STOP

---

layout: false
class: inverse, middle, center

# Prostate data

---

# Prostate data

* The response variable `lpsa` is the log prostate specific antigen level of men who had surgery for prostate cancer

* Predictors:
    - `lcavol` : log cancer volume
    - `lweight` : log prostate weight
    - `age` : in years
    - `lbph` : log of the amount of benign prostatic hyperplasia
    - `svi` : seminal vesicle invasion
    - `lcp` : log of capsular penetration
    - `gleason` : a numeric vector
    - `pgg45` : percent of Gleason score 4 or 5

* The data are split into training set and test set with
`\(n=67\)` and `\(m=30\)` observations, respectively

---


```r
rm(list=ls())
# load the data and divide in train/test
require(ElemStatLearn)
train = prostate[prostate$train,-10]
test = prostate[!prostate$train,-10]
X = as.matrix(train[,-9])
y = train$lpsa
n = nrow(X)
p = ncol(X)
X.star = as.matrix(test[,-9])
y.star = test$lpsa
m = nrow(X.star)
```

---


```r
# Full model
fit.full = lm(lpsa ~ ., train)
err.full = mean( (predict(fit.full, newdata=test) - test$lpsa )^2 )
err.full
```

```
[1] 0.521274
```

---


```r
# Ridge regression
require(glmnet)
K = 5
set.seed(123)
ridge.cv&lt;-cv.glmnet(X,y,alpha=0, nfolds = K, grouped=FALSE)
hatlambda &lt;-ridge.cv$lambda.min
yhat.ridge = predict(ridge.cv, s=hatlambda, newx=X.star, exact=TRUE)
err.ridge = mean( (yhat.ridge - test$lpsa)^2 )
err.ridge
```

```
[1] 0.4921148
```


---


```r
# LASSO
fit.lasso &lt;- glmnet(X, y, alpha=1)
plot(fit.lasso, xvar="lambda")
```

![](13_Lasso_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---


```r
# The one-standard error rule
set.seed(123)
cv.lasso &lt;-cv.glmnet(X, y, alpha=1, nfolds = K)
hatlambda&lt;-cv.lasso$lambda.1se
predict(fit.lasso, s=hatlambda, type ="coefficients")
```

```
9 x 1 sparse Matrix of class "dgCMatrix"
                       1
(Intercept) 0.3311283595
lcavol      0.4532150483
lweight     0.4040963451
age         .           
lbph        0.0085235150
svi         0.2449894443
lcp         .           
gleason     .           
pgg45       0.0001967525
```

```r
# predicted values
yhat.lasso = predict(fit.lasso, s=hatlambda, newx=X.star, exact=T)
err.lasso = mean( (yhat.lasso - test$lpsa)^2 )
err.lasso
```

```
[1] 0.4731328
```

---


```r
# Best subset
require(leaps)
fit.bests &lt;- regsubsets(lpsa~.,train)
summary.bests&lt;-summary(fit.bests)
summary.bests
```

```
Subset selection object
Call: regsubsets.formula(lpsa ~ ., train)
8 Variables  (and intercept)
        Forced in Forced out
lcavol      FALSE      FALSE
lweight     FALSE      FALSE
age         FALSE      FALSE
lbph        FALSE      FALSE
svi         FALSE      FALSE
lcp         FALSE      FALSE
gleason     FALSE      FALSE
pgg45       FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive
         lcavol lweight age lbph svi lcp gleason pgg45
1  ( 1 ) "*"    " "     " " " "  " " " " " "     " "  
2  ( 1 ) "*"    "*"     " " " "  " " " " " "     " "  
3  ( 1 ) "*"    "*"     " " " "  "*" " " " "     " "  
4  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     " "  
5  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     "*"  
6  ( 1 ) "*"    "*"     " " "*"  "*" "*" " "     "*"  
7  ( 1 ) "*"    "*"     "*" "*"  "*" "*" " "     "*"  
8  ( 1 ) "*"    "*"     "*" "*"  "*" "*" "*"     "*"  
```

---


```r
# Best Cp
plot(summary.bests$cp, xlab="k", ylab="Cp", type="b")
```

![](13_Lasso_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---


```r
# best BIC
plot(fit.bests, scale="bic")
```

![](13_Lasso_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---


```r
# function predict for regsubsets
predict.regsubsets =function(object ,newdata ,id ,...){
 form=as.formula(object$call[[2]])
 mat=model.matrix(form, newdata)
 coefi =coef(object, id=id)
 xvars =names(coefi)
 mat[,xvars]%*%coefi
}

yhat.bestBIC = predict.regsubsets(fit.bests, newdata=test, id=2)
err.BIC = mean( (yhat.bestBIC - test$lpsa)^2 )
err.BIC
```

```
[1] 0.4924823
```

---


```r
# Forward with AIC stopping rule
fit.null = lm(lpsa ~ 1, train)
fit.fwdAIC = step(fit.null, scope=list(upper=fit.full), direction="forward", k=2, trace=0)
summary(fit.fwdAIC)$coeff
```

```
              Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) -0.3259212 0.77997661 -0.4178603 6.774930e-01
lcavol       0.5055209 0.09256330  5.4613531 8.854996e-07
lweight      0.5388292 0.22071263  2.4413156 1.750193e-02
svi          0.6718487 0.27322793  2.4589311 1.674022e-02
lbph         0.1400111 0.07041152  1.9884688 5.117869e-02
```

```r
yhat.fwdAIC = predict(fit.fwdAIC, newdata=test)
err.fwdAIC = mean( (yhat.fwdAIC - test$lpsa)^2 )
err.fwdAIC
```

```
[1] 0.4563321
```



---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

* Consider the following approach: best subset selection __outside__ cross-validation

1. Perform best subset selection and obtain `\(B_k, k=0,1,\ldots,p\)`

2. Use cross-validation to select `\(k\)` and
to estimate the prediction error of the final model

* Here the training set is being used for three purposes:

1. predictor selection
2. model fitting 
3. performance evaluation

* Unless the number of observations is large, especially in relation to the number of predictors, one training set may not be able to fulfill these needs

* To obtain accurate estimates of the test
error, we must use only the training observations to perform all aspects of
model-fitting, including variable selection.

---

# Variable selection inside cross-validation

* Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error

* In order to incorporate the variation due to variable selection, it is wiser to perform best subset selection __inside__ cross-validation

* Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation

* When conducting a search for a subset of variables, it is important to realize that there may not be a unique set of predictors that will produce the best performance

---


```r
# K-fold Cross-Validation
set.seed(123)
folds = sample(1:K, n, replace =TRUE)
KCV = matrix(NA, K, p)
for (k in 1:K){
  fit_k = regsubsets(lpsa ~.,data=train[folds!=k,])
  for (j in 1:p){
    yhat_k=predict(fit_k, train[folds==k,], id=j)
    KCV[k,j]=mean( (train$lpsa[folds==k]-yhat_k)^2 )
  }
}
```

---


```r
plot(1:p,apply(KCV,2,mean), type="b")
```

![](13_Lasso_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---


```r
library(caret)
set.seed(123)
ctrl &lt;- rfeControl(functions = lmFuncs,
                   number = K,
                   method = "repeatedcv",
                   repeats = 1,
                   verbose = FALSE)
subsets &lt;- 1:p
lmProfile &lt;- rfe(X, y,
                 sizes = subsets,
                 rfeControl = ctrl)
predictors(lmProfile)
```

```
[1] "svi"     "lweight" "lcavol"  "lcp"     "lbph"   
```

---


```r
plot(lmProfile, type = c("g", "o"))
```

![](13_Lasso_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

```r
yhat.CV = predict(lmProfile$fit, newdata=test)
err.CV = mean( (yhat.CV - test$lpsa)^2 )
err.CV
```

```
[1] 0.4744146
```

---

# ESL Chapter 7.10.2

* Consider a classification problem with a large number of predictors 

* A typical strategy for analysis might be as follows:

1. Screen the predictors: find a subset of "good" predictors that show
fairly strong (univariate) correlation with the class labels

2. Using just this subset of predictors, build a multivariate classifier.

3. Use cross-validation to estimate the unknown tuning parameters and
to estimate the prediction error of the final model

* Is this a correct application of cross-validation? 

---

# Simulation

* Consider a scenario with `\(n = 50\)` samples in two equal-sized classes, and `\(p = 5000\)` quantitative
predictors (standard Gaussian) that are independent of the class labels

* The true (test) error rate of any classifier is 50%. 

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use `\(K\)`-fold CV to estimate the test error of the final model

* Over 50 simulations from this
setting, the average CV error rate was 3%. This is far lower than the true
error rate of 50%. What has happened?


---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred &lt;- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.02
```

---

* The problem is that the predictors have an unfair
advantage, as they were chosen in step 1. on the basis of all of the observations

* Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations

* Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into `\(K\)` cross-validation folds at random.
2. For each fold `\(k = 1,\ldots,K\)`

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold `\(k\)`
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold `\(k\)`
    
    c. Use the classifier to predict the class labels for the observations in fold `\(k\)`
    
* The error estimates from step 2.c are then accumulated over all `\(K\)` folds, to produce the cross-validation estimate of prediction error

---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred &lt;- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.58
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
