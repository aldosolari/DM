---
title: "Lasso and best subset selection"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Outline

* The three norms
* Lasso
* Best subset selection
* Prostate data
* Variable selection and cross-validation

---

# Three norms

* Let's consider three canonical choices: the $\ell_0$
, $\ell_1$ and $\ell_2$ norms
$$\|\boldsymbol{\beta} \|_0 = \sum_{j=1}^{p}1\{\beta_j\neq 0\}, \quad \|\boldsymbol{\beta} \|_1 = \sum_{j=1}^{p} |\beta_j |, \quad \|\boldsymbol{\beta} \|_2 =\Big( \sum_{j=1}^{p} \beta_j^2 \Big)^{1/2}$$

* Strictly speacking, "the $\ell_0$ norm" is not a norm : it does not satisfy positive homogeneity, i.e. $\|a\boldsymbol{\beta} \|_0 \neq a \|\boldsymbol{\beta} \|_0$

* The three norms are special cases of the $\ell_q$ norm:
$$\| \boldsymbol{\beta} \|_{\ell_q} = (|\beta_1|^q + \ldots + |\beta_p|^q)^{1/q} = (\sum_{j=1}^{p}|\beta_j|^q)^{1/q}$$

---

![](images/four.jpg)

---

# Penalized problems

In penalized form, the use of the $\ell_0$
, $\ell_1$ and $\ell_2$ norms gives rise to the problems

* Best subset selection
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_0$$
* Lasso regression
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$
*  Ridge regression
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_0$$

with $\lambda\geq 0$ the tuning parameter

---

# Constrained problems

In constrained form, the use of the $\ell_0$
, $\ell_1$ and $\ell_2$ norms gives rise to the problems

* Best subset selection
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$
* Lasso regression
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$
*  Ridge regression
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t$$

where $k$ and $t$ are the tuning parameters

Note that it makes sense to restrict $k$ to be an integer: in best subset selection, we want to find the "best" subset of predictors of size $k$, where "best" is in terms of the achieved training error

---

# Penalized and constrained problems

* The penalized and constrained problems with the $\ell_1$ and $\ell_2$ norms (lasso and ridge) are equivalent: for any $t\geq 0$ and solution $\hat{\boldsymbol{\beta}}$ of the constrained problem, there is a value $\lambda\geq 0$ such that  $\hat{\boldsymbol{\beta}}$ also solves the penalized problem, and vice versa

* The penalized and constrained problems with the  $\ell_0$ norm (best subset selection) are not equivalent: for every value of $\lambda \geq 0$ and solution $\hat{\boldsymbol{\beta}}$ of the penalized problem, there is a value of $k$ such that $\hat{\boldsymbol{\beta}}$ solves the constrained problem, but the converse is not true

---

# Convexity

* The lasso and ridge regression problems have a very important property: they are __convex optimization problems__

* It is convexity that allows to equate the penalized and constrained problems

* It is also convexity that allows to efficiently obtain the lasso and ridge regression solutions

* Why is a convex optimization problem so special? The short answer: because any local minimizer is a global minimizer

* Best subset selection is not convex. It is known to be __NP-hard__, in some sense the worst kind of nonconvex problem

---


# Sparsity

* Sparsity is the assumption that only a "small" number of predictors have an effect, i.e. have $\beta_j \neq 0$

*  We would like our estimator $\hat{\boldsymbol{\beta}}$ to be __sparse__, meaning that most $\hat{\beta}_j$ are zero

* The ridge regression estimator is not sparse

* The best subset selection estimator and the lasso estimator are sparse

* Why does the $\ell_1$ norm induces sparsity and not the $\ell_2$ norm? Look at the lasso and ridge constraint sets (ISLR, Figure 6.7)

---

![](images/constraints.jpg)

---

layout: false
class: inverse, middle, center

# Lasso

---


# Lasso regression

* Solve the penalized problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$
or, equivalently, the constrained problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$

* The solution is known as LASSO: Least Absolute Shrinkage and
Selection Operator (Tibshirani, 1996)

* Shrinkage: $\hat{\boldsymbol{\beta}}^\lambda$ is the lasso shrunken estimate

* Selection: $\hat{S}^{\lambda} = \{X_j: \hat{\beta}^{\lambda}_j \neq 0\}$ is the set of selected predictors

---

Figure 2.1 in Hastie, Tibshirani, Wainwright (2015) [Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity/) 

![](images/lassoridge.jpg)

---

# The one standard error rule

* Usually we choose the tuning parameter $\lambda$ that minimize the CV error

* This rule often ends up selecting models that are larger than desiderable for interpretation purposes

* We can achieve smaller, simpler models with comparable predictive performance by using a simple device called the __one standard error rule__

* We can compute cross-validation "standard errors" as
$$\mathrm{SE}(\mathrm{CVErr}) = \frac{1}{\sqrt{K}}\mathrm{sd}(\mathrm{Err}^{-1},\ldots,\mathrm{Err}^{-K})$$
where $\mathrm{Err}^{-k}$ denotes the error incurred in predicting the observations in the $k$ hold-out fold, $k=1,\ldots,K$

* The one standard error rule choooses the tuning parameter value corresponding to the simplest model whose CV error is within one standard error of the minimum

---

![](images/oneserule.jpg)

---

layout: false
class: inverse, middle, center

# Best subset selection

---

# Best subset selection

* Solve the constrained problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$

* The solution is the best subset of predictors of size $k$, known as __best subset selection__

* The problem is nonconvex (NP-hard): it requires
searching through all ${p \choose k}$ subsets of predictors

---

# Algorithm

Set $B_0$ as the null model (only intercept)

For $k=1,\ldots,p$:


1. Fit all ${ p \choose k }$ models that contain exactly $k$ predictors

2. Pick the "best" among these ${ p \choose k }$ models, and call it $B_k$, where "best" is defined having the smallest residual sum of squares RSS = $n \mathrm{MSE}_{\mathrm{Tr}}$

Select a single best model from among $B_0,B_1,\ldots,B_p$ using AIC, BIC, Cross-Validation, etc.


---

![](images/bestsubset.jpg)

---

# Backward stepwise selection


* __Greedy algorithm__ sub-optimal to best subset selection but computationally efficient
* Applicable only when $n>p$

__Algorithm__

Set $S_p$ as the full model (all $p$ predictors)

For $k=p,p-1,\ldots,1$:

1. Consider all $k$ models that contain all but one of the predictors in $S_k$, for a total of $k-1$ predictors

2. Choose the "best" among these $k$ models and call it $S_{k-1}$, where "best" is defined having the smallest RSS

Select a single best model from among $S_0,S_1,\ldots,S_p$ using AIC, BIC, cross-validation, etc.

---

# Forward stepwise selection


* Greedy algorithm sub-optimal to best subset selection but computationally efficient
* Applicable also when $p>n$ to construct the sequence $S_0, S_1,\ldots,S_{n-1}$

__Algorithm__

Set $S_0$ as the null model (only intercept)

For $k=0,\ldots,\min(n-1,p-1)$:


1. Consider all $p-k$ models that augment the predictors in $S_k$ with one additional predictor

2. Choose the \emph{best} among these $p-k$ models and call it $S_{k+1}$, where \emph{best} is defined having the smallest RSS

Select a single best model from among $S_0,S_1,S_2, \ldots$ using AIC, BIC, cross-validation, etc.


---

# Forward with AIC-based stopping rule

Set $S_0$ as the null model and $k=0$

1. Consider all $p-k$ models that augment the predictors in $S_k$ with one additional predictor

2. Choose the "best" among these $p-k$ models and call it $S_{k+1}$, where "best" is defined having the smallest AIC

3. If AIC( $S_{k+1}$ ) $<$ AIC( $S_k$ ), set $k=k+1$ and go to 1., otherwise STOP

---

layout: false
class: inverse, middle, center

# Prostate data

---

# Prostate data

* The response variable `lpsa` is the log prostate specific antigen level of men who had surgery for prostate cancer

* Predictors:
    - `lcavol` : log cancer volume
    - `lweight` : log prostate weight
    - `age` : in years
    - `lbph` : log of the amount of benign prostatic hyperplasia
    - `svi` : seminal vesicle invasion
    - `lcp` : log of capsular penetration
    - `gleason` : a numeric vector
    - `pgg45` : percent of Gleason score 4 or 5

* The data are split into training set and test set with
$n=67$ and $m=30$ observations, respectively

---

```{r}
rm(list=ls())
# load the data and divide in train/test
require(ElemStatLearn)
train = prostate[prostate$train,-10]
test = prostate[!prostate$train,-10]
X = as.matrix(train[,-9])
y = train$lpsa
n = nrow(X)
p = ncol(X)
X.star = as.matrix(test[,-9])
y.star = test$lpsa
m = nrow(X.star)
```

---

```{r}
# Full model
fit.full = lm(lpsa ~ ., train)
err.full = mean( (predict(fit.full, newdata=test) - test$lpsa )^2 )
err.full
```

---

```{r}
# Ridge regression
require(glmnet)
K = 5
set.seed(123)
ridge.cv<-cv.glmnet(X,y,alpha=0, nfolds = K, grouped=FALSE)
hatlambda <-ridge.cv$lambda.min
yhat.ridge = predict(ridge.cv, s=hatlambda, newx=X.star, exact=TRUE)
err.ridge = mean( (yhat.ridge - test$lpsa)^2 )
err.ridge
```


---

```{r}
# LASSO
fit.lasso <- glmnet(X, y, alpha=1)
plot(fit.lasso, xvar="lambda")
```

---

```{r}
# The one-standard error rule
set.seed(123)
cv.lasso <-cv.glmnet(X, y, alpha=1, nfolds = K)
hatlambda<-cv.lasso$lambda.1se
predict(fit.lasso, s=hatlambda, type ="coefficients")
# predicted values
yhat.lasso = predict(fit.lasso, s=hatlambda, newx=X.star, exact=T)
err.lasso = mean( (yhat.lasso - test$lpsa)^2 )
err.lasso
```

---

```{r}
# Best subset
require(leaps)
fit.bests <- regsubsets(lpsa~.,train)
summary.bests<-summary(fit.bests)
summary.bests
```

---

```{r}
# Best Cp
plot(summary.bests$cp, xlab="k", ylab="Cp", type="b")
```

---

```{r}
# best BIC
plot(fit.bests, scale="bic")
```

---

```{r}
# function predict for regsubsets
predict.regsubsets =function(object ,newdata ,id ,...){
 form=as.formula(object$call[[2]])
 mat=model.matrix(form, newdata)
 coefi =coef(object, id=id)
 xvars =names(coefi)
 mat[,xvars]%*%coefi
}

yhat.bestBIC = predict.regsubsets(fit.bests, newdata=test, id=2)
err.BIC = mean( (yhat.bestBIC - test$lpsa)^2 )
err.BIC
```

---

```{r}
# Forward with AIC stopping rule
fit.null = lm(lpsa ~ 1, train)
fit.fwdAIC = step(fit.null, scope=list(upper=fit.full), direction="forward", k=2, trace=0)
summary(fit.fwdAIC)$coeff
yhat.fwdAIC = predict(fit.fwdAIC, newdata=test)
err.fwdAIC = mean( (yhat.fwdAIC - test$lpsa)^2 )
err.fwdAIC
```



---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

* Consider the following approach: best subset selection __outside__ cross-validation

1. Perform best subset selection and obtain $B_k, k=0,1,\ldots,p$

2. Use cross-validation to select $k$ and
to estimate the prediction error of the final model

* Here the training set is being used for three purposes:

1. predictor selection
2. model fitting 
3. performance evaluation

* Unless the number of observations is large, especially in relation to the number of predictors, one training set may not be able to fulfill these needs

* To obtain accurate estimates of the test
error, we must use only the training observations to perform all aspects of
model-fitting, including variable selection.

---

# Variable selection inside cross-validation

* Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error

* In order to incorporate the variation due to variable selection, it is wiser to perform best subset selection __inside__ cross-validation

* Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation

* When conducting a search for a subset of variables, it is important to realize that there may not be a unique set of predictors that will produce the best performance

---

```{r}
# K-fold Cross-Validation
set.seed(123)
folds = sample(1:K, n, replace =TRUE)
KCV = matrix(NA, K, p)
for (k in 1:K){
  fit_k = regsubsets(lpsa ~.,data=train[folds!=k,])
  for (j in 1:p){
    yhat_k=predict(fit_k, train[folds==k,], id=j)
    KCV[k,j]=mean( (train$lpsa[folds==k]-yhat_k)^2 )
  }
}
```

---

```{r}
plot(1:p,apply(KCV,2,mean), type="b")
```

---

```{r}
library(caret)
set.seed(123)
ctrl <- rfeControl(functions = lmFuncs,
                   number = K,
                   method = "repeatedcv",
                   repeats = 1,
                   verbose = FALSE)
subsets <- 1:p
lmProfile <- rfe(X, y,
                 sizes = subsets,
                 rfeControl = ctrl)
predictors(lmProfile)
```

---

```{r}
plot(lmProfile, type = c("g", "o"))
yhat.CV = predict(lmProfile$fit, newdata=test)
err.CV = mean( (yhat.CV - test$lpsa)^2 )
err.CV
```

---

# ESL Chapter 7.10.2

* Consider a classification problem with a large number of predictors 

* A typical strategy for analysis might be as follows:

1. Screen the predictors: find a subset of "good" predictors that show
fairly strong (univariate) correlation with the class labels

2. Using just this subset of predictors, build a multivariate classifier.

3. Use cross-validation to estimate the unknown tuning parameters and
to estimate the prediction error of the final model

* Is this a correct application of cross-validation? 

---

# Simulation

* Consider a scenario with $n = 50$ samples in two equal-sized classes, and $p = 5000$ quantitative
predictors (standard Gaussian) that are independent of the class labels

* The true (test) error rate of any classifier is 50%. 

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use $K$-fold CV to estimate the test error of the final model

* Over 50 simulations from this
setting, the average CV error rate was 3%. This is far lower than the true
error rate of 50%. What has happened?


---

```{r}
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K<-5
set.seed(123)
folds <- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred <- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

---

* The problem is that the predictors have an unfair
advantage, as they were chosen in step 1. on the basis of all of the observations

* Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations

* Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into $K$ cross-validation folds at random.
2. For each fold $k = 1,\ldots,K$

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold $k$
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold $k$
    
    c. Use the classifier to predict the class labels for the observations in fold $k$
    
* The error estimates from step 2.c are then accumulated over all $K$ folds, to produce the cross-validation estimate of prediction error

---

```{r}
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K<-5
set.seed(123)
folds <- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred <- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```
