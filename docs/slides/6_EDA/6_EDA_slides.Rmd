---
title: "Exploratory data analysis and the modeling process"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Outline

* Exploratory data analysis
* The modeling process
* Important concepts
* Car Data
* Expanded Car Data

---
layout: false
class: inverse, middle, center

# Exploratory data analysis

---

# Exploratory data analysis

* The use of __visualisation__,  __transformation__ and __modeling__ to explore your data is a task that statisticians call __exploratory data analysis__, or __EDA__ for short

* Your goal during EDA is to develop an understanding of your data

* EDA is an iterative cycle:
<center>
![](images/data-science-explore.png)
</center>
.center[Figure from r4ds, section 1.1]

* EDA is not a formal process with a strict set of rules. EDA is fundamentally a creative process


* Exploratory data analysis was promoted by __John Tukey__ to encourage statisticians to explore the data

---

# Data visualization, transformation and modeling

* __Data visualisation__ is a great place to start: make elegant and informative plots that help you understand data (see r4ds, section 3)

* Visualisation alone is typically not enough. __Data transformation__ allows you to select important variables, filter out key observations, create new variables, and compute summaries (see r4ds, section 5)

* Patterns in your data provide clues about relationships. __Data modeling__ is a useful tool for extracting patterns out of data. 
Find patterns with visualisation, then make them concrete and precise with a model. Repeat the process, but replace the old response variable with the residuals from the model (see r4ds, sections 22-25 for an introduction)

* __Exploratory data analysis__ (see r4ds, section 7) is used to understand the major characteristics of the predictors and outcome so that any particular challenges associated with the data can be discovered prior to modeling. This can include investigations of correlation structures in the variables, patterns of missing data, and/or anomalous motifs in the data that might challenge the initial expectations of the modeler.


---

# EDA steps

* One of the first steps of the exploratory data process when the ultimate purpose is to predict a response is to create visualizations that help elucidate knowledge of the response and then to uncover relationships between the predictors and the response (see FES, section 4)

* Start with the response, understanding the characteristics of its distribution

* Explore the relationships among the predictors and between predictors and the response:

    - scatter plots of individual predictors and the response,
    - a pairwise correlation plot among the predictors,
    - a projection of high dimensional predictors into a lower dimensional space,
    - line plots for time-based predictors,
    - the first few levels of a regression or classification tree,
    - a heat map across the samples and predictors, or
    - mosaic plots for examining associations among categorical variables
    - etc

* EDA should not stop at this point, but should continue after initial models have been built. Post model building, visual tools can be used to assess model lack-of-fit and to evaluate the potential effectiveness of new predictors that were not in the original model


---
layout: false
class: inverse, middle, center

# The modelling process

---

# The model versus the modeling process

* The modeling technique is a __small part__ of the overall process

* The process of developing an effective model is both __iterative__ and __heuristic__

* It is difficult to know the needs of any data set prior to working with it and it is common for many approaches to be evaluated and modified before a model can be finalized

* Common steps during model building are:

    - estimating model parameters (i.e. training models)

    - determining the values of tuning parameters that cannot be directly calculated from the data

    - model selection (within a model type) and model comparison (between types)

    - calculating the performance of the final model that will generalize to new data


---

# The modeling process

![](images/intro-process-1.svg)

.center[Figure from FEV, section 1.2.4]

---

![](images/intro-process-1.svg)

* __(a)__ *Exploratory data analysis*. The initial activity begins with EDA investigate the data

* __(b)__ *Early quantitative analysis*. This could include evaluating simple summary measures or identifying predictors that have strong correlations with the outcome. The process might iterate between visualization and analysis until the modeler feels confident that the data are well understood. 

* __(c)__ *Early feature engineering*. The first draft for how the predictors will be represented in the models is created based on the previous analysis.

---

![](images/intro-process-1.svg)


* __(d)__ *Early parameter tuning*. At this point, several different modeling methods might be evaluated with the initial feature set. However, many models can contain hyperparameters that require tuning

* __(e)__ *Early evaluation of performance*. Once the four models have been tuned, they are numerically evaluated on the data to understand their performance characteristics. Summary measures for each model, such as MSE,  accuracy, etc. are used 

* __(f)__ *Residual analysis*. Based on these results, more EDA can be conducted on the model results, such as residual analysis. 

* __(g)__ *Systematic issues*. If there is any systematic issues with the model, another round of feature engineering might be used to compensate for these obstacles


---

![](images/intro-process-1.svg)

* __(h)__ *Parameter tuning*. By this point, it may be apparent which models tend to work best for the problem at hand and another, more extensive, round of model tuning can be conducted on fewer models

* __(i)__ *Performance evaluation*. After more tuning and modification of the predictor representation, the two candidate models (#2 and #4) have been finalized. These models can be evaluated on an external test set as a final evaluation between the models 

* __(h)__ *Final model*. The final model is then chosen and this fitted model will be used going forward to predict new samples 

---

# No Free Lunch theorem

> The "No Free Lunch" Theorem (Wolpert 1996) is the idea that, without any specific knowledge of the problem or data at hand, no one predictive model can be said to be the best

* There have been experiments to judge which models tend to do better than others on average

* However, the rate of "winning" is not high enough to enact a strategy of "always use model X"

---
layout: false
class: inverse, middle, center

# Important concepts

---

# Important concepts

1. __Overfitting__ Overfitting is the situation where a model fits very well to the current data but fails when predicting new samples. Often, models that are very flexible have a higher likelihood of overfitting the data. 

2. __ Model Bias and Variance__ 
    
3.  __Experience Driven Modeling and Empirically Driven Modeling__
    - Subject matter experts may have a good sense of what predictors should be in the model based on previous experience prior to looking at the data
    - However, there can be a strong desire to be __data driven__ rather than __experience driven__
    - Usually a combination of the two approaches works best

4. __Feature selection__
    - Models can have reduced performance due to __irrelevant predictors__ causing excess model variation
    - When conducting a search for a subset of predictor to use, it is important to realize that there may not be a unique set of predictors that will produce the best performance
    
5. __Feature engineering__
    
---

# Feature engineering

* The idea that there are different ways to represent predictors in a model, and that some of these representations are better than others, leads to the idea of feature engineering - the process of creating representations of data that increase the effectiveness of a model

* Key relationships that are not directly available as predictors may be between the response and:
    -  a transformation of a predictor,
    - an interaction of two or more predictors such as a product or ratio,
    - a functional relationship among predictors, or
    - an equivalent re-representation of a predictor.


---
layout: false
class: inverse, middle, center

# Car data

---

# Car data

* From Azzalini and Scarpa, Chapter 2

* The cars data were obtained by simple manipulation of original data which referred to the characteristics of 203 automobile models imported into the USA in 1985. 

* The original data are available at: [ftp://ftp.ics.uci.edu/pub/machine-learning-databases/autos](ftp://ftp.ics.uci.edu/pub/machine-learning-databases/autos)
and their manipulation on our part simply consisted of converting one unit of measurement to another, and eliminating some variables

* Goal: identify a relationship that allows us to predict the consumption of fuel or, equivalently, the distance covered per unit of fuel as a function of certain characteristics of a car

---

# Variables

* brand   :             manufacturer (factor, 22 levels), casa produttrice (fattore, 22 livelli) 
* fuel    :       type of engine fuel (factor, 2 levels: diesel, gasoline), tipo di alimentazione del motore (fattore, 2 livelli) 
* aspiration :          type of engine aspiration (factor, 2 levels: standard, turbo), tipo di aspirazione del motore (fattore, 2 livelli)   
* bodystyle :         type of body style (factor, 5 levels: hardtop, wagon, sedan, hatchback, convertible),  tipo di carrozzeria (fattore, 5 livelli)  
* drive.wheels :       type of drive wheels (factor,  3 levels: 4wd, fwd, rwd), tipo di trazione (fattore,  3 livelli)    
* engine.location :    location of engine (factor, 2 levels: front, rear), posizione del motore (fattore, 2 livelli)  
* wheel.base :         distance between axes (cm), distanza tra gli assi (cm) 
* length :             length (cm), lunghezza (cm)      
* width :              width (cm), larghezza (cm)    
* height :             height (cm), altezza (cm)    
* curb.weight :        weight (kg), peso (kg)   
* engine size :        engine size (l), cilindrata (l)   
* compression.ratio :   compression ratio, rapporto di compressione   
* HP :                  horsepower, cavalli motore   
* peak.rot     :       number of peak revolutions per minute, numero di giri massimi del motore al minuto    
* __city.distance__ :      city distance covered (km/l), percorrenza urbana (km/l)   
* highway.distance :    highway distance (km/l), percorrenza extra urbana (km/l)  
* n.cylinders  :       number of cylinders, numero di cilindri

---


```{r}
rm(list=ls())
auto <- read.table("http://azzalini.stat.unipd.it/Book-DM/auto.dat", header=TRUE, quote="\"")
dim(auto)
# any missing value?
sum(is.na(auto))
```


---

```{r}
# variable types
str(auto)
```

---


```{r}
# reduced data
names(auto)[which(names(auto)=="city.distance")] <- "y"
vars <- c("y", "engine.size","n.cylinders","curb.weight","fuel")
train <- auto[,vars]
# pairs
pairs(train[,-5], col=train$fuel)
```

---


.pull-left[
```{r}
# histogram of y
hist(train$y)
```
]


.pull-right[
```{r}
# density plot of y
plot(density(train$y))
```
]

---

.pull-left[
```{r}
# scatterplot
plot(y~engine.size, train, col=fuel) 
```
]

.pull-right[
```{r}
# jitter
plot(jitter(y)~jitter(engine.size), train, col=fuel) 
```
]

---

$$Y = \beta_1 + \beta_2 \mathrm{engine.size} + \beta_3 \mathrm{engine.size}^2 + \beta_4 \mathrm{engine.size}^3 + \beta_5 I\{\mathrm{fuel} = \mathrm{gas}\} + \varepsilon$$

```{r}
fit1 <- lm(y ~ poly(engine.size, degree=3, raw=T) + fuel, train)
```

```{r, echo=FALSE}
x <- seq(1,5.5,  length=200)
beta<- coef(fit1)
plot(y ~ engine.size, col=fuel, train)
lines(x, beta[1]+ beta[2]*x+beta[3]*x^2+beta[4]*x^3)
lines(x,  beta[1]+ beta[2]*x+beta[3]*x^2+beta[4]*x^3+beta[5],col="red")
```

---

$$1/y = \beta_1 + \beta_2 \mathrm{engine.size} +  \beta_3 I\{\mathrm{fuel} = \mathrm{gas}\}+ \varepsilon$$

```{r}
fit2<-lm(1/y ~ engine.size + fuel, train)
```

.pull-left[
```{r, echo=FALSE}
plot(1/y ~ engine.size, col=fuel, train)
beta<- coef(fit2)
abline(beta[1:2])
abline(beta[1]+beta[3], sum(beta[2]) , col=2)
```
]

.pull-right[
```{r, echo=FALSE}
plot(y ~ engine.size, col=fuel, train)
lines(x, 1/(beta[1]+ beta[2]*x))
lines(x, 1/(beta[1]+beta[3]+ beta[2]*x),  col=2)
```
]


---

$$\log(y) = \beta_1 + \beta_2 \log(\mathrm{engine.size}) +  \beta_3 I\{\mathrm{fuel} = \mathrm{gas}\} + \varepsilon$$
```{r}
fit3 <- lm(log(y) ~ log(engine.size) + fuel, train)
```

.pull-left[
```{r, echo=FALSE}
plot(log(y) ~ log(engine.size), col=fuel, train)
beta<- coef(fit3)
abline(beta[1:2])
abline(beta[1]+beta[3], sum(beta[2]) , col=2)
```
]

.pull-right[
```{r, echo=FALSE}
plot(y ~ engine.size, col=fuel, train)
x <- log(seq(1,5.5,  length=200))
lines(exp(x), exp(beta[1]+ beta[2]*x))
lines(exp(x), exp(beta[1]+beta[3]+ beta[2]*x),  col=2)
```
]

---

.pull-left[
```{r}
plot(fit3, which=1)
```
]

.pull-right[
```{r}
plot(fit3, which=4)
```
]

---

```{r}
train[which(cooks.distance(fit3) > .07), ]
table(train$n.cylinders)
```


---


$$\log(y) = \beta_1 + \beta_2 \log(\mathrm{engine.size}) +  \beta_3 I\{\mathrm{fuel} = \mathrm{gas}\}  + \beta_4 \log(\mathrm{curb.weight})+ \beta_5I\{\mathrm{n.cylinders} = 2\} + \varepsilon$$

```{r}
# MSE.tr
fit1 = update(fit1, . ~ . +  log(curb.weight) + I(n.cylinders==2), train)
mean(resid(fit1)^2) 
fit2 = update(fit2, . ~ . +  log(curb.weight) + I(n.cylinders==2), train)
mean((train$y - 1/fitted(fit2))^2) 
fit3 = update(fit3, . ~ . +  log(curb.weight) + I(n.cylinders==2), train)
mean((train$y - exp(fitted(fit3)))^2) 
```

---

layout: false
class: inverse, middle, center

# Expanded car data

---

# Expanded car data


* The data that are used here are an extended version of the ubiquitous `mtcars` data set that contains information on the design and performance characteristics of older model cars (1973-74). [fueleconomy.gov](fueleconomy.gov) was used to obtain fuel efficiency data on cars from 2015-2019.

* Over this time range, duplicate ratings were eliminated; these occur when the same car is sold for several years in a row. As a result, there are 4422 cars that are listed in the data. The predictors include the automaker and addition information about the cars (e.g. cylinders, etc).

* In our analysis, the data from 2015-2108 are used for training to see if we can predict the 435 cars that were new in 2019. We will restrict ourselves to cars that use some type of gas.

* Non-combustion engines have their MPG estimated even though there are no real gallons. Let's get rid of these for simplicity.

---

# dplyr and ggplot2

* Two packages developed by Hadley Wickham greatly simplify the task of exploring and graphing data:
    - `dplyr` for exploring (r4ds, section 5)
    - `ggplot2` for graphing (r4ds, section 3)
    
* `ggplot2` implements the __grammar of graphics__, a coherent system for describing and building graphs

* `dplyr` implements the __split-apply-combine strategy__ for data analysis. The strategy, package author Hadley Wickham explains, is to 

> break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together.
    
* Additionally, the __piping syntax__ offered in the `magrittr` package (included with `dplyr`) helps to write more understandable `dplyr` code (see r4ds, section 18)

* Documentation and examples for both packages are widely available on the web

---

```{r}
rm(list=ls())
PATH <- "/Users/aldosolari/Documents/mygithub/DM/dataset/"
load(paste(PATH,"car_data_splits.RData", sep=""))
# remove non-combustion engines
library(dplyr)
removals <- c("CNG", "Electricity")
train <- 
  car_train %>% 
  filter(!(fuel_type %in% removals)) %>%
  mutate(fuel_type = relevel(fuel_type, "Gasoline_or_natural_gas"))
test <-
  car_test %>% 
  filter(!(fuel_type %in% removals)) %>%
  mutate(fuel_type = relevel(fuel_type, "Gasoline_or_natural_gas"))
# training and test set by year
train %>% pull(year) %>% table()
test %>% dim()
```

---

# Data visualization with ggplot2

* Among the variables in the expanded car data
    - `mpg`, a car’s fuel efficiency on the highway, in miles per gallon (MPG). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. This is the response
    - `eng_displ`, a car's engine size
    - `fuel_type`, fuel type

* We begin with these variables to illustrate how the `ggplot2` can be used to create data graphics

* `ggplot2` main reference: [https://ggplot2.tidyverse.org/](https://ggplot2.tidyverse.org/)

* You begin a plot with the function `ggplot()`. The first argument of `ggplot()` is the dataset to use in the graph, and `aes(x, y, ...)` to specify which variables to map to the x and y axes

* You complete your graph by adding one or more layers to `ggplot()`. For univariate displays, `geom_histogram()` and  `geom_density()` create an histogram or density plot

---

```{r ggplot, include = FALSE}
library(ggplot2)
thm <- theme_bw() + 
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA), 
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

.pull-left[
```{r}
ggplot(data = train, aes(x=mpg)) + 
  geom_histogram(binwidth=5) 
```
]

.pull-right[
```{r}
ggplot(data = train, aes(x=mpg)) + 
  geom_density(adjust=0.8) 
```
]

---


* Now use `aes(x = eng_displ, y = mpg)` put `eng_displ` on the x-axis and `mpg` on the y-axis. The function `geom_point()` adds a layer of points to your plot, which creates a scatterplot

* The values of `eng_displ` and `mpg` are rounded so the points appear on a grid and many points overlap each other. This problem is known as __overplotting__. This arrangement makes it hard to see where the mass of the data is

* You can avoid this gridding by setting the `position` adjustment to jitter

* Or you can use the `alpha` aesthetic, which controls the transparency of the points:

* To add `fuel_type`, which is categorical, you can map the colors of your points or split your plot into __facets__, subplots that each display one subset of the data

---

.pull-left[
```{r}
ggplot(data = train) + 
  geom_point(mapping = aes(x = eng_displ, y = mpg), position = "jitter") 
```
]

.pull-right[
```{r}
ggplot(data = train) + 
  geom_point(mapping = aes(x = eng_displ, y = mpg), alpha = I(1/10)) 
```
]

---

.pull-left[
```{r}
ggplot(data = train) + 
  geom_point(mapping = aes(x = eng_displ, y = mpg, color=fuel_type))
```
]

.pull-right[
```{r}
ggplot(data = train) + 
  geom_point(mapping = aes(x = eng_displ, y = mpg, color=fuel_type)) + 
  facet_wrap(~ fuel_type)
```
]

---

.pull-left[
```{r}
ggplot(data = train, aes(x = eng_displ, y = mpg)) + 
  geom_point() +
  geom_smooth()
```
]

.pull-right[
```{r}
ggplot(data = train, mapping = aes(x = eng_displ, y = mpg, group = as.factor(plug_in_hybrid), color = as.factor(plug_in_hybrid))) + 
  geom_point() +
  geom_smooth(se = FALSE)
```
]







