<!DOCTYPE html>
<html>
  <head>
    <title>Ridge regression</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Ridge regression
### Aldo Solari

---





# Outline

* Problems in linear regression
* Problem 1: collinearity 
* Problem 2: overfitting
* Problem 3: high-dimensional data
* Ridge regression
* Bias-variance trade-off
* Constrained estimation

---

# Matrix notation

* Vector of response: 
`$$\underset{n\times 1}{\mathbf{y}} = 
\left[
\begin{array}{c}
y_1   \\
\cdots\\
y_i  \\
\cdots\\
y_n \\
\end{array}\right]$$`

* Design matrix:
`$$\underset{n\times p}{\mathbf{X}} = \left[
\begin{array}{cccccc}
x_{1}^\mathsf{T}   \\
x_{2}^\mathsf{T}  \\
\cdots   \\
x_{i}^\mathsf{T}    \\
\cdots\\
x_{n}^\mathsf{T}\\
\end{array}\right] = \left[
\begin{array}{cccccc}
x_{11}  &amp; x_{12}  &amp; \cdots   &amp;  x_{1j}  &amp; \cdots   &amp;   x_{1p}  \\
x_{21}  &amp; x_{22} &amp; \cdots   &amp;  x_{2j}  &amp; \cdots   &amp;   x_{2p}  \\
\cdots   &amp; \cdots   &amp;  \cdots &amp; \cdots   &amp;  \cdots  \\
x_{i1}  &amp; x_{i2} &amp; \cdots   &amp;  x_{ij}&amp; \cdots   &amp; x_{ip}    \\
\cdots   &amp; \cdots   &amp;  \cdots  &amp;  \cdots   &amp;  \cdots\\
x_{n1}   &amp; x_{n2} &amp; \cdots   &amp; x_{nj}    &amp;  \cdots   &amp;   x_{np}\\
\end{array}\right]$$`

---

# Linear regression

* Suppose `\(\mathrm{rank}(\mathbf{X}) = p\)`

* Least squares problem: 
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2 = \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2$$`
where  `\(\| \underset{p \times 1}{ \mathbf{v}} \|_{2} = (\mathbf{v}^\mathsf{T}\mathbf{v})^{1/2} = \sqrt{\sum_{j=1}^{p} v_j^2}\)` denotes the `\(L_2\)` norm



* Normal equations: `\(\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T}\mathbf{y}\)`

* OLS estimator: `$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`

* Fitted values: `\(\underset{n\times 1}{\hat{\mathbf{y}} } = \mathbf{X}\hat{\boldsymbol{\beta}}\)`

* Test data: `\(\underset{m\times 1}{\mathbf{y}^*}\)`, `\(\underset{m\times p}{\mathbf{X}^*}\)`, prediction on test data: `\(\underset{m\times 1}{\hat{\mathbf{y}}^*} = \mathbf{X}^*\hat{\boldsymbol{\beta}}\)`


---

# Problems in linear regression

1. Collinearity

2. Overfitting: when `\(p \approx n\)`, the linear regression fit often has low bias but high variance 

3. High-dimensional data: when `\(p&gt;n\)`, we have super-collinearity 

---

layout: false
class: inverse, middle, center

# Problem 1: collinearity

---

# Credit data

* `Balance` : average credit card debt (response) 
* `Age`
* `Cards` : number of credit cards
* `Education` : years of education
* `Income` : in thousands of dollars
* `Limit` : credit limit
* `Rating` : credit rating
* `Gender`
* `Student` : student status
* `Status` : marital status
* `Ethnicity` : Caucasian, African American
or Asian

---


```r
library(readr)
Credit &lt;- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv")[,-1]
plot(Credit[,c("Balance","Limit","Rating")])
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;

---

# Collinearity

* `Limit` and `Rating` are very highly correlated with each other, i.e. they are __collinear__

* Collinearity is problematic for regression, since it can be difficult to separate out the individual effects of collinear variables on the response


```r
round( summary(lm(Balance ~ Limit + Rating, Credit))$coeff , 4)
```

```
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) -377.5368    45.2542 -8.3426   0.0000
Limit          0.0245     0.0638  0.3840   0.7012
Rating         2.2017     0.9523  2.3120   0.0213
```

* The importance of the `Limit` variable has been masked due to the presence of collinearity

* [ISL Figure 3.15](http://www-bcf.usc.edu/~gareth/ISL/Chapter3/3.15.pdf) 

---

# Simulated data

True model

`$$Y = 3 + 1\cdot X_1 + 1\cdot X_2 + \varepsilon$$`


```r
set.seed(473)
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20, mean=x1, sd=.01)
y &lt;- 3 + x1 + x2 + rnorm(20)
round( summary(lm(y~x1+x2))$coef , 4)
```

```
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   2.5347     0.2396 10.5787   0.0000
x1           39.9802    21.3313  1.8743   0.0782
x2          -37.8708    21.3976 -1.7699   0.0947
```

* Here the strong correlation between `\(X_1\)` and `\(X_2\)` is problematic for the estimation of `\(\beta_1\)` and `\(\beta_2\)`

---
layout: false
class: inverse, middle, center

# Problem 2: Overfitting

---

# Simulated data

* `\(n=10\)` and `\(p=9\)`
* Model: `\(Y = \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_9X_9 +  \varepsilon\)`
* True `\(\boldsymbol{\beta} = (1,0,0,\ldots,0)^\mathsf{T}\)`


```r
n = 10
p = 9
set.seed(1793)
X = matrix(rnorm(n*p),nrow=n, ncol=p)
y = X[,1] + rnorm(n,0,0.5)
fit = lm(y~ 0 + X)
coef(fit)
```

```
        X1         X2         X3         X4         X5         X6         X7         X8         X9 
 13.527953  -5.466373 -24.512290  -6.610876   2.732160 -10.670639  15.472750  16.831498 -11.331475 
```

* Overfitting refers to the phenomenon of modelling the noise
rather than the signal

* Large estimates of regression coefficients (in absolute value)
are often an indication of overfitting

---



```r
yhat = predict(fit)
plot(X[,1],y, xlab="x1")
ix = sort(X[,1], index.return=T)$ix
lines(X[ix,1], yhat[ix])
abline(a=0,b=1, col=4)
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---

# Complete overfitting


```r
y = c(-1,0); x1 = c(.75,.5); x2 = c(1,.5)
summary(lm(y~0+x1+x2))
```

```

Call:
lm(formula = y ~ 0 + x1 + x2)

Residuals:
ALL 2 residuals are 0: no residual degrees of freedom!

Coefficients:
   Estimate Std. Error t value Pr(&gt;|t|)
x1        4         NA      NA       NA
x2       -4         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:    NaN 
F-statistic:   NaN on 2 and 0 DF,  p-value: NA
```

---

# Observations space

![](images/overfit.jpg)

---

* In `\(p\geq n\)` settings overfitting is a real threat: it
is possible to form a linear combination of the predictors
that perfectly explains the response, including the noise

* [ISL Figure 6.22](http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.22.pdf) Least squares regression in the low-dimensional setting with `\(n=20\)`, `\(p=2\)` and in the `\(n=p=2\)` setting

---
layout: false
class: inverse, middle, center

# Problem 3: High-dimensional data

---

# Riboflavin data

* The data has been kindly provided by [DSM](https://www.dsm.com/corporate/home.html) (Kaiseraugst, Switzerland)

* The response variable is the logarithm of the riboflavin (vitamin B2) production rate in [Bacillus subtilis](https://en.wikipedia.org/wiki/Bacillus_subtilis)

* `\(n=71\)` observations

* `\(p=4088\)` predictors measuring the logarithm of the expression level of 4088 genes

* Data sets containing more variables than observations, i.e. `\(p &gt; n\)` are referred to as __high-dimensional__

* An high-dimensional `\(\mathbf{X}\)` suffers from __super-collinearity__ : `\(p&gt;n\)` implies that the columns of `\(\mathbf{X}\)` are linearly dependent, i.e.  `\(\mathrm{rank}(\mathbf{X})&lt;p\)`. Then `\(\mathbf{X}^\mathsf{T}\mathbf{X}\)` is singular and it does not have an inverse

* How to perform __high-dimensional regression__?

---


```r
library(hdi)
data(riboflavin)
Riboflavin = as.data.frame( cbind(y=riboflavin$y, x=riboflavin$x) )
names(Riboflavin) = c("y",attr(riboflavin$x, "dimnames")[[2]])
dim(Riboflavin)
```

```
[1]   71 4089
```



```r
coef( lm( y ~ ., Riboflavin) )[1:100]
```

```
(Intercept)     AADK_at     AAPA_at     ABFA_at      ABH_at     ABNA_at     ABRB_at     ACCA_at     ACCB_at     ACCC_at     ACDA_at     ACKA_at     ACOA_at     ACOB_at     ACOC_at     ACOL_at     ACOR_at     ACPA_at 
-2.04286709  2.66498746 -2.16773713 -0.90204415 -1.36239163 -1.12395692  1.80341933  1.77030566  0.04895629 -1.77897762  2.87871238  3.09031706 -1.71121111  3.61659914 -2.71881162  0.53536079 -1.04826773 -0.20873401 
    ACSA_at     ACUA_at     ACUB_at     ACUC_at     ADAA_at     ADAB_at     ADDA_at     ADDB_at     ADEC_at     ADHA_at     ADHB_at      ADK_at     AHPC_at     AHPF_at     AHRC_at     ALAS_at      ALD_at     ALDX_at 
-1.92256737 -2.41654532 -1.76593431  6.34334112  1.73608095 -1.23913307 -1.24858497  1.66389296 -0.66900097  1.90018145 -4.81855093  0.33413745  0.48211084 -0.20039815 -0.53329911  4.50154225 -0.65453871 -0.74650627 
    ALDY_at     ALKA_at     ALSD_at     ALSR_at     ALSS_at     ALST_at     AMHX_at     AMPS_at     AMYC_at     AMYD_at     AMYE_at     AMYX_at     ANSA_at     ANSB_at     ANSR_at     APPA_at     APPB_at     APPC_at 
 1.38732907 -0.25703937  3.21285329 -1.34814259 -4.16228098  5.11751318 -1.80911059  0.27154095  0.34434621 -0.11283781  0.95456839  1.87926516 -4.30714076 -1.65699081 -0.58712488  2.59164003 -0.83931985 -2.06755571 
    APPD_at     APPF_at     APRE_at     APRX_at      APT_at     ARAA_at     ARAB_at     ARAD_at     ARAE_at     ARAL_at     ARAM_at     ARAN_at     ARAP_at     ARAQ_at     ARAR_at     ARGB_at     ARGC_at     ARGD_at 
 2.18898421 -3.80756793  4.08954861  0.09820901 -3.45116308 -0.32434621  4.03237534 -0.34840374 -0.21522426 -2.09292044  1.07242536  2.17828300 -2.51813771 -1.31425090 -1.89524240  2.31358027 -2.45305386          NA 
    ARGE_at     ARGF_at     ARGG_at     ARGH_at     ARGJ_at     ARGS_at     AROA_at     AROB_at     AROC_at     AROD_at     AROE_at     AROF_at     AROH_at     AROI_at     arsB_at     arsC_at     arsR_at      ASD_at 
         NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA          NA 
   ASK_r_at     ASNB_at     ASNH_at     ASNS_at     ASPB_at     ASPS_at     ATPA_at     ATPB_at     ATPC_at     ATPD_at 
         NA          NA          NA          NA          NA          NA          NA          NA          NA          NA 
```

---

# The failure of least squares in high dimensions

* When `\(\mathrm{rank}(\mathbf{X}) &lt; p\)`, e.g. this happens when `\(p&gt;n\)`, there are infinitely many solutions in the least square problem

* Suppose `\(p&gt;n\)` and `\(\mathrm{rank}(\mathbf{X}) = n\)`. Let `\(U=\mathrm{span}(\mathbf{X})\)` be the `\(n\)`-dimensional space spanned by the columns of `\(\mathbf{X}\)` and `\(V=U^\bot\)` the `\(p-n\)` dimensional space orthogonal complement of `\(U\)`, i.e. i.e. the non-trivial null space of `\(\mathbf{X}\)`

* Then `\(\mathbf{X}\mathbf{v} = \mathbf{0}_p\)` for all `\(\mathbf{v} \in V\)`, and `\(\mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{v} = \mathbf{X}^\mathsf{T}\mathbf{0}_p = \mathbf{0}_n\)`, the solution of the normal equations is
`$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-}\mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{v} \quad \forall\,\, \mathbf{v} \in V$$`
where `\(\mathbf{A}^-\)` denotes the Moore-Penrose inverse of the matrix `\(\mathbf{A}\)`


---

# Regularization

* How we deal with problems 1, 2 and 3? The short answer is __regularization__

* Least squares: 
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2$$`

* __Penalized form__
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2+ P(\boldsymbol{\beta})$$`
where `\(P(\cdot)\)` is some (typically convex) penalty function

* __Constrained form__
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \quad \mathrm{subject\,\,to\,\,} \boldsymbol{\beta}\in C$$`
where `\(C\)` is some (typically convex) set

* At its core, regularization provides us a way of navigating the bias-variance trade-off

---

layout: false
class: inverse, middle, center

# Ridge regression

---

# Ridge regression

* Penalized least squares:
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2 + \lambda\sum_{j=1}^{p}\beta_j^2 = \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \underbrace{ \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 }_{\mathrm{RSS} }+ \underbrace{\lambda\| \boldsymbol{\beta}\|^2_2}_{\mathrm{penalty}}$$`
Here `\(\lambda \in [0,\infty)\)` is the __tuning parameter__ which controls the strenght of the penalty term

* The minimum of the RSS is attained at `\(\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}\)` while the minimum of the ridge penalty is attained at `\(\boldsymbol{\beta}=\mathbf{0}_p\)`. The effect of the penalty in this balancing act is to __shrink__ the
coefficient estimates towards zero

* The solution of the minimization problem is the __ridge estimator__ :
`$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}^\lambda} = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`
where `\(\mathbf{I}_p\)` is the `\(p\times p\)` identity matrix

* For any design matrix `\(\mathbf{X}\)`, the quantity
`\((\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\)` is always invertible provided that `\(\lambda &gt; 0\)`; thus,
there is always a unique solution `\(\hat{\boldsymbol{\beta}}^\lambda\)`

---


# Solution path

* As `\(\lambda \rightarrow 0\)`, `\(\hat{\boldsymbol{\beta}}^\lambda \rightarrow \hat{\boldsymbol{\beta}}\)`

* As `\(\lambda \rightarrow \infty\)`, `\(\hat{\boldsymbol{\beta}}^\lambda \rightarrow \mathbf{0}_p\)`

* __Solution path__ of the ridge estimator:
`$$\{\hat{\boldsymbol{\beta}}^{\lambda}: \lambda \in [0,\infty) \}$$`

* All regression coefficients are shrunken towards zero as the tuning parameter `\(\lambda\)` increases

* This behaviour is not strictly monotone in `\(\lambda\)`: `\(\lambda_a &gt; \lambda_b\)` does not necessarily imply `\(|\hat{\beta}^{\lambda_a}_j| &lt; |\hat{\beta}^{\lambda_b}_j|\)`

---

# Standardization

* Some care is needed in the application of ridge regression

* First of all, usually the intercept term is not included in the penalty

* Second, the size of the regression coefficient depends on the
scale with which the associated predictor is measured, and the penalty term `\(\sum_{j=1}^{p}\beta_j\)` is unfair if the predictors are not on the same scale

* If predictors are not in the same units already, they are standardized to have mean zero and standard deviation 1 prior to model fitting

* If we center the columns of `\(\mathbf{X}\)`, then the intercept estimate ends up just being `\(\hat{\beta}_0=\bar{y}\)`. Usually the response is also standardized to have mean zero and standard deviation 1 and the intercept term is not included in the model

* This can be accomplished without any loss of generality because the coefficients can be returned on the original scale

---


```r
fit = lm(Balance ~ ., Credit)
X= model.matrix(fit)
y = Credit$Balance
n = nrow(X)
Xstd = scale(X[,-1])[,]*sqrt((n-1)/n)
ystd = scale(y)[,]*sqrt((n-1)/n)
```


---


```r
lambdas = exp(seq(-4,12,length.out = 100))
hatbetas =sapply(lambdas, function(lambda)
solve(t(Xstd)%*%Xstd + lambda*diag(ncol(Xstd))) %*% t(Xstd) %*% ystd
)
matplot(log(lambdas), t(hatbetas), type="l", lty=1, ylab="standardized coefficients")
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---

# glmnet()


```r
glmnet(X, y,
family="gaussian",
alpha = 0, # default = 1 (LASSO), 0 = ridgde
nlambda = 100, # default
standardize = TRUE, # default
intercept=TRUE # default
)
```

* See the [glmnet vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)

---


```r
library(glmnet)
fit = glmnet(Xstd, ystd, alpha = 0, family="gaussian", lambda=lambdas/n)
# solution path
plot(fit, xvar="lambda", label = TRUE)
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

---


```r
# Original scale
fit = glmnet(X, y, alpha = 0, family="gaussian")
plot(fit, xvar="lambda")
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---

# Choice of the penalty parameter

* Throughout the introduction of ridge regression and the
subsequent discussion of its properties the penalty parameter is considered known or given

* In practice, it is unknown and the user needs to make an informed decision on its value

* Usually by cross-validation or generalized cross-validation

---


```r
# K-fold cross validation
K &lt;- 5
fit.cv &lt;-cv.glmnet(X[,-1],y, alpha=0, nfolds = K, grouped=FALSE)
plot(fit.cv)
```

![](12_Ridge_slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---


```r
# lambda selected by K fold cross validation
hatlambda &lt;-fit.cv$lambda.min
hatlambda
```

```
[1] 43.52271
```

```r
predict(fit.cv, s=hatlambda, type ="coefficients")
```

```
12 x 1 sparse Matrix of class "dgCMatrix"
                              1
(Intercept)        -397.7917347
Income               -4.9882897
Limit                 0.1128011
Rating                1.6402507
Cards                15.9007303
Age                  -0.9748936
Education            -0.4459264
GenderMale            4.4495021
StudentYes          378.2073036
MarriedYes          -12.1751781
EthnicityAsian       12.7589783
EthnicityCaucasian    9.0004175
```

---

# Collinearity


```r
set.seed(473)
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20, mean=x1, sd=.01)
y &lt;- rnorm(20, mean=3+x1+x2)
# OLS estimate
coef(lm(y~x1+x2))
```

```
(Intercept)          x1          x2 
   2.534666   39.980190  -37.870805 
```

```r
# Ridge estimate
require(MASS)
lm.ridge(y~x1+x2, lambda=1)
```

```
               x1       x2 
2.699856 1.138304 1.037330 
```

It can be shown that ridge regression improves estimates for the coefficients of strongly positively correlated predictors with similar values of true coefficients 

---

# High-dimensional data

![](12_Ridge_slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---

layout: false
class: inverse, middle, center

# Bias-variance trade-off

---

# Assumptions

* Consider a fixed `\(X\)` setting

* Assume that the true model is linear 
`$$\mathbf{y} =  \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$`
where the the distribution of the errors is Gaussian, i.e. `\(\boldsymbol{\varepsilon} \sim N(\mathbf{0}_n,\sigma^2 \mathbf{I}_{n})\)`

* These are strong assumptions

---

# Ridge regression bias-variance trade-off

* The linear operator 
`$$\mathbf{W}^{\lambda} = [\mathbf{I}_{p\times p} + \lambda(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}]^{-1}$$`
transforms the OLS estimator into the ridge estimator 
`$$\mathbf{W}^{\lambda} \hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}^{\lambda}$$`

* Bias `$$\mathbb{E}(\hat{\boldsymbol{\beta}}^{\lambda}) =  \mathbf{X}^\mathsf{T}\mathbf{X}(\lambda\mathbf{I}_{p}  + \mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \boldsymbol{\beta}$$`


* Variance
`$$\mathbb{V}\mathrm{ar}(\hat{\boldsymbol{\beta}}^{\lambda}) = \sigma^2 \mathbf{W}^{\lambda} (\mathbf{X}^\mathsf{T}\mathbf{X} )^{-1}(\mathbf{W}^{\lambda})^\mathsf{T}$$`


---

# Shrinkage

* Estimates are shrunk towards zero

* Consequence
    - Introduces bias
    - Reduces variance

---

# Theorem 2 of Theobald, 1974

* There exists `\(\lambda &gt; 0\)` such that `\((\mathbb{B}\mathrm{ias} )^2 +   \mathbb{V}\mathrm{ar}\)` of the ridge regression estimator is lower than the OLS estimator 

* For a certain value of `\(\lambda\)`, the decrease in variance of the ridge
regression estimator exceeds the increase in its bias

* The optimal choice of `\(\lambda\)` depends on the quantities `\(\boldsymbol{\beta}\)` and `\(\sigma^2\)`. These are unknown in practice

---

![](12_Ridge_slides_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---

# Predicting a Gaussian random variable

* `\(Y = \mu + \varepsilon\)` with `\(\varepsilon \sim N(0,\sigma^2)\)`

* Training data: `\(Y_1,\ldots,Y_n\)`

* Sample mean: `\(\bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i\)`

* Bias-Variance decomposition 
$$
`\begin{aligned}
\mathbb{E}\{[Y - \bar{Y} ]^2 \} &amp; = \sigma^2 + [\mathbb{B}\mathrm{ias}( \bar{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \bar{Y} )\\
&amp; = \sigma^2 + 0 + \sigma^2/n
\end{aligned}`
$$

* Rao-Blackwell theorem says that `\(\bar{Y}\)` has lower variance than any other unbiased estimator

* Is `\(\bar{Y}\)` the optimal prediction for `\(Y\)` ?

---

# Shrunken sample mean

* Shrunken sample mean: `\(\hat{Y} = \lambda \bar{Y}\)` with `\(\lambda \in [0,1]\)` 

* Bias-Variance decomposition 
$$
`\begin{aligned}
\mathbb{E}\{[Y - \hat{Y} ]^2 \} &amp; = \sigma^2 + [\mathbb{B}\mathrm{ias}( \hat{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \hat{Y} )\\
&amp; = \sigma^2 + [\mu - \lambda \mu]^2 + \lambda^2(\sigma^2/n)
\end{aligned}`
$$

* Solving `\(\displaystyle \frac{\partial}{\partial \lambda} \mathbb{E}\{[Y - \hat{Y} ]^2 \}  = 0\)` gives the value
`$$\lambda=\frac{\mu^2}{\mu^2 + \sigma^2/n}$$` 
that minimizes `\(\mathbb{E}\{[Y - \hat{Y} ]^2 \}\)`

* However, the optimal `\(\lambda\)` depends on unknown parameters


---

layout: false
class: inverse, middle, center

# Constrained estimation

---

# Constrained estimation

* Solving the penalized problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|^2_2$$`
is equivalent to solving the constrained problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t$$`

* The constrained problem can be solved by means of the Karuch-Kuhn-Tucker (KTT) multiplier method, which minimizes a function subject to inequality constraint

---

![](images/l2.jpg)

---

![](images/biasvar.jpg)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
