<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to the course</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="fira-code.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to the course
### Aldo Solari

---





# Outline

* Course overview
* The two cultures
* Data science
* Big data
* Your turn


---
layout: false
class: inverse, middle, center

# Course overview

---

# Teacher

Aldo Solari

**E-mail** aldo.solari@unimib.it

**Web page** https://aldosolari.github.io/

**Office hours** Wednesday, 17:00-18:00, room 2030, building U7, II floor

**Course page** https://aldosolari.github.io/DM/


| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 12:30-15:30 |
| Thursday | Lab713 | 13:30-15:30|
| Friday | Lab713 | 12:30-15:30|

.center[**Time table**]

---

# The course


* This course aims to provide statistical and computational tools for __data mining__ and __supervised learning__ by using the R software environment for statistical computing

* In particular, you will learn how to build __predictive/machine learning models__ that generate accurate predictions for future, yet-to-be-seen data

* This framework includes 
    - data visualization
    - exploratory data analysis
    - feature engineering
    - splitting the data into training and testing sets
    - building models
    - selecting an approach for identifying optimal tuning parameters
    - estimating predictive performance

---

# Competitions

* Participating in predictive modelling __competitions__ can help you gain practical experience and improve your data modelling skills in various domains such as credit, insurance, marketing, sales' forecasting etc. 

* At the same time you get to do it in a competitive context against dozens of participants where each one tries to build the most predictive algorithm

* Inspired by [kaggle](https://www.kaggle.com/), the world's largest community of data scientists and machine learners

* Since 2015, this course utilises [BeeViva](http://www.bee-viva.com/competitions), an Italian public data platform for hosting competitions

* Beeviva is also used for the event [Stats Under the Stars](http://sus.stat.unipd.it/)

---

# Stats Under the Stars 

.pull-left[

![](images/SUS.JPG)
.center[Caffè Pedrocchi, Padua]

]

.pull-left[
* [SUS](http://sus.stat.unipd.it/) Padua (8/9/15)
* [SUS^2](http://www.labeconomia.unisa.it/sus2/) Salerno (7/6/16)
* [SUS^3](http://local.disia.unifi.it/sus3/) Florence (27/6/17)
* [SUS^4](https://www.unipa.it/dipartimenti/seas/sus4/) Palermo (19/6/18)
* SUS^5 Milan ! 
]


---

# Exam

* The exams consists of two parts:

    - Test (in lab)
        - Programming exercises (selected R scripts available)
        - "Few" multiple-choice/open questions about theory 

    - Competitions (homework)
        - Attending students can form a team (max. 3 persons)

* The final grade is a weighted average of test (1/2) and competitions (1/2)

* See the [course syllabus](https://aldosolari.github.io/DM/syllabus/syllabus.html) for more information

---

# This year competitions

* **House prices** (40%) Individual competition

* **OkCupid** (30%) Individual/Team

* **TBD** (30%) Individual/Team

| Competition | Problem |  Entry | Submission | 
|---|---|----|----|
| House prices | Regression | October 8 | November 16  |
| OkCupid | Classification | October 15 | November 16  |
| TBD | TBD | October 15 | November 16  |

.center[Competitions timeline]


---
layout: false
class: inverse, middle, center

# The two cultures

---

# The two cultures

Reading: [Breiman (2001) Statistical modelling: the two cultures](http://www2.math.uu.se/~thulin/mm/breiman.pdf) 

Breiman distinguished between

**Statistical modeling**
* emphasis on probability models
* the goal is explanation (by assessing the uncertainty of the
estimates)

**Machine learning**
* emphasis on algorithms
* the goal is prediction accuracy

What is the difference today? 

Reading: [Carmichael and Marron (2018) 
Data Science vs. Statistics: Two Cultures?](https://arxiv.org/pdf/1801.00371.pdf)

---

# Rock-Paper-Scissor algorithm

&lt;img src="images/rps.jpg" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

.center[http://www.nytimes.com/interactive/science/rock-paper-scissors.html]

---

# What the computer is thinking

* Four last throws

    - **PAPER, SCISSORS, SCISSORS, ROCK** &lt;- HUMAN 

    - **SCISSORS, PAPER, PAPER, SCISSORS** &lt;- COMPUTER

* COMPUTER is going to search over 200,000 rounds of Rock-Paper-Scissors data and find
all the times that HUMAN played **P, S, S, R**  when COMPUTER played **S, P, P, S**

* Of all the times HUMANS played **P, S, S, R**  when COMPUTER played **S, P, P, S**, HUMANS played **PAPER** at their next throw the most. 

* COMPUTER thinks that you will do the same this time

---

# Is this machine learning or statistics?

* **Theorem** 
A truly random game would result in a tie

* **Hypothesis** 
A human is not truly random

* **Algorithm** 
Learn humans non-random patterns from the data

* **Counter-attack?** 

---

# Same thing, different name?

| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| target variable | `\(Y\)` | response variable |
| attribute, feature | `\(X\)` | predictor, explanatory variable |
| supervised learning | model `\(Y\)` as a function of `\(X\)` | regression |
| hypothesis | `\(Y= f(X) + \varepsilon\)` | model, regression function |
| instances, examples | `\((Y_1,X_1),\ldots,(Y_n,X_n)\)` | samples, observations |
| learning | `\(\displaystyle \hat{f} = \underset{f}{\arg\min} \sum_{i=1}^{n} \mathrm{loss}(Y_i,f(X_i))\)` | estimation, fitting |
| classification | `\(\hat{Y} = \hat{f}(X)\)` | prediction |
| generalization error | `\(\mathbb{E} [\, \mathrm{loss}(Y,\hat{f}(X))\,]\)` | risk |



---

# So, what's the difference? 


| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| | FOCUS | |
|prediction |  |significance |
| | CULTURE | |
|algorithmic/optimization |  | modeling |
| | METHODS |  |
|decision trees |  | linear/logistic regression |
|k-nearest-neighbors | | discriminant analysis |
|neural networks | | mixed models |
|support vector machines || ridge/lasso regression |
|adaboost | | GAM |
|random forests | | random forests |

---

# Working together

* Leo Breiman's fundamental contributions:
    - Bagging
    - Random Forests
    - Boosting

* Breiman's work helped to bridge the gap between statistical
modeling and machine learning

* __Statistical Learning__ = __Statistical__ Modeling + Machine __Learning__

---

# Statistical learning

* **Unsupervised learning**
    - the data consists of `\(p\)` variables `\(X_1,\ldots,X_p\)`; no variable has a special status
    - the goal is clustering, dimensionality reduction, etc.

* **Supervised learning**
    - the data consists of both response `\(Y\)` and predictors `\(X_1,\ldots,X_p\)`
    - the problem is called supervised learning since the response supervises the learning process
    - the goal is (usually) prediction of the response
    - `\(Y\)` continuous : __regression__ problem 
    - `\(Y\)` binary/multi-class : __classification__ problem 

---

# Convergence

__Machine learning__ constructs algorithms that can learn from data

__Statistical learning__ is a branch of Statistics that was born in response to Machine
learning, emphasizing statistical models and assessment of
uncertainty

__Data mining__ finding patterns in data

__. . .__

all converge into __data science__

---
layout: false
class: inverse, middle, center

# Data science

---

# What is data science?

&lt;img src="images/datascience.png" width="60%" height="60%" style="display: block; margin: auto;" /&gt;


---

# Data science

&gt; is an interdisciplinary field [...]

&gt; to extract knowledge or insights from data [...]

&gt; Wikipedia

---

# Interdisciplinary field

![](images/venn.png)

---

# Data science = applied statistics?

.pull-left[

![](images/tukey.jpg)

.center[John Tukey (1915-2000)]
]

.pull-right[

Reading:

* [Tukey (1962) The future of data analysis](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177704711)
* [Donoho (2015) 50 years of Data Science](https://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf)

]

---

# Statistics ⊂ Data science

&gt; While statistics - as the science of learning from data -
is necessary for turning data into knowledge and action,
it's not the only critical component within data science

&gt; When statistics, database management, and
distributed/parallel computing combine, we will see
growth in cross-trained experts who are better equipped
to solve complex challenges in today's massive data
revolution

&gt; Jessica Utts, ASA president

---

# Key word

Most people hyping Data Science have focused on the first word:
Data

&gt; The key word in "Data Science" is not Data, it is Science

&gt; Jeff Leek 

Data Science is only useful when the data are used to answer a
scientific question

---

# Is Gauss a data scientist?


&lt;img src="images/gauss.jpg" width="30%" height="30%" style="display: block; margin: auto;" /&gt;
.center[Carl Friedrich Gauss (1777 - 1855)]


---

# Gauss' problems

__Astronomy problem__ 

Predict the position of the asteroid Ceres at 31 December 1801 on the basis of data provided by the italian astronomer Giuseppe Piazzi

__Statistical problem__

Find `\(\boldsymbol{\beta}\)` such that minimizes `\((\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\)`

Gauss solution: least squares `\(\hat{\boldsymbol{\beta}}=(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}\)`


__Computational problem__

Solve (by hand!) the system of equations `\(\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T} \mathbf{y}\)`

Gauss solution: fast optimization algorithm (Gaussian elimination)

---
layout: false
class: inverse, middle, center

# Big Data

---

# Big data revolution

&lt;img src="images/magazines.jpg" width="50%" height="50%" style="display: block; margin: auto;" /&gt;


---

# Big data

&gt; is a term for data sets that are so large or complex that traditional data processing application software is inadequate to deal with them


&gt; Wikipedia

* If you have several gigabytes of data or several million
observations, standard relational databases become unwieldy;
databases to manage data of this size are generically known
as NoSQL databases

* Tools for manipulating big data are given in the next table (but won't be covered in this course)

---

.pull-left[

![](images/tools.png)

]

.pull-right[
Reading: [Varian (2013) Big Data: New Tricks for Econometrics](http://people.ischool.berkeley.edu/~hal/Papers/2013/ml.pdf)
]


---

# Data: tall and fat

The outcome of the big-data processing:

data set = `\(n \times p\)` matrix

BIG `\(n\)` = TALL = computational problem

BIG `\(p\)` = FAT = curse of dimensionality

Which of the two is more problematic? Why?

---

# The end of theory?

Petabytes allow us to say: correlation is enough


![](images/end.jpg)


Anderson (2008) The End of Theory: The Data Deluge Makes
the Scientific Method Obsolete, Wired Magazine 16.07




---
layout: false
class: inverse, middle, center

# Your turn

---

# Training set and test set

* Suppose that yesterday we observed `\(n\)` pairs of data, the __training set__
`$$(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$$` 

* The data were generated artificially by 
`$$y = f(x)+ \varepsilon$$`

* We wish to obtain an estimate `\(\hat{f}\)` of `\(f\)` that allows us to predict future `\(y\)` as new observations of `\(x\)` become available

* Tomorrow data, or the __test set__, will be like
`$$(x^*_1,y^*_1), (x^*_2,y^*_2), \ldots, (x^*_m,y^*_m)$$`

* To simplify, consider a __fixed-X setting__: `\(x_1,\ldots,x_n\)` are fixed (i.e. non-random) and the new `\(y^*_i\)` are associated with the same `\(x_i\)` used for yesterday's data 

`$$(x_1,y^*_1), (x_2,y^*_2), \ldots, (x_n,y^*_n)$$`
---



```r
# import data
library(readr)
*df &lt;- read_table2("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat")[-31,]
train &lt;- data.frame(x=df$x, y=df$y.yesterday)
test &lt;- data.frame(x=df$x, y=df$y.tomorrow)

# scatterplot
plot( y ~ x , train)
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-5-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---

# Mean squared error

* Suppose we have estimated `\(f\)` by `\(\hat{f}\)`

* For yesterday data (training set), we can compute the __mean squared error__ 
`$$\mathrm{MSE}_{\mathrm{Tr}} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2$$` 

* However, we would like to have a good performance on tomorrow data (test set)
`$$\mathrm{MSE}_{\mathrm{Te}} = \frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2$$` 


---

# Polynomials


* Consider polynomial regression of degree `\(d\)`
`$$f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_d x^{d}$$`

* Use the training set to get the estimate 
`$$\hat{f}(x)=\hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_d x^{d}$$` and predict tomorrow `\(y_i^*\)` by using
`$$\hat{y}_i^*=\hat{f}(x_i)$$`

* Find the degree `\(d\)` that minimizes the expected value of  `\(\mathrm{MSE}_{\mathrm{Te}}=\frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2\)`


---


```r
# 3rd degree polynomial regression fit
*fit &lt;- lm( y ~ poly(x, degree=3), train)
*yhat &lt;- predict(fit, newdata=test)

# plot
plot( y ~ x , train)
lines( yhat ~ x, train)
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-6-1.png" width="50%" style="display: block; margin: auto;" /&gt;

```r
# MSE.tr
( MSE.tr &lt;- mean( (train$y - yhat)^2 ) )
```

```
[1] 0.0002085353
```


---



```r
n &lt;- nrow(train)
ds = 0:(n-1)
ps = ds + 1

# function to fit polynomial model of degree d
*fun &lt;- function(d) if (d==0) lm(y~1, train) else lm(y~poly(x,degree=d, raw=T), train)
*fits &lt;- sapply(ds, fun)

# compute MSE.tr for all degrees
MSEs.tr &lt;- unlist( lapply(fits, deviance) )/n
plot(ps, MSEs.tr, type="b", xlab="p", ylab="MSE.tr")
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-7-1.png" width="50%" style="display: block; margin: auto;" /&gt;


---



```r
# compute predictions for all degrees
yhats &lt;- lapply(fits, predict)

# compute MSE.te for all degrees
MSEs.te &lt;- unlist(lapply(yhats, 
           function(yhat) mean((test$y - yhat)^2)
           ))

# plot
plot(ps, MSEs.te, type="b", col=4, xlab="p", ylab="MSE.te")
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-8-1.png" width="50%" style="display: block; margin: auto;" /&gt;

```r
which.min(MSEs.te)
```

```
[1] 5
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
