<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Introduction to the course
### Aldo Solari

---





# Outline

* Course overview
* The two cultures
* Polynomials

---
layout: false
class: inverse, middle, center

# Course overview

---

# Teacher

Aldo Solari

**E-mail** aldo.solari@unimib.it

**Office hours** Wednesday, 17:30-18:30, room 2030, building U7, II floor

**Personal page** https://aldosolari.github.io/

**Course page** https://aldosolari.github.io/DM/


| When | Where | Hours |
|---|---|----|
| Monday | Lab717 | 13:30-15:30 |
| Thursday | Lab717 | 13:30-16:30|
| Friday | Lab717 | 9:30-12:30 |
.center[**Time table**]

---

# The course


* This course aims to provide **statistical** and **computational** tools for data mining and supervised learning by using the R software environment for statistical computing

* In particular, you will learn how to use **statistical (machine) learning** for buildind **predictive models** that generate accurate predictions for future, yet-to-be-seen data

* This framework includes 
    - data visualization
    - exploratory data analysis
    - feature engineering
    - splitting the data into training and testing sets
    - building models
    - selecting an approach for identifying optimal tuning parameters
    - estimating predictive performance

---

# Exam

* The exams consists of two parts:

    - **Test** (in lab)
        - Questions about theory (open and/or multiple choice)
        - Data Analysis in R 
  
    - **Competitions** (homework)
        - Attending students can form a team (max. 3 persons)

* The final grade is a weighted average of Test (25% Questions, 25% Data Analysis) and Competitions (50%)

* You must reach a **minimum passing score** for the Questions 

---


# Competitions

* Participating in predictive modelling __competitions__ can help you gain practical experience and improve your data modelling skills in various domains such as credit, insurance, marketing, sales' forecasting etc. 

* At the same time you get to do it in a competitive context against dozens of participants where each one tries to build the most predictive algorithm

* Inspired by [kaggle](https://www.kaggle.com/), the world's largest community of data scientists and machine learners

* Since 2015, this course utilises [BeeViva](http://www.bee-viva.com/competitions), an Italian public data platform for hosting competitions

* Beeviva is also used for the event [Stats Under the Stars](http://sus.stat.unipd.it/)

---

# Stats Under the Stars 

.pull-left[

![](images/SUS.JPG)
.center[Caff√® Pedrocchi, Padua]

]

.pull-left[
* [SUS](http://sus.stat.unipd.it/) Padua 8.9.15)
* [SUS^2](http://www.labeconomia.unisa.it/sus2/) Salerno 7.6.16
* [SUS^3](http://local.disia.unifi.it/sus3/) Florence 27.6.17
* [SUS^4](https://www.unipa.it/dipartimenti/seas/sus4/) Palermo 19.6.18
* [SUS^5](http://www.sus5.unibocconi.eu/wps/wcm/connect/Site/SUS5/Home) Milan 18.6.19
* [SUS^6] Pisa
]


---

# SUS5


&lt;img src="images/sus5winners.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;
.center[Team **GoldFisher** (from the left): Beatrice Somaschini, Federico Melograna, Alice Giampino, Nicholas Missineo  - **Best Predictive Performance**]


---

# Books

Primary references:

- [Analisi dei dati e data mining](http://azzalini.stat.unipd.it/Book-DM/index.html) (AS)

- [Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) (ISLR)

Advanced reading:

- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) (ESL)

For improving R programming and predictive modeling skills: 

- [R for Data Science](http://r4ds.had.co.nz/) (r4ds)

- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) (APM)

- [Feature Engineering and Selection](http://www.feat.engineering/) (FEV)

---
layout: false
class: inverse, middle, center

# The two cultures

---

# The two cultures

1. Data Mining and Statistics: What's the connection?

2. Statistical Modeling: The Two Cultures

3. 50 years of Data Science

4. Data Science vs. Statistics: Two Cultures?


**Reading**: 

1. [Friedman (1998)](http://docs.salford-systems.com/dm-stat.pdf)

2. [Breiman (2001)](http://www2.math.uu.se/~thulin/mm/breiman.pdf)

3. [Donoho (2015)](https://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf)

4. [Carmichael and Marron (2018)](https://arxiv.org/pdf/1801.00371.pdf)

---

# Two goals

Leo Breiman (2001) says

&gt; Statistics starts with data. Think of the data as being generated by a black box in
which a vector of input variables `\(x\)` go in one side, and on the
other side the response variables `\(y\)` come out. Inside the black box, nature functions to
associate the predictor variables with the response variables...
There are two goals in analyzing the data:

1. **Prediction**
To be able to predict what the responses are going to be to future input
variables
  - Machine learning algorithms
  - 2% of the academic statisticians
  
2. **Inference** 
To infer how nature is associating the response variables to the input variables
  - Stochastic models (true model generating the data)
  - 98% of the academic statisticians

---

# To explain or to predict?

**True model**

`$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon$$`

**Underspecified model**

`$$Y = \gamma_0+ \gamma_1 X_1 + \epsilon$$`

Explanation requires estimating the coefficients of the true model,
but **a wrong model can sometimes predict better**:
  - when the predictors `\(X_1\)` and `\(X_2\)` are highly correlated
  - when the data are very noisy
  - when `\(\beta_2\)` is small
  
**Reading**: 

[Shmueli (2010)](https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf)

---

# Rock-Paper-Scissor algorithm

&lt;img src="images/rps.jpg" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

.center[http://www.nytimes.com/interactive/science/rock-paper-scissors.html]

---

# What the computer is thinking

* Four last throws

    - **PAPER, SCISSORS, SCISSORS, ROCK** &lt;- HUMAN 

    - **SCISSORS, PAPER, PAPER, SCISSORS** &lt;- COMPUTER

* COMPUTER is going to search over 200,000 rounds of Rock-Paper-Scissors data and find
all the times that HUMAN played **P, S, S, R**  when COMPUTER played **S, P, P, S**

* Of all the times HUMANS played **P, S, S, R**  when COMPUTER played **S, P, P, S**, HUMANS played **PAPER** at their next throw the most. 

* COMPUTER thinks that you will do the same this time

---

# Is this machine learning or statistics?

* **Theorem** 
A truly random game would result in a tie

* **Hypothesis** 
A human is not truly random

* **Algorithm** 
Learn humans non-random patterns from the data

* **Counter-attack?** 

---

# Same thing, different name?

| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| target variable | `\(Y\)` | response variable |
| attribute, feature | `\(X\)` | predictor, explanatory variable |
| supervised learning | model `\(Y\)` as a function of `\(X\)` | regression |
| hypothesis | `\(Y= f(X) + \varepsilon\)` | model, regression function |
| instances, examples | `\((Y_1,X_1),\ldots,(Y_n,X_n)\)` | samples, observations |
| learning | `\(\displaystyle \hat{f} = \underset{f}{\arg\min} \sum_{i=1}^{n} \mathrm{loss}(Y_i,f(X_i))\)` | estimation, fitting |
| classification | `\(\hat{Y} = \hat{f}(X)\)` | prediction |
| generalization error | `\(\mathbb{E} [\, \mathrm{loss}(Y,\hat{f}(X))\,]\)` | risk |



---

# So, what's the difference? 


| Machine Learning       |             | Statistics | 
|------------------------|--------------------|-------------|
| | FOCUS | |
|prediction |  | inference |
| | CULTURE | |
|algorithmic/optimization |  | modeling |
| | METHODS |  |
|decision trees |  | linear/logistic regression |
|k-nearest-neighbors | | discriminant analysis |
|neural networks | | mixed models |
|support vector machines || ridge/lasso regression |
|adaboost | | GAM |
|...||...|
|random forests | | random forests |

---

# Working together

* Leo Breiman's fundamental contributions:
    - Bagging
    - Random Forests
    - Boosting

* Breiman's work helped to bridge the gap between statistical
modeling and machine learning

* __Statistical Learning__ = __Statistical__ Modeling + Machine __Learning__

---

# Statistical learning

* **Unsupervised learning**
    - the data consists of `\(p\)` variables `\(X_1,\ldots,X_p\)`; no variable has a special status
    - the goal is 
          - clustering
          - dimensionality reduction
          - etc.

* **Supervised learning**
    - the data consists of both response `\(Y\)` and predictors `\(X_1,\ldots,X_p\)`
    - the problem is called supervised learning since the response supervises the learning process
    - the goal is prediction of the response
        - `\(Y\)` continuous : __regression__ problem 
        - `\(Y\)` binary/multi-class : __classification__ problem 

---

# Evolution of multivariate statistics

1. **Classic**
    - **Multivariate Analysis** &lt;br&gt;
      Books by Anderson (1958) and Mardia, Kent &amp; Bibby (1979)

    - **Statistical Modeling** &lt;br&gt;
      Nelder &amp; Wedderburn (1972) paper on GLM

2. **Computer-age**
    - **Data Mining**: process of discovering patterns in data
    
    - **Machine Learning**: algorithms that can learn from data

3. **Modern**
    - **Statistical Learning** &lt;br&gt;
      Hastie, Tibshirani &amp; Friedman ESL (2001) book
      
    - **Data Science**
    

---
layout: false
class: inverse, middle, center

# Data science

---

# What is data science?

&lt;img src="images/datascience.png" width="60%" height="60%" style="display: block; margin: auto;" /&gt;


---

# Data science

&gt; is an interdisciplinary field [...]

&gt; to extract knowledge or insights from data [...]

&gt; Wikipedia

---

# Interdisciplinary field

![](images/venn.png)

---

# Statistics ‚äÇ Data science

&gt; While statistics - as the science of learning from data -
is necessary for turning data into knowledge and action,
it's not the only critical component within data science

&gt; When statistics, database management, and
distributed/parallel computing combine, we will see
growth in cross-trained experts who are better equipped
to solve complex challenges in today's massive data
revolution

&gt; Jessica Utts, ASA president

---

# Key word

Most people hyping Data Science have focused on the first word:
Data

&gt; The key word in "Data Science" is not Data, it is Science

&gt; Jeff Leek 

Data Science is only useful when the data are used to answer a
scientific question

---

# The future of data analysis 

&lt;img src="images/tukey.jpg" width="60%" height="60%" style="display: block; margin: auto;" /&gt;
.center[John Tukey (1915-2000)]

Reading: 

[Tukey (1962)](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177704711)


---

# Is Gauss a data scientist?


&lt;img src="images/gauss.jpg" width="30%" height="30%" style="display: block; margin: auto;" /&gt;
.center[Carl Friedrich Gauss (1777 - 1855)]


---

# Gauss' problems

__Astronomy problem__ 

Predict the position of the asteroid Ceres at 31 December 1801 on the basis of data provided by the italian astronomer Giuseppe Piazzi

__Statistical problem__

Find `\(\boldsymbol{\beta}\)` such that minimizes `\((\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\)`

Gauss solution: least squares `\(\hat{\boldsymbol{\beta}}=(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}\)`


__Computational problem__

Solve (by hand!) the system of equations `\(\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T} \mathbf{y}\)`

Gauss solution: fast optimization algorithm (Gaussian elimination)

---
layout: false
class: inverse, middle, center

# Polynomials

---

# Training set and test set

* From AS, chapter 3 (Yesterday-tomorrow data)

* Yesterday we observed `\(n=30\)` pairs of data, the __training set__
`$$(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$$` 

* Tomorrow we will observe `\(n=30\)` pairs of data, the __test set__
`$$(x_1,y^*_1), (x_2,y^*_2), \ldots, (x_n,y^*_n)$$`


```
    x.train   y.train    x.test y.test
1 0.5000000 0.4269584 0.5000000     NA
2 0.5862069 0.4841696 0.5862069     NA
3 0.6724138 0.4957393 0.6724138     NA
4 0.7586207 0.5438164 0.7586207     NA
5 0.8448276 0.5379404 0.8448276     NA
6 0.9310345 0.5375140 0.9310345     NA
```



---

# Fixed-X view

For simplicity:

* the training predictor values `\(x_1,\ldots,x_n\)` are treated as fixed (i.e. non-random)
* the test predictor values `\(x_1,\ldots,x_n\)` exactly match the the training predictor values `\(x_1,\ldots,x_n\)`

Reading: 
[Rosset and Tibshirani (2018)](https://arxiv.org/pdf/1704.08160.pdf)

The responses in training data were generated artificially by 
`$$Y_i = f(x_i)+ \varepsilon_i, \quad i=1,\ldots,n$$`
and the responses in test data by
`$$Y^*_{i}=f(x_i)+\varepsilon^*_i, \quad i=1,\ldots,n$$`
where `\(\varepsilon_1,\ldots,\varepsilon_n\)` and `\(\varepsilon^*_1,\ldots,\varepsilon^*_n\)` are i.i.d. `\(N(0,\sigma^2)\)` with `\(\sigma=0.01\)` and `\(f\)` is a function which we leave unspecified
  

---



```r
# import data
library(readr)
*df &lt;- read_table2("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat")[-31,]
train &lt;- data.frame(x=df$x, y=df$y.yesterday)
test &lt;- data.frame(x=df$x, y=df$y.tomorrow)

# scatterplot
plot( y ~ x , train)
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-7-1.png" width="75%" style="display: block; margin: auto;" /&gt;

---

# Mean squared error

* Suppose we have estimated `\(f\)` by `\(\hat{f}\)`

* For yesterday data (training set), we can compute the __mean squared error__ 
`$$\mathrm{MSE}_{\mathrm{Tr}} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2$$` 

* However, we would like to have a good performance on tomorrow data (test set)
`$$\mathrm{MSE}_{\mathrm{Te}} = \frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2$$` 


---

# Polynomials


* Consider the polynomial regression model of degree `\(d\)`
`$$f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_d x^{d}$$`

* Use the training set to get the estimate 
`$$\hat{f}(x)=\hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_d x^{d}$$` and predict tomorrow `\(y_i^*\)` by using
`$$\hat{y}_i^*=\hat{f}(x_i), \quad i=1,\ldots,n$$`

* Find the degree `\(d\)` that minimizes the expected value of  `\(\mathrm{MSE}_{\mathrm{Te}}=\frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2\)`


---


```r
# 3rd degree polynomial regression fit
*fit &lt;- lm( y ~ poly(x, degree=3), train)
*yhat &lt;- predict(fit, newdata=test)

# plot
plot( y ~ x , train)
lines( yhat ~ x, train)
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-8-1.png" width="75%" style="display: block; margin: auto;" /&gt;

```r
# MSE.tr
( MSE.tr &lt;- mean( (train$y - yhat)^2 ) )
```

```
[1] 0.0002085353
```


---



```r
n &lt;- nrow(train)
ds = 0:(n-1)
ps = ds + 1

# function to fit polynomial model of degree d
*fun &lt;- function(d) if (d==0) lm(y~1, train) else lm(y~poly(x,degree=d, raw=T), train)
*fits &lt;- sapply(ds, fun)

# compute MSE.tr for all degrees
MSEs.tr &lt;- unlist( lapply(fits, deviance) )/n
plot(ps, MSEs.tr, type="b", xlab="p", ylab="MSE.tr")
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-9-1.png" width="75%" style="display: block; margin: auto;" /&gt;


---



```r
# compute predictions for all degrees
yhats &lt;- lapply(fits, predict)
# compute MSE.te for all degrees
MSEs.te &lt;- unlist(lapply(yhats, 
           function(yhat) mean((test$y - yhat)^2)
           ))
# plot
plot(ps, MSEs.te, type="b", col=4, xlab="p", ylab="MSE.te")
```

&lt;img src="1_Intro_files/figure-html/unnamed-chunk-10-1.png" width="75%" style="display: block; margin: auto;" /&gt;

```r
# best?
ps[which.min(MSEs.te)]
```

```
[1] 5
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
