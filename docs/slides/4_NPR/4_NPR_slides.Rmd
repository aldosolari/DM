---
title: "Non-parametric methods, the caret R package and Random-X prediction error"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=T, R.options=list(width=220))
```

# Outline

* Non-parametric methods: kNN
* the caret package
* Random-X prediction error

---

# Nonparametric methods

* Non-parametric methods do not make explicit assumptions about the functional
form of $f$ (e.g. polynomials)

* Leave data __speak for themselves__ in a free way

* Advantage: by avoiding the assumption of a particular functional form for $f$, they have a wider range of possible shapes for $f$.

* Disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations is required

---
layout: false
class: inverse, middle, center

# k-nearest neighbors 

---

# k-nearest neighbors

* A very simple and quite commonly used method is __k-nearest neighbors__ (kNN)

* Suppose we want to make a prediction at some $x^*_1$. Define the neighbourhood $N_k(x^*_1)$ to be the set of $k$ training observations having values $x_i$ closest to $x^*_1$ in Euclidean norm $\| x_i - x^*_1 \|=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)}$

$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$
* The number $k$ is a __tuning parameter__ 

* Small $k$ corresponds to a more flexible fit

* Large k corresponds to a less flexible fit

* Since we are computing a distance, usually we __center__ and __scale__ the predictors

---

# House prices: 5-Nearest Neighbors Model

![](images/5NN_ames.png)

---

# Exercise

Consider the $n=250$ points version of the yesterday-tomorrow data:

* the true $f$, named `f.true250`, is given [here](http://azzalini.stat.unipd.it/Book-DM/f_true.R)

* the true $\sigma$ is $0.01$

* the $x$ values are given by `x = seq(0.5,3,length=250)`

```{r, echo=F}
rm(list=ls())
n = 250
sigmatrue = 0.01
ftrue <- c(
   0.4342,0.4402,0.4460,0.4515,0.4568,0.4618,0.4665,0.4712,0.4755,0.4797,
   0.4837,0.4875,0.4911,0.4945,0.4978,0.5009,0.5039,0.5067,0.5094,0.5119,
   0.5143,0.5166,0.5187,0.5208,0.5227,0.5245,0.5262,0.5278,0.5293,0.5307,
   0.5321,0.5333,0.5345,0.5355,0.5365,0.5375,0.5383,0.5391,0.5399,0.5405,
   0.5411,0.5417,0.5422,0.5426,0.5430,0.5434,0.5437,0.5440,0.5442,0.5444,
   0.5445,0.5447,0.5447,0.5448,0.5448,0.5448,0.5448,0.5447,0.5446,0.5445,
   0.5444,0.5442,0.5441,0.5439,0.5437,0.5434,0.5432,0.5429,0.5427,0.5424,
   0.5421,0.5418,0.5415,0.5412,0.5408,0.5405,0.5401,0.5398,0.5394,0.5390,
   0.5387,0.5383,0.5379,0.5375,0.5371,0.5367,0.5363,0.5359,0.5355,0.5351,
   0.5347,0.5343,0.5339,0.5335,0.5330,0.5326,0.5322,0.5318,0.5314,0.5310,
   0.5306,0.5302,0.5298,0.5294,0.5290,0.5286,0.5282,0.5278,0.5274,0.5270,
   0.5267,0.5263,0.5259,0.5255,0.5251,0.5248,0.5244,0.5240,0.5237,0.5233,
   0.5230,0.5226,0.5223,0.5219,0.5216,0.5213,0.5209,0.5206,0.5203,0.5200,
   0.5197,0.5193,0.5190,0.5187,0.5184,0.5181,0.5178,0.5176,0.5173,0.5170,
   0.5167,0.5164,0.5162,0.5159,0.5156,0.5154,0.5151,0.5149,0.5146,0.5144,
   0.5141,0.5139,0.5136,0.5134,0.5132,0.5130,0.5127,0.5125,0.5123,0.5121,
   0.5119,0.5117,0.5115,0.5113,0.5111,0.5109,0.5107,0.5105,0.5103,0.5101,
   0.5100,0.5098,0.5096,0.5094,0.5093,0.5091,0.5090,0.5088,0.5086,0.5085,
   0.5083,0.5082,0.5080,0.5079,0.5077,0.5076,0.5075,0.5073,0.5072,0.5071,
   0.5069,0.5068,0.5067,0.5066,0.5065,0.5063,0.5062,0.5061,0.5060,0.5059,
   0.5058,0.5057,0.5056,0.5055,0.5054,0.5053,0.5052,0.5051,0.5050,0.5049,
   0.5048,0.5047,0.5046,0.5045,0.5044,0.5044,0.5043,0.5042,0.5041,0.5041,
   0.5040,0.5039,0.5038,0.5038,0.5037,0.5036,0.5035,0.5035,0.5034,0.5034,
   0.5033,0.5032,0.5032,0.5031,0.5030,0.5030,0.5029,0.5029,0.5028,0.5028,
   0.5027,0.5027,0.5026,0.5026,0.5025,0.5025,0.5024,0.5024,0.5023,0.5023)
x = seq(0.5,3,length=n)
```

Obtain the training set and test set by

```{r}
set.seed(123)
y = ftrue + rnorm(n, 0, sigmatrue)
ystar = ftrue + rnorm(n, 0, sigmatrue)
train = data.frame(x=x, y=y)
test = data.frame(x=x, y=ystar)
```

---

Install the R package `kknn` and perform kNN with $k=21$ by using the function 
`kknn(y ~ x, train, test, kernel = "rectangular", k = 21)`


```{r, echo=F}
library(kknn)
k = 21
fit = kknn(y ~ x, train, test, kernel = "rectangular", k = k)
yhat = fit$fitted.values
plot(y ~ x, train, main=paste("k = ", k))
lines(x,yhat, col=4, type="s")
```

---

Compute MSE.tr and MSE.te for $k=21$

```{r, echo=F}
MSE.tr = mean( (train$y - yhat)^2 )
MSE.tr
MSE.te = mean( (test$y - yhat)^2 )
MSE.te
```

---

Compute MSE.tr and MSE.te for $k=1,2,\ldots,40$ and find the $k$ corresponding to the minimum MSE.te

```{r, echo=F}
ks = 1:40
MSEs.tr = sapply(ks, function(k)
mean( 
  (train$y - kknn(y ~ x, train, test, kernel = "rectangular", k = k)$fitted.values )^2
))
plot(1/ks, MSEs.tr, type="b", ylab="MSE.tr", xlab="1/k")
MSEs.te = sapply(ks, function(k)
mean( 
  (test$y - kknn(y ~ x, train, test, kernel = "rectangular", k = k)$fitted.values)^2
))
plot(1/ks,MSEs.te,type="b", ylab="MSE.te", col=4, xlab="1/k")
ks[which.min(MSEs.te)]
```

---

In the fixed-X setting, theoretical results show that for kNN
$$\mathrm{OptF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) - \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))= \frac{2\sigma^2}{k}$$
Find the $k$ corresponding to the minimum $$\widehat{\mathrm{Err}} = \mathrm{MSE}_{\mathrm{Tr}}  + \frac{2\sigma^2}{k}$$

```{r, echo=F}
hatErr = MSEs.tr + (2*sigmatrue^2)/ks
ks[which.min(hatErr)]
```

---

In the fixed-X setting, theoretical results show that for kNN
$$\mathrm{Variance}(\hat{f})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2}{k}$$
Remember that the expected value $\mathbb{E}\hat{f}(x_i)$ can be obtained by fitting the kNN model using $y_i = f(x_i)$. This allows to compute the squared bias $[\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2$. Compute the redubile error $[\mathrm{Bias}(\hat{f})]^2 + \mathrm{Variance}(\hat{f})$ for $k=1,2,\ldots,40$ and find the $k$ that minimize it. 
```{r, echo=F}
Vars = sigmatrue^2/ks
Bias2s = sapply(ks, function(k) 
  mean( 
    ( ftrue - kknn(ftrue ~ x, train, test, kernel = "rectangular", k = k)$fitted.values )^2 
      )
  )
Reds = Bias2s+Vars 
barplot(Reds, ylab="Reducible error", names.arg=ks)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
ks[which.min(Reds)]
```

---
layout: false
class: inverse, middle, center

# The caret package

---

# The caret package

The [caret](http://topepo.github.io/caret/index.html) package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

as well as other functionality.

---

# Install and help

* The package utilizes a number of R packages but tries not to load them all at package start-up. The package "suggests" field includes 76 packages

* __Not now!__ but full installation by 
```{r, eval=F}
 install.packages("caret", dependencies = c("Depends", "Suggests"))
```

* The main help pages for the package are at [https://topepo.github.io/caret/](https://topepo.github.io/caret/)

* The book [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) can be view as an introduction to the `caret` package.

---

# train function pseudocode

![](https://i2.wp.com/www.quintuitive.com/wp-content/uploads/2016/09/TrainAlgo.png)


---


```{r}
library(caret)
fit.knn = train(
  y ~ ., train,
  method = "knn")
# result
fit.knn
```

---

.pull-left[
```{r}
rcv <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2)

fit.knn = train(
  y ~ ., train,
  method = "knn",
  trControl=rcv,
  tuneLength = 10)
```
]

.pull-right[
```{r}
plot(fit.knn)
fit.knn$bestTune
```
]

---

.pull-left[
```{r}
fit.poly = train(
  y ~ poly(x,degree=6), train,
  method = "lm",
  trControl=rcv)
models = list(
    knn=fit.knn,
    poly=fit.poly
  )
resamps = resamples(models)
```
]

.pull-right[
```{r}
bwplot(resamps, metric = "RMSE")
yhats = predict(models, newdata=test)
lapply(yhats, function(yhat) sqrt( mean( (yhat - test$y)^2) ) )
```
]

---
layout: false
class: inverse, middle, center

# Random-X prediction error

---

# Random-X prediction error

* Response $Y$
* Predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$
* $(X,Y)$ have some unknown joint distribution
* True model $Y = f(X) + \varepsilon$
where $f(X)=\mathbb{E}(Y|X)$ is the __regression function__ and $\varepsilon$ is the error term independent from $X$ with $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2$
* Training set: $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$
* Test set: $(x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$ i.i.d. from $(X,Y)$

The __Random-X prediction error__ is given by
$$\mathrm{ErrR}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}(Y^*_i - \hat{f}(X^*_i))^2\right] = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2]$$
where the last equality follows by exchangeability and the expectation of the last term is with respect to the training set $(X_1,Y_1),\ldots,(X_n,Y_n)$ and to the new test point $(X^*_1,Y^*_1)$


---

# Gaussian example

For example, suppose that 
$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
\mu_y \\
\mu_x \\
\end{array}\right), \left(\begin{array}{cc} 
\sigma^2_y & \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  & \sigma^2_x \\
\end{array}\right)\right)$$
so that the conditional distribution of $Y$ given $X=x$ is
$$(Y|X=x) \sim N\Big(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x-\mu_x), \sigma^2_y (1-\rho^2)\Big)$$

---

This corresponds to generate the training data as follows:

For $i=1,\ldots,n$:

1. $x_i$ is the realization of $X_i \sim N(\mu_x,\sigma^2_x)$ 
2. $y_i$ is the realization of $(Y_i|X_i=x_i) = f(x_i) + \varepsilon_i$ where 
    - $f(x_i)=\alpha + \beta x_i$ with $\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)$ and $\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)$
    - $\varepsilon_i \sim N(0,\sigma^2)$ with $\sigma^2=\sigma^2_y (1-\rho^2)$

and the test data as

For $i=1,\ldots,m$:

1. $x^*_i$ is the realization of $X^*_i \sim N(\mu_x,\sigma^2_x)$ 
2. $y^*_i$ is the realization of $(Y^*_i|X^*_i=x^*_i) = f(x^*_i) + \varepsilon^*_i$ where 
    - $f(x^*_i)=\alpha + \beta x^*_i$ with $\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)$ and $\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)$
    - $\varepsilon^*_i \sim N(0,\sigma^2)$ with $\sigma^2=\sigma^2_y (1-\rho^2)$


---

Suppose that $\mu_x=\mu_y=0$, $\rho=0.5$, $\sigma_x=1$, $\sigma_y=2$. Then

* $X\sim N(0,1)$
* $Y|X=x \sim N(\beta x,\sigma^2)$ with $\beta=1$ and $\sigma^2=3$


Consider the estimator  
$$\hat{y^*_1} = \hat{f}(x^*_1) =  \hat{\beta} x_1^*$$
where the estimate of $\beta$ is given by
$$\displaystyle \hat{\beta} = \frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2 }$$
where $(x_1,y_1),\ldots,(x_n,y_n)$ is the traing set with $n>2$

---

Show that

1. ErrR conditional to the training set is given by
$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | (X_1,Y_1) = (x_1,y_1), \ldots, (X_n,Y_n) = (x_n,y_n)] = \sigma^2 + (\beta-\hat{\beta})^2$$

2. ErrR conditional to $X_1^*,X_1,\ldots,X_n$ is given by
$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | X^*_1 = x^*_1, X_1 = x_1, \ldots, X_n = x_n] = \sigma^2 + \sigma^2\left(\frac{(x_1^*)^2}{\sum_{i=1}^{n}x^2_i}\right)$$

3. ErrR is given by
$$\mathrm{ErrR} = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2] =  \sigma^2 + \frac{\sigma^2}{n-2}$$

---

```{r}
n = 20
beta = 1
sigma = sqrt(3)
set.seed(123)
# observed training set
x = rnorm(n)
y = beta*x + rnorm(n,0,sigma)
# estimate
hatbeta <- sum(x*y)/sum(x^2)
# observed test point
x1star = rnorm(1)
y1star = beta*x1star + rnorm(1,0,sigma)
# error
(y1star - hatbeta*x1star)^2
```

---

```{r}
# 1.
B = 5000
set.seed(123)
x1stars = rnorm(B,0,1)
y1stars = beta*x1stars + rnorm(B,0,sigma)
# empirical
mean( ( y1stars - hatbeta*x1stars )^2 )
# theoretical
sigma^2 + (beta - hatbeta)^2
```

---

```{r}
# 2.
set.seed(123)
sim2 = function(n){
  xs = c(x,x1star)
  ys = xs*beta + rnorm(n+1,0,sigma)
  hatbeta <- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim2(n)))
# theoretical
sigma^2 + (sigma^2)*(x1star^2/sum(x^2))
```

---

```{r}
# 3.
set.seed(123)
sim3 = function(n){
  xs = rnorm(n+1,0,1)
  ys = beta*xs + rnorm(n+1,0,sigma)
  hatbeta <- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim3(n)))
# theoretical
sigma^2 + (sigma^2)/(n-2)
```



