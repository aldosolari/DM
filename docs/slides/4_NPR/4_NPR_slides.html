<!DOCTYPE html>
<html>
  <head>
    <title>Non-parametric methods and the caret R package</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Non-parametric methods and the caret R package
### Aldo Solari

---




# Outline

* Non-parametric methods: kNN
* the caret package

---

# Nonparametric methods

* Non-parametric methods do not make explicit assumptions about the functional
form of `\(f\)` (e.g. polynomials)

* Leave data __speak for themselves__ in a free way

* Advantage: by avoiding the assumption of a particular functional form for `\(f\)`, they have a wider range of possible shapes for `\(f\)`.

* Disadvantage: since they do not reduce the problem of estimating `\(f\)` to a small number of parameters, a very large number of observations is required

---
layout: false
class: inverse, middle, center

# k-nearest neighbors 

---

# k-nearest neighbors

* A very simple and quite commonly used method is __k-nearest neighbors__ (kNN)

* Suppose we want to make a prediction at some `\(x^*_1\)`. Define the neighbourhood `\(N_k(x^*_1)\)` to be the set of `\(k\)` training observations having values `\(x_i\)` closest to `\(x^*_1\)` in Euclidean norm `\(\| x_i - x^*_1 \|=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)}\)`

`$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$`
* The number `\(k\)` is a __tuning parameter__ 

* Small `\(k\)` corresponds to a more flexible fit

* Large k corresponds to a less flexible fit

* Since we are computing a distance, usually we __center__ and __scale__ the predictors

---

# House prices: 5-Nearest Neighbors Model

![](images/5NN_ames.png)

---

# Exercise

Consider the `\(n=250\)` points version of the yesterday-tomorrow data:

* the true `\(f\)`, named `f.true250`, is given [here](http://azzalini.stat.unipd.it/Book-DM/f_true.R)

* the true `\(\sigma\)` is `\(0.01\)`

* the `\(x\)` values are given by `x = seq(0.5,3,length=250)`



Obtain the training set and test set by


```r
set.seed(123)
y = ftrue + rnorm(n, 0, sigmatrue)
ystar = ftrue + rnorm(n, 0, sigmatrue)
train = data.frame(x=x, y=y)
test = data.frame(x=x, y=ystar)
```

---

Install the R package `kknn` and perform kNN with `\(k=21\)` by using the function 
`kknn(y ~ x, train, test, kernel = "rectangular", k = 21)`


![](4_NPR_slides_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

Compute MSE.tr and MSE.te for `\(k=21\)`


```
[1] 0.0001116343
```

```
[1] 0.0001333713
```

---

Compute MSE.tr and MSE.te for `\(k=1,2,\ldots,40\)` and find the `\(k\)` corresponding to the minimum MSE.te

![](4_NPR_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;![](4_NPR_slides_files/figure-html/unnamed-chunk-5-2.png)&lt;!-- --&gt;

```
[1] 9
```

---

In the fixed-X setting, theoretical results show that for kNN
`$$\mathrm{OptF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) - \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))= \frac{2\sigma^2}{k}$$`
Find the `\(k\)` corresponding to the minimum `$$\widehat{\mathrm{Err}} = \mathrm{MSE}_{\mathrm{Tr}}  + \frac{2\sigma^2}{k}$$`


```
[1] 8
```

---

In the fixed-X setting, theoretical results show that for kNN
`$$\mathrm{Variance}(\hat{f})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2}{k}$$`
Remember that the expected value `\(\mathbb{E}\hat{f}(x_i)\)` can be obtained by fitting the kNN model using `\(y_i = f(x_i)\)`. This allows to compute the squared bias `\([\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2\)`. Compute the redubile error `\([\mathrm{Bias}(\hat{f})]^2 + \mathrm{Variance}(\hat{f})\)` for `\(k=1,2,\ldots,40\)` and find the `\(k\)` that minimize it. 
![](4_NPR_slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

```
[1] 9
```

---
layout: false
class: inverse, middle, center

# The caret package

---

# The caret package

The [caret](http://topepo.github.io/caret/index.html) package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

as well as other functionality.

---

# Install and help

* The package utilizes a number of R packages but tries not to load them all at package start-up. The package "suggests" field includes 76 packages

* __Not now!__ but full installation by 

```r
 install.packages("caret", dependencies = c("Depends", "Suggests"))
```

* The main help pages for the package are at [https://topepo.github.io/caret/](https://topepo.github.io/caret/)

* The book [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) can be view as an introduction to the `caret` package.

---

# train function pseudocode

![](https://i2.wp.com/www.quintuitive.com/wp-content/uploads/2016/09/TrainAlgo.png)


---



```r
library(caret)
fit.knn = train(
  y ~ ., train,
  method = "knn")
# result
fit.knn
```

```
k-Nearest Neighbors 

250 samples
  1 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 250, 250, 250, 250, 250, 250, ... 
Resampling results across tuning parameters:

  k  RMSE        Rsquared   MAE        
  5  0.01146809  0.7261602  0.008978840
  7  0.01105863  0.7438812  0.008674320
  9  0.01095533  0.7485859  0.008545349

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.
```

---

.pull-left[

```r
rcv &lt;- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2)

fit.knn = train(
  y ~ ., train,
  method = "knn",
  trControl=rcv,
  tuneLength = 10)
```
]

.pull-right[

```r
plot(fit.knn)
```

![](4_NPR_slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

```r
fit.knn$bestTune
```

```
  k
3 9
```
]

---

.pull-left[

```r
fit.poly = train(
  y ~ poly(x,degree=6), train,
  method = "lm",
  trControl=rcv)
models = list(
    knn=fit.knn,
    poly=fit.poly
  )
resamps = resamples(models)
```
]

.pull-right[

```r
bwplot(resamps, metric = "RMSE")
```

![](4_NPR_slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

```r
yhats = predict(models, newdata=test)
lapply(yhats, function(yhat) sqrt( mean( (yhat - test$y)^2) ) )
```

```
$knn
[1] 0.01073157

$poly
[1] 0.01008743
```
]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
