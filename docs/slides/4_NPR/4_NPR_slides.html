<!DOCTYPE html>
<html>
  <head>
    <title>Non-parametric methods, the caret R package and Random-X prediction error</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Non-parametric methods, the caret R package and Random-X prediction error
### Aldo Solari

---




# Outline

* Non-parametric methods: kNN
* the caret package
* Random-X prediction error

---

# Nonparametric methods

* Non-parametric methods do not make explicit assumptions about the functional
form of `\(f\)` (e.g. polynomials)

* Leave data __speak for themselves__ in a free way

* Advantage: by avoiding the assumption of a particular functional form for `\(f\)`, they have a wider range of possible shapes for `\(f\)`.

* Disadvantage: since they do not reduce the problem of estimating `\(f\)` to a small number of parameters, a very large number of observations is required

---
layout: false
class: inverse, middle, center

# k-nearest neighbors 

---

# k-nearest neighbors

* A very simple and quite commonly used method is __k-nearest neighbors__ (kNN)

* Suppose we want to make a prediction at some `\(x^*_1\)`. Define the neighbourhood `\(N_k(x^*_1)\)` to be the set of `\(k\)` training observations having values `\(x_i\)` closest to `\(x^*_1\)` in Euclidean norm `\(\| x_i - x^*_1 \|=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)}\)`

`$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$`
* The number `\(k\)` is a __tuning parameter__ 

* Small `\(k\)` corresponds to a more flexible fit

* Large k corresponds to a less flexible fit

* Since we are computing a distance, usually we __center__ and __scale__ the predictors

---

# House prices: 5-Nearest Neighbors Model

![](images/5NN_ames.png)

---

# Exercise

Consider the `\(n=250\)` points version of the yesterday-tomorrow data:

* the true `\(f\)`, named `f.true250`, is given [here](http://azzalini.stat.unipd.it/Book-DM/f_true.R)

* the true `\(\sigma\)` is `\(0.01\)`

* the `\(x\)` values are given by `x = seq(0.5,3,length=250)`



Obtain the training set and test set by


```r
set.seed(123)
y = ftrue + rnorm(n, 0, sigmatrue)
ystar = ftrue + rnorm(n, 0, sigmatrue)
train = data.frame(x=x, y=y)
test = data.frame(x=x, y=ystar)
```

---

Install the R package `kknn` and perform kNN with `\(k=21\)` by using the function 
`kknn(y ~ x, train, test, kernel = "rectangular", k = 21)`


![](4_NPR_slides_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

Compute MSE.tr and MSE.te for `\(k=21\)`


```
[1] 0.0001116343
```

```
[1] 0.0001333713
```

---

Compute MSE.tr and MSE.te for `\(k=1,2,\ldots,40\)` and find the `\(k\)` corresponding to the minimum MSE.te

![](4_NPR_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;![](4_NPR_slides_files/figure-html/unnamed-chunk-5-2.png)&lt;!-- --&gt;

```
[1] 9
```

---

In the fixed-X setting, theoretical results show that for kNN
`$$\mathrm{OptF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) - \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))= \frac{2\sigma^2}{k}$$`
Find the `\(k\)` corresponding to the minimum `$$\widehat{\mathrm{Err}} = \mathrm{MSE}_{\mathrm{Tr}}  + \frac{2\sigma^2}{k}$$`


```
[1] 8
```

---

In the fixed-X setting, theoretical results show that for kNN
`$$\mathrm{Variance}(\hat{f})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2}{k}$$`
Remember that the expected value `\(\mathbb{E}\hat{f}(x_i)\)` can be obtained by fitting the kNN model using `\(y_i = f(x_i)\)`. This allows to compute the squared bias `\([\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2\)`. Compute the redubile error `\([\mathrm{Bias}(\hat{f})]^2 + \mathrm{Variance}(\hat{f})\)` for `\(k=1,2,\ldots,40\)` and find the `\(k\)` that minimize it. 
![](4_NPR_slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

```
[1] 9
```

---
layout: false
class: inverse, middle, center

# The caret package

---

# The caret package

The [caret](http://topepo.github.io/caret/index.html) package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

as well as other functionality.

---

# Install and help

* The package utilizes a number of R packages but tries not to load them all at package start-up. The package "suggests" field includes 76 packages

* __Not now!__ but full installation by 

```r
 install.packages("caret", dependencies = c("Depends", "Suggests"))
```

* The main help pages for the package are at [https://topepo.github.io/caret/](https://topepo.github.io/caret/)

* The book [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) can be view as an introduction to the `caret` package.

---

# train function pseudocode

![](https://i2.wp.com/www.quintuitive.com/wp-content/uploads/2016/09/TrainAlgo.png)


---



```r
library(caret)
fit.knn = train(
  y ~ ., train,
  method = "knn")
# result
fit.knn
```

```
k-Nearest Neighbors 

250 samples
  1 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 250, 250, 250, 250, 250, 250, ... 
Resampling results across tuning parameters:

  k  RMSE        Rsquared   MAE        
  5  0.01146809  0.7261602  0.008978840
  7  0.01105863  0.7438812  0.008674320
  9  0.01095533  0.7485859  0.008545349

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 9.
```

---

.pull-left[

```r
rcv &lt;- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2)

fit.knn = train(
  y ~ ., train,
  method = "knn",
  trControl=rcv,
  tuneLength = 10)
```
]

.pull-right[

```r
plot(fit.knn)
```

![](4_NPR_slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

```r
fit.knn$bestTune
```

```
  k
3 9
```
]

---

.pull-left[

```r
fit.poly = train(
  y ~ poly(x,degree=6), train,
  method = "lm",
  trControl=rcv)
models = list(
    knn=fit.knn,
    poly=fit.poly
  )
resamps = resamples(models)
```
]

.pull-right[

```r
bwplot(resamps, metric = "RMSE")
```

![](4_NPR_slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

```r
yhats = predict(models, newdata=test)
lapply(yhats, function(yhat) sqrt( mean( (yhat - test$y)^2) ) )
```

```
$knn
[1] 0.01073157

$poly
[1] 0.01008743
```
]

---
layout: false
class: inverse, middle, center

# Random-X prediction error

---

# Random-X prediction error

* Response `\(Y\)`
* Predictors `\(X=(X_1,\ldots,X_p)^\mathsf{T}\)`
* `\((X,Y)\)` have some unknown joint distribution
* True model `\(Y = f(X) + \varepsilon\)`
where `\(f(X)=\mathbb{E}(Y|X)\)` is the __regression function__ and `\(\varepsilon\)` is the error term independent from `\(X\)` with `\(\mathbb{E}(\varepsilon)=0\)` and `\(\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2\)`
* Training set: `\((x_1,y_1),\ldots,(x_n,y_n)\)` i.i.d. from `\((X,Y)\)`
* Test set: `\((x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)\)` i.i.d. from `\((X,Y)\)`

The __Random-X prediction error__ is given by
`$$\mathrm{ErrR}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}(Y^*_i - \hat{f}(X^*_i))^2\right] = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2]$$`
where the last equality follows by exchangeability and the expectation of the last term is with respect to the training set `\((X_1,Y_1),\ldots,(X_n,Y_n)\)` and to the new test point `\((X^*_1,Y^*_1)\)`


---

# Gaussian example

For example, suppose that 
`$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
\mu_y \\
\mu_x \\
\end{array}\right), \left(\begin{array}{cc} 
\sigma^2_y &amp; \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  &amp; \sigma^2_x \\
\end{array}\right)\right)$$`
so that the conditional distribution of `\(Y\)` given `\(X=x\)` is
`$$(Y|X=x) \sim N\Big(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x-\mu_x), \sigma^2_y (1-\rho^2)\Big)$$`

---

This corresponds to generate the training data as follows:

For `\(i=1,\ldots,n\)`:

1. `\(x_i\)` is the realization of `\(X_i \sim N(\mu_x,\sigma^2_x)\)` 
2. `\(y_i\)` is the realization of `\((Y_i|X_i=x_i) = f(x_i) + \varepsilon_i\)` where 
    - `\(f(x_i)=\alpha + \beta x_i\)` with `\(\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)\)` and `\(\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)\)`
    - `\(\varepsilon_i \sim N(0,\sigma^2)\)` with `\(\sigma^2=\sigma^2_y (1-\rho^2)\)`

and the test data as

For `\(i=1,\ldots,m\)`:

1. `\(x^*_i\)` is the realization of `\(X^*_i \sim N(\mu_x,\sigma^2_x)\)` 
2. `\(y^*_i\)` is the realization of `\((Y^*_i|X^*_i=x^*_i) = f(x^*_i) + \varepsilon^*_i\)` where 
    - `\(f(x^*_i)=\alpha + \beta x^*_i\)` with `\(\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)\)` and `\(\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)\)`
    - `\(\varepsilon^*_i \sim N(0,\sigma^2)\)` with `\(\sigma^2=\sigma^2_y (1-\rho^2)\)`


---

Suppose that `\(\mu_x=\mu_y=0\)`, `\(\rho=0.5\)`, `\(\sigma_x=1\)`, `\(\sigma_y=2\)`. Then

* `\(X\sim N(0,1)\)`
* `\(Y|X=x \sim N(\beta x,\sigma^2)\)` with `\(\beta=1\)` and `\(\sigma^2=3\)`


Consider the estimator  
`$$\hat{y^*_1} = \hat{f}(x^*_1) =  \hat{\beta} x_1^*$$`
where the estimate of `\(\beta\)` is given by
`$$\displaystyle \hat{\beta} = \frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2 }$$`
where `\((x_1,y_1),\ldots,(x_n,y_n)\)` is the traing set with `\(n&gt;2\)`

---

Show that

1. ErrR conditional to the training set is given by
`$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | (X_1,Y_1) = (x_1,y_1), \ldots, (X_n,Y_n) = (x_n,y_n)] = \sigma^2 + (\beta-\hat{\beta})^2$$`

2. ErrR conditional to `\(X_1^*,X_1,\ldots,X_n\)` is given by
`$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | X^*_1 = x^*_1, X_1 = x_1, \ldots, X_n = x_n] = \sigma^2 + \sigma^2\left(\frac{(x_1^*)^2}{\sum_{i=1}^{n}x^2_i}\right)$$`

3. ErrR is given by
`$$\mathrm{ErrR} = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2] =  \sigma^2 + \frac{\sigma^2}{n-2}$$`

---


```r
n = 20
beta = 1
sigma = sqrt(3)
set.seed(123)
# observed training set
x = rnorm(n)
y = beta*x + rnorm(n,0,sigma)
# estimate
hatbeta &lt;- sum(x*y)/sum(x^2)
# observed test point
x1star = rnorm(1)
y1star = beta*x1star + rnorm(1,0,sigma)
# error
(y1star - hatbeta*x1star)^2
```

```
[1] 0.2132094
```

---


```r
# 1.
B = 5000
set.seed(123)
x1stars = rnorm(B,0,1)
y1stars = beta*x1stars + rnorm(B,0,sigma)
# empirical
mean( ( y1stars - hatbeta*x1stars )^2 )
```

```
[1] 3.034306
```

```r
# theoretical
sigma^2 + (beta - hatbeta)^2
```

```
[1] 3.021398
```

---


```r
# 2.
set.seed(123)
sim2 = function(n){
  xs = c(x,x1star)
  ys = xs*beta + rnorm(n+1,0,sigma)
  hatbeta &lt;- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim2(n)))
```

```
[1] 3.077386
```

```r
# theoretical
sigma^2 + (sigma^2)*(x1star^2/sum(x^2))
```

```
[1] 3.078788
```

---


```r
# 3.
set.seed(123)
sim3 = function(n){
  xs = rnorm(n+1,0,1)
  ys = beta*xs + rnorm(n+1,0,sigma)
  hatbeta &lt;- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim3(n)))
```

```
[1] 3.194863
```

```r
# theoretical
sigma^2 + (sigma^2)/(n-2)
```

```
[1] 3.166667
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
