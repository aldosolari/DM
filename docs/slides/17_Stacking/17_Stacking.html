<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Model stacking
### Aldo Solari

---





# Model stacking

* Stacking is a general method to combine models 

* Consider a __library__ of `\(L\)` models `\(\hat{f}_1,\ldots, \hat{f}_L\)` fitted on the training data
`$$(x_1,y_1),\ldots, (x_n,y_n)$$`


* Perhaps by combining their respective efforts, we could get even better prediction than using any
particular one

* The issue then how we might combine them for predicting the test set `\(y^*_1,\ldots,y^*_m\)`

* A linear combination 
`$$\hat{y}^{*}_i = \sum_{l=1}^{L}w_l \hat{f}_l(x^*_i)$$`
requires to define the the weights `\(w_1,\ldots,w_L\)`

---

# Least squares

* The method of least squares provides the weights 
`$$\hat{w}_1,\ldots,\hat{w}_L=  \underset{w_1,\ldots,w_L}{\arg \min\,\,} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}_l(x_i) \right]^2$$`

* However, in this way we fail to take into account for model complexity: models with higher complexity get higher weights

* For example, consider `\(L\)` predictors and let `\(\hat{f}_{l}\)` be the linear model formed the best subset of predictors of size `\(l\)`, `\(l=1,\ldots,L\)`, where best is defined as having the smallest `\(\mathrm{MSE}_{\mathrm{Tr}}\)`

* Then all the weight goes on the largest model, that is, `\(\hat{w}_L = 1\)` and `\(\hat{w}_l = 0\)` for `\(l&lt; L\)`

---

# Stacked regression

* Wolpert (1992) presented an interesting idea, called __stacked generalizations__.  This proposal was translated in statistical language by Breiman, in 1993

* If we exclude `\(y_i\)` in the fitting procedure of the models, then 
 `\(\hat{f}^{-i}_1(x_i),\ldots, \hat{f}^{-i}_L(x_i)\)` do not depend on `\(y_i\)` 

* __Stacked regression__ is a particular case of the model stacking algorithm (next) with `\(\hat{f}_{\mathrm{stack}}=\)` linear model and cross-validation with `\(K=n\)`

* Read [Guide to Model Stacking](https://gormanalysis.com/guide-to-model-stacking-i-e-meta-ensembling/
) by Ben Gorman

---

# Staked regression algorithm

1. Let `\(\hat{f}^{-i}_l(x_i)\)` be the prediction at `\(x_i\)` using model `\(l\)` fitted to the training data with the
`\(i\)`th training observation `\((x_i,y_i)\)` removed

2. Obtain the weights by least squares
`$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$`

3. Compute the predictions for the test data as
`$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$`

---

# Boston data


```r
rm(list=ls())
library(MASS)
set.seed(123)
istrain = rbinom(n=nrow(Boston),size=1,prob=0.5)&gt;0
train &lt;- Boston[istrain,]
n=nrow(train)
test = Boston[!istrain,-14]
test.y = Boston[!istrain,14]
m=nrow(test)
```

The training and test data are 
`$$(x_1,y_1),\ldots,(x_n,y_n),\quad (x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$$`
with `\(n=235\)` and `\(m=271\)` for the Boston data set. 

The response variable is `medv`, and the
predictor variables are `crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat` 

---

Fit a library of `\(L=2\)` models `\(\hat{f}_1\)` and `\(\hat{f}_2\)` to the training set.

The first model `\(\hat{f}_1\)` is a linear model with all predictors


```r
fit1 = lm(medv ~ ., train)
```

The second model `\(\hat{f}_1\)` is a regression tree with all predictors and default settings


```r
library(rpart)
fit2 = rpart(medv ~ ., train)
```

The mean squared error for the test set


```
MSE stack:  21.95002
```

```
MSE lm:  28.83767
```

```
MSE rpart:  24.39206
```




---

# Model stacking algorithm

1. Partition the training data into `\(K\)` folds `\(\mathcal{F}_1,\ldots,\mathcal{F}_K\)`

2. For each test fold `\(\mathcal{F}_k\)`, `\(k=1,\ldots,K\)` ,combine the other `\(K-1\)` folds to be used as a training fold 
    - For `\(l=1,\ldots,L\)`,  fit the `\(l\)`th model to the training fold and make predictions on the test fold `\(\mathcal{F}_k\)`. Store these predictions `$$z_i = (\hat{f}_1^{-\mathcal{F}_k}(x_i),\ldots,\hat{f}_L^{-\mathcal{F}_k}(x_i)), \quad  i \in \mathcal{F}_k$$`

3. Fit the stacking model `\(\hat{f}_{\mathrm{stack}}\)` using 
`$$(y_1,z_1),\ldots, (y_n,z_n)$$`

4. For `\(l=1,\ldots,L\)`,  fit the `\(l\)`th model to the full training data and make predictions on the test data. Store these predictions
`$$z^*_i = (\hat{f}_1(x^*_i),\ldots,\hat{f}_L(x^*_i)), \quad i=1,\ldots,m$$`
5. Make final predictions `\(\hat{y}^{*}_i = \hat{f}_{\mathrm{stack}}(z^*_i)\)`, `\(i=1,\ldots,m\)`

---


# caretEnsemble

See [A Brief Introduction to caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
) by Zach Mayer


```r
# K fold CV for regression
KCV &lt;- trainControl(method="cv", 
                    number=K,
                    savePredictions="final",
                    index=createResample(train$y, K)
                    )
# list of models fit1 and fit2
List &lt;- caretList(y ~. ,
                  data=train,
                  trControl=KCV,
                  methodList=c("fit1","fit2")
                  )
# ensemble fit
fit.ensemble &lt;- caretEnsemble(List, metric="RMSE")
summary(fit.ensemble)
```



---

# Boston data


```r
# libraries
rm(list=ls())
library(MASS)
library(caret)
library(caretEnsemble)

# import data
set.seed(123)
split &lt;- createDataPartition(y=Boston$medv, p = 0.5, list=FALSE)
train &lt;- Boston[split,]
test = Boston[-split,-14]
test.y = Boston[-split,14]
nrow(test)

# cross-validation settings
K = 10
my_control &lt;- trainControl(
  method="cv",
  number=K,
  savePredictions="final",
  index=createResample(train$medv, K)
  )
```

---

# Library of models


```r
model_list &lt;- caretList(
  medv~., data=train,
  methodList=c("lm","ctree"), 
  tuneList=list(
    rf=caretModelSpec(method="rf", tuneLength=3)
  ),
  trControl=my_control
)

xyplot(resamples(model_list))
modelCor(resamples(model_list))
```

---

# Ensemble


```r
greedy_ensemble &lt;- caretEnsemble(
  model_list, 
  metric="RMSE"
  )
summary(greedy_ensemble)
```

---

# Test MSE


```r
yhats &lt;- lapply(model_list, predict, newdata=test)
lapply(yhats, function(yhat) mean((yhat - test.y)^2) )

yhat.en &lt;- predict(greedy_ensemble, newdata=test)
mean((yhat.en - test.y)^2)
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
