<!DOCTYPE html>
<html>
  <head>
    <title>Random Forests</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Random Forests
### Aldo Solari

---





# Outline

* Random forests
* Boston housing data
* Spam data 

---

# Random forests

* Create even more variation in individual trees

* Bagging varies the __rows__ of the training set (randomly draw observations)

* Random forests varies also the __columns__ of the training set (randomly draw predictors)

---

# Tuning parameter

* Before each split, select `\(m \leq p\)` of the predictors at random as candidates for splitting

* `\(m\)` is a __tuning parameter__

* Typically `\(m = \sqrt{p}\)` for classification and `\(m=p/3\)` for regression

* `\(m=p\)` gives Bagging as a special case


---

# Why is random forests working?

* Random sampling of the predictors __decorrelates__ the trees. This reduces the variance when we 
average the trees

* Recall that given a set of identical distributed (but not necessarily independent) variables `\(Z_1,\ldots,Z_B\)` with pairwise correlation `\(\mathbb{C}\mathrm{orr}(Z_j, Z_l) = \rho\)`, mean `\(\mathbb{E}(Z_j) = \mu\)` and variance `\(\mathbb{V}\mathrm{ar}(Z_j) = \sigma^2\)`, then (see the next slide)

$$
\displaystyle \mathbb{V}\mathrm{ar}(\bar{Z}) = \rho \sigma^2 + \frac{(1-\rho)}{B} \sigma^2
$$
 
* The idea in random forests is to improve the variance
reduction of bagging by reducing the correlation `\(\rho\)` between the
trees, without increasing the variance `\(\sigma^2\)` too much

---

*  `\(\displaystyle \rho = \frac{1}{\sigma^2}[ \mathbb{E}(Z_i Z_j) - \mathbb{E}(Z_i) \mathbb{E}(Z_j)]\)`

* `\(\mathbb{E}(Z_i Z_j) = \rho \sigma^2 + \mu^2\)` if `\(i\neq j\)` 

* `\(\mathbb{E}(Z_i^2) = \sigma^2 + \mu^2\)`

* `\(\displaystyle \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] = \sum_{i=1}^{B}\sum_{j=1}^B \mathbb{E}(Z_i Z_j)  =  B \mathbb{E}(Z_i^2) + (B^2-B)\mathbb{E}(Z_i Z_j)\)`

* `\(\displaystyle \mathbb{E}(\sum_{j=1}^{B}Z_j) = \sum_{j=1}^{B} \mathbb{E}(Z_j)=  B \mu\)`

`\begin{eqnarray*}
\mathbb{V}\mathrm{ar}(\bar{Z}) &amp;=&amp; \frac{1}{B^2} \mathbb{V}\mathrm{ar}(\sum_{j=1}^{B}Z_j) \\
&amp;=&amp;  \frac{1}{B^2} \{ \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] - [ \mathbb{E}(\sum_{j=1}^{B}Z_j) ]^2  \} 
\end{eqnarray*}`

---

ESL p. 589 Bagging and random forest applied to the
spam data

&lt;img src="images/randomforest.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;


---

* [ISL, Fig. 8.10](http://www-bcf.usc.edu/~gareth/ISL/Chapter8/8.10.pdf) 

* Results from random forests for the 15-class gene expression
data set with `\(p = 500\)` predictors. The test error is displayed as a function of
the number of trees. Each colored line corresponds to a different value of `\(m\)`, the
number of predictors available for splitting at each interior tree node. Random
forests `\((m &lt; p)\)` lead to a slight improvement over bagging `\((m = p)\)`. A single
classification tree has an error rate of 45.7%

---

# randomForest()


```r
randomForest(formula, 
  data = ,
  ntree = , # default 500
  mtry = , # default sqrt(p) or p/3
  nodesize = , # default 1 or 5
  importance = ) # default: FALSE
```

* In bagging and random forests trees are grown large without pruning, only the minimum number of observations per node - `nodesize` -  is fixed `\(=1\)` for classification and `\(=5\)` for regression

* Number of randomly selected predictors `mtry` `\(=\sqrt{p}\)` for classification and `\(=p/3\)` for regression. Note that `mtry` `\(=p\)` is bagging


---

# Variable importance

* How might we get such a measure of variable importance from a random forest?

* The function `randomForests()` provides two different ways (argument `importance=TRUE`):

1. The first measure (`type=1`) is computed from permuting OOB (out-of-bag) data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable (adding noise). The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences

2. The second measure (`type=2`) is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by RSS

* So, the first compares the prediction of a variable with a random version of itself, while the second considers the error rates induced by splitting on a variable

---

[Efron and Hastie (2016) Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) (CASI) p. 332

&lt;img src="images/varimp.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;

---

# Random forest takeaways

* Bagging stabilizes decision trees and improves accuracy by reducing variance. Random forests further improve decision tree performance by de-correlating the individual trees in the bagging ensemble

*  Random forests' variable importance measures can help you determine which
variables are contributing the most strongly to your model

* Because the trees in a random forest ensemble are unpruned and potentially
quite deep, there's still a danger of overfitting

* Random forests are one of the better off-the-shelf methods. Strengths:
    - Easily incorporates features of different types (categorical and numeric).
    - Tolerance to irrelevant features.
    - Some tolerance to correlated inputs.
    - Good with big data.
    - Handling of missing values


---

layout: false
class: inverse, middle, center

# Boston housing data

---

# Boston housing data

* The Boston Housing data is a standard benchmark data set for regression models. It contains data for 506 census tracts of Boston from the 1970 census


* The main objective of the Boston Housing data is to investigate variables associated with predicting the median value of homes (continuous `medv` response) within 506 suburban areas of Boston

* We will use the data contained in the `MASS` package

* We will follow the analysis of [Ehrlinger (2015) ggRandomForests: Random Forests for Regression](https://arxiv.org/pdf/1501.07196.pdf)

---


| Variable | Description | type |
|---|---|---|
| crim | Crime rate by town | numeric |
| zn |  Proportion of residential land zoned for lots over 25,000 sq.ft | numeric |
| indus | Proportion of non-retail business acres per town | numeric |
| chas | Charles River (tract bounds river) | logical |
| nox | Nitrogen oxides concentration (10 ppm) | numeric |
| rm | Number of rooms per dwelling | numeric |
| age | Proportion of units built prior to 1940 | numeric |
| dis | Distances to Boston employment center | numeric |
| rad | Accessibility to highways | integer |
| tax | Property-tax rate per $10,000 | numeric |
| ptratio | Pupil-teacher ratio by town | numeric |
| black | Proportion of blacks by town | numeric |
| lstat | Lower status of the population (percent) | numeric |
| medv | Median value of homes ($1000s) | numeric |

---


```r
rm(list=ls())
data(Boston, package = "MASS")
Boston$chas &lt;- as.logical(Boston$chas)
n = nrow(Boston)
```

---


```r
# load packages
library("ggplot2") # Graphics engine
library("dplyr") # Better data manipulations

# analysis packages
library("randomForestSRC") # random forests 
library("ggRandomForests") # ggplot2 random forest figures

#default settings 
theme_set(theme_bw()) # A ggplot2 theme with white background
```

---

![](9_RF_slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---



```r
library(tree)
set.seed(123)
obs = sample(1:n, size = n, replace=TRUE)
# OOB MSE 
mean( ( predict(
tree(medv ~ ., data=Boston[obs,], 
      control=tree.control(nobs=n,mindev = 0)
      ), newdata = Boston[-obs,]) - Boston$medv[-obs] )^2 )
```

```
[1] 18.31595
```

```r
# OOB MSE with permuted predictor
pBoston = Boston
pBoston$lstat = sample(pBoston$lstat)
mean( ( predict(
tree(medv ~ ., data=Boston[obs,], 
      control=tree.control(nobs=n, mindev = 0)
      ), newdata = pBoston[-obs,]) - Boston$medv[-obs] )^2 )
```

```
[1] 51.36433
```


---


```r
fit = rfsrc(medv~., data=Boston, 
            nsplit=0, 
            ntree = 1000, 
            importance=TRUE, 
            block.size=1)
fit
```

```
                         Sample size: 506
                     Number of trees: 1000
           Forest terminal node size: 5
       Average no. of terminal nodes: 104.287
No. of variables tried at each split: 5
              Total no. of variables: 13
       Resampling used to grow trees: swr
    Resample size used to grow trees: 506
                            Analysis: RF-R
                              Family: regr
                      Splitting rule: mse
                % variance explained: 87.02
                          Error rate: 10.98
```

---


```r
# Plot the OOB errors against the growth of the forest
gg_e &lt;- gg_error(fit)
plot(gg_e)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;


---


```r
# Plot the variable importance
plot(gg_vimp(fit))
```

![](9_RF_slides_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---


```r
# Variable dependence plot
gg_v &lt;- gg_variable(fit)
plot(gg_v, 
     xvar=c("lstat","rm","nox","crim","pratio","dis","indus"), 
     panel=TRUE, se=.95, span=1.2, alpha=.4)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;



---
layout: false
class: inverse, middle, center

# Spam data

---


```r
#Load the data and split into training and test sets.
rm(list=ls())
spam &lt;- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
testy = spam[spam$testid,1]
test = spam[spam$testid, -c(1,2)]
n = nrow(train)
m = nrow(test)
```

---


```r
library("caret")
cv10 &lt;- trainControl(
  method = "cv",
  number = 10,
  ## Estimate class probabilities
  classProbs = TRUE,
  ## Evaluate performance using 
  ## the following function
  summaryFunction = twoClassSummary
)
```

---


```r
tree &lt;- train(
  spam ~ ., train,
  method = "rpart2",
  tuneLength = 9,
  trControl=cv10, 
  ## Specify which metric to optimize
  metric = "ROC")
```

---


```r
tree
```

```
CART 

3065 samples
  57 predictor
   2 classes: 'email', 'spam' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2758, 2758, 2759, 2760, 2759, 2758, ... 
Resampling results across tuning parameters:

  maxdepth  ROC        Sens       Spec     
   1        0.7586295  0.9539953  0.5632638
   2        0.8338587  0.9420740  0.7250034
   4        0.9020859  0.9236604  0.8259992
   5        0.9136121  0.9458490  0.8161293
   6        0.9142879  0.9404348  0.8366482
   7        0.9147313  0.9388132  0.8464842
   8        0.9147313  0.9388132  0.8464842
  10        0.9147313  0.9388132  0.8464842
  11        0.9147313  0.9388132  0.8464842

ROC was used to select the optimal model using the largest value.
The final value used for the model was maxdepth = 7.
```


---


```r
mtryGrid = data.frame(mtry=c(2,29,57))
rf &lt;- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  preProcess = c('center', 'scale'),
  tuneGrid = mtryGrid,
  localImp = TRUE,
  trControl=cv10, 
  metric = "ROC")
```

---


```r
rf
```

```
Random Forest 

3065 samples
  57 predictor
   2 classes: 'email', 'spam' 

Pre-processing: centered (57), scaled (57) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2759, 2758, 2759, 2758, 2759, 2758, ... 
Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec     
   2    0.9800707  0.9669683  0.8965520
  29    0.9792465  0.9583108  0.9137786
  57    0.9773430  0.9593978  0.9047758

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.
```

---


```r
plot(varImp(rf))
```

![](9_RF_slides_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---


```r
models = list(
  tree=tree,
  rf=rf)
resamps = resamples(models)
bwplot(resamps)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---


```r
yhat.tree = predict(tree, newdata=test)
confusionMatrix(yhat.tree, testy, positive='email')
```

```
Confusion Matrix and Statistics

          Reference
Prediction email spam
     email   878   94
     spam     63  501
                                          
               Accuracy : 0.8978          
                 95% CI : (0.8815, 0.9125)
    No Information Rate : 0.6126          
    P-Value [Acc &gt; NIR] : &lt; 2e-16         
                                          
                  Kappa : 0.7826          
 Mcnemar's Test P-Value : 0.01665         
                                          
            Sensitivity : 0.9330          
            Specificity : 0.8420          
         Pos Pred Value : 0.9033          
         Neg Pred Value : 0.8883          
             Prevalence : 0.6126          
         Detection Rate : 0.5716          
   Detection Prevalence : 0.6328          
      Balanced Accuracy : 0.8875          
                                          
       'Positive' Class : email           
                                          
```

---


```r
yhat.rf = predict(rf, newdata=test)
confusionMatrix(yhat.rf, testy)
```

```
Confusion Matrix and Statistics

          Reference
Prediction email spam
     email   911   59
     spam     30  536
                                          
               Accuracy : 0.9421          
                 95% CI : (0.9292, 0.9532)
    No Information Rate : 0.6126          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.8768          
 Mcnemar's Test P-Value : 0.002998        
                                          
            Sensitivity : 0.9681          
            Specificity : 0.9008          
         Pos Pred Value : 0.9392          
         Neg Pred Value : 0.9470          
             Prevalence : 0.6126          
         Detection Rate : 0.5931          
   Detection Prevalence : 0.6315          
      Balanced Accuracy : 0.9345          
                                          
       'Positive' Class : email           
                                          
```

---


```r
phats.all &lt;- extractProb(models, testX = test, testY = testy)
phats &lt;- subset(phats.all, dataType == "Test")
plotClassProbs(phats)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
