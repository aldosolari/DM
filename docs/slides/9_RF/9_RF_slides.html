<!DOCTYPE html>
<html>
  <head>
    <title>Random Forests</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Random Forests
### Aldo Solari

---





# Outline

* Random Forests
* Spam data


# Random Forests intuition

* Create even more variation in individual trees

* Bagging varies the __rows__ of the training set (randomly draw observations)

* Random forests varies also the __columns__ of the training set (randomly draw predictors)

---

# Random Forests tuning parameter

* Before each split, select `\(m \leq p\)` of the predictors at random as candidates for splitting

* `\(m\)` is the \alert{tuning parameter}

* Typically `\(m = \sqrt{p}\)` for classification and `\(m=p/3\)` for regression

* `\(m=p\)` gives Bagging as a special case


---

# Why is Random Forests working?

* Random sampling of the predictors __decorrelates__ the trees. This reduces the variance when we 
average the trees

* Recall that given a set of identical distributed (but not necessarily independent) variables `\(Z_1,\ldots,Z_B\)` with pairwise correlation `\(\mathbb{C}\mathrm{orr}(Z_j, Z_l) = \rho\)`, mean `\(\mathbb{E}(Z_j) = \mu\)` and variance `\(\mathbb{V}\mathrm{ar}(Z_j) = \sigma^2\)`, then (see next slide)

$$
\displaystyle \mathbb{V}\mathrm{ar}(\bar{Z}) = \rho \sigma^2 + \frac{(1-\rho)}{B} \sigma^2
$$
 
* The idea in random forests is to improve the variance
reduction of bagging by reducing the correlation `\(\rho\)` between the
trees, without increasing the variance `\(\sigma^2\)` too much

---

*  `\(\displaystyle \rho = \frac{1}{\sigma^2}[ \mathbb{E}(Z_i Z_j) - \mathbb{E}(Z_i) \mathbb{E}(Z_j)]\)`

* `\(\mathbb{E}(Z_i Z_j) = \rho \sigma^2 + \mu^2\)` if `\(i\neq j\)` 

* `\(\mathbb{E}(Z_i^2) = \sigma^2 + \mu^2\)`

* `\(\displaystyle \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] = \sum_{i=1}^{B}\sum_{j=1}^B \mathbb{E}(Z_i Z_j)  =  B \mathbb{E}(Z_i^2) + (B^2-B)\mathbb{E}(Z_i Z_j)\)`

* `\(\displaystyle \mathbb{E}(\sum_{j=1}^{B}Z_j) = \sum_{j=1}^{B} \mathbb{E}(Z_j)=  B \mu\)`

`\begin{eqnarray*}
\mathbb{V}\mathrm{ar}(\bar{Z}) &amp;=&amp; \frac{1}{B^2} \mathbb{V}\mathrm{ar}(\sum_{j=1}^{B}Z_j) \\
&amp;=&amp;  \frac{1}{B^2} \{ \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] - [ \mathbb{E}(\sum_{j=1}^{B}Z_j) ]^2  \} 
\end{eqnarray*}`

---

ESL p. 589 Bagging and random forest applied to the
spam data

![](images/randomforest.jpg)

---

* [ISL, Fig. 8.10](http://www-bcf.usc.edu/~gareth/ISL/Chapter8/8.10.pdf) 

* Results from random forests for the 15-class gene expression
data set with `\(p = 500\)` predictors. The test error is displayed as a function of
the number of trees. Each colored line corresponds to a different value of `\(m\)`, the
number of predictors available for splitting at each interior tree node. Random
forests `\((m &lt; p)\)` lead to a slight improvement over bagging `\((m = p)\)`. A single
classification tree has an error rate of 45.7%

---

# Variable importance

* We can calculate the importance of a predictor `\(X_j\)`

* The function `importance` of the R package `randomForests` with argument `type=1` :

1. For each tree, record the accuracy on the OOB observations

2. Do the same is done but with `\(X_j\)` values randomly permuted in the OOB observations

3. Compute the decrease in each tree's accuracy 

* If the average decrease over all the trees is large, then the predictor
is considered important - its value makes a big difference in predicting the response

* If the average decrease is small, then the predictor doesn't make much difference to the response

---

[Efron and Hastie (2016) Computer Age Statistical Inference:
Algorithms, Evidence and Data Science](https://web.stanford.edu/~hastie/CASI/) (CASI) p. 332

&lt;img src="images/varimp.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;




---

# randomForest()


```r
fit &lt;- randomForest(y ~ ., 
           data = train,
           ntree = B,
           mtry = m, # default sqrt(p) or p/3
           nodesize = ns, # default 1 or 5
           importance=TRUE) # default: F
```

* In bagging and random forests trees are grown large without pruning, only the minimum number of observations per node - `nodesize` -  is fixed `\(=1\)` for classification and `\(=5\)` for regression

* Number of randomly selected predictors `mtry` `\(=\sqrt{p}\)` for classification and `\(=p/3\)` for regression. Note that `mtry` `\(=p\)` is bagging

---

# Bagging and random forest takeaways

* Bagging stabilizes decision trees and improves accuracy by reducing variance

*  Random forests further improve decision tree performance by de-correlating
the individual trees in the bagging ensemble

*  Random forests' variable importance measures can help you determine which
variables are contributing the most strongly to your model

* Because the trees in a random forest ensemble are unpruned and potentially
quite deep, there's still a danger of overfitting

---
layout: false
class: inverse, middle, center

# Spam data

---


```r
#Load the data and split into training and test sets.
rm(list=ls())
spam &lt;- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test.y = spam[spam$testid,1]
test = spam[spam$testid, -c(1,2)]
n = nrow(train)
m = nrow(test)
```

---


```r
library("caret")

cv &lt;- trainControl(
  method = "cv",
  number = 10
)
```

---


```r
tree &lt;- train(
  spam ~ ., train,
  method = "rpart2",
  tuneLength = 9,
  trControl=cv)
plot(tree)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

```r
rf &lt;- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  trControl=cv)
```

--


```r
rf
```

```
Random Forest 

3065 samples
  57 predictor
   2 classes: 'email', 'spam' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2759, 2759, 2758, 2758, 2758, 2759, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.9363755  0.8657343
  29    0.9422419  0.8788754
  57    0.9357198  0.8650163

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 29.
```

---


```r
plot(rf)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---


```r
plot(varImp(rf))
```

![](9_RF_slides_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---


```r
models = list(
    tree=tree,
    rf=rf
  )

resamps = resamples(models)
bwplot(resamps, metric = "Accuracy")
```

![](9_RF_slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---


```r
yhat.tree = predict(tree, newdata=test)
confusionMatrix(yhat.tree, test.y)
```

```
Confusion Matrix and Statistics

          Reference
Prediction email spam
     email   878   94
     spam     63  501
                                          
               Accuracy : 0.8978          
                 95% CI : (0.8815, 0.9125)
    No Information Rate : 0.6126          
    P-Value [Acc &gt; NIR] : &lt; 2e-16         
                                          
                  Kappa : 0.7826          
 Mcnemar's Test P-Value : 0.01665         
                                          
            Sensitivity : 0.9330          
            Specificity : 0.8420          
         Pos Pred Value : 0.9033          
         Neg Pred Value : 0.8883          
             Prevalence : 0.6126          
         Detection Rate : 0.5716          
   Detection Prevalence : 0.6328          
      Balanced Accuracy : 0.8875          
                                          
       'Positive' Class : email           
                                          
```

```r
yhat.rf = predict(rf, newdata=test)
confusionMatrix(yhat.rf, test.y)
```

```
Confusion Matrix and Statistics

          Reference
Prediction email spam
     email   907   47
     spam     34  548
                                          
               Accuracy : 0.9473          
                 95% CI : (0.9349, 0.9579)
    No Information Rate : 0.6126          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.8884          
 Mcnemar's Test P-Value : 0.1824          
                                          
            Sensitivity : 0.9639          
            Specificity : 0.9210          
         Pos Pred Value : 0.9507          
         Neg Pred Value : 0.9416          
             Prevalence : 0.6126          
         Detection Rate : 0.5905          
   Detection Prevalence : 0.6211          
      Balanced Accuracy : 0.9424          
                                          
       'Positive' Class : email           
                                          
```

---


```r
models = list(
  tree = tree,
  rf = rf)

phats.all &lt;- extractProb(models, testX = test, testY = test.y)
phats &lt;- subset(phats.all, dataType == "Test")

plotClassProbs(phats)
```

![](9_RF_slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
