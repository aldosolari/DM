---
title: "Random Forests"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Outline

* Random forests
* Boston housing data
* Spam data 

---

# Random forests

* Create even more variation in individual trees

* Bagging varies the __rows__ of the training set (randomly draw observations)

* Random forests varies also the __columns__ of the training set (randomly draw predictors)

---

# Tuning parameter

* Before each split, select $m \leq p$ of the predictors at random as candidates for splitting

* $m$ is a __tuning parameter__

* Typically $m = \sqrt{p}$ for classification and $m=p/3$ for regression

* $m=p$ gives Bagging as a special case


---

# Why is random forests working?

* Random sampling of the predictors __decorrelates__ the trees. This reduces the variance when we 
average the trees

* Recall that given a set of identical distributed (but not necessarily independent) variables $Z_1,\ldots,Z_B$ with pairwise correlation $\mathbb{C}\mathrm{orr}(Z_j, Z_l) = \rho$, mean $\mathbb{E}(Z_j) = \mu$ and variance $\mathbb{V}\mathrm{ar}(Z_j) = \sigma^2$, then (see the next slide)

$$
\displaystyle \mathbb{V}\mathrm{ar}(\bar{Z}) = \rho \sigma^2 + \frac{(1-\rho)}{B} \sigma^2
$$
 
* The idea in random forests is to improve the variance
reduction of bagging by reducing the correlation $\rho$ between the
trees, without increasing the variance $\sigma^2$ too much

---

*  $\displaystyle \rho = \frac{1}{\sigma^2}[ \mathbb{E}(Z_i Z_j) - \mathbb{E}(Z_i) \mathbb{E}(Z_j)]$

* $\mathbb{E}(Z_i Z_j) = \rho \sigma^2 + \mu^2$ if $i\neq j$ 

* $\mathbb{E}(Z_i^2) = \sigma^2 + \mu^2$

* $\displaystyle \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] = \sum_{i=1}^{B}\sum_{j=1}^B \mathbb{E}(Z_i Z_j)  =  B \mathbb{E}(Z_i^2) + (B^2-B)\mathbb{E}(Z_i Z_j)$

* $\displaystyle \mathbb{E}(\sum_{j=1}^{B}Z_j) = \sum_{j=1}^{B} \mathbb{E}(Z_j)=  B \mu$

\begin{eqnarray*}
\mathbb{V}\mathrm{ar}(\bar{Z}) &=& \frac{1}{B^2} \mathbb{V}\mathrm{ar}(\sum_{j=1}^{B}Z_j) \\
&=&  \frac{1}{B^2} \{ \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] - [ \mathbb{E}(\sum_{j=1}^{B}Z_j) ]^2  \} 
\end{eqnarray*}

---

ESL p. 589 Bagging and random forest applied to the
spam data

```{r, echo=FALSE, fig.align = 'center', out.width = '70%', out.height = '70%'}
knitr::include_graphics("images/randomforest.jpg")
```


---

* [ISL, Fig. 8.10](http://www-bcf.usc.edu/~gareth/ISL/Chapter8/8.10.pdf) 

* Results from random forests for the 15-class gene expression
data set with $p = 500$ predictors. The test error is displayed as a function of
the number of trees. Each colored line corresponds to a different value of $m$, the
number of predictors available for splitting at each interior tree node. Random
forests $(m < p)$ lead to a slight improvement over bagging $(m = p)$. A single
classification tree has an error rate of 45.7%

---

# randomForest()

```{r, eval=FALSE}
randomForest(formula, 
  data = ,
  ntree = , # default 500
  mtry = , # default sqrt(p) or p/3
  nodesize = , # default 1 or 5
  importance = ) # default: FALSE
```

* In bagging and random forests trees are grown large without pruning, only the minimum number of observations per node - `nodesize` -  is fixed $=1$ for classification and $=5$ for regression

* Number of randomly selected predictors `mtry` $=\sqrt{p}$ for classification and $=p/3$ for regression. Note that `mtry` $=p$ is bagging


---

# Variable importance

* How might we get such a measure of variable importance from a random forest?

* The function `randomForests()` provides two different ways (argument `importance=TRUE`):

1. The first measure (`type=1`) is computed from permuting OOB (out-of-bag) data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable (adding noise). The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences

2. The second measure (`type=2`) is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by RSS

* So, the first compares the prediction of a variable with a random version of itself, while the second considers the error rates induced by splitting on a variable

---

[Efron and Hastie (2016) Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) (CASI) p. 332

```{r, echo=FALSE, fig.align = 'center', out.width = '70%', out.height = '70%'}
knitr::include_graphics("images/varimp.jpg")
```

---

# Random forest takeaways

* Bagging stabilizes decision trees and improves accuracy by reducing variance. Random forests further improve decision tree performance by de-correlating the individual trees in the bagging ensemble

*  Random forests' variable importance measures can help you determine which
variables are contributing the most strongly to your model

* Because the trees in a random forest ensemble are unpruned and potentially
quite deep, there's still a danger of overfitting

* Random forests are one of the better off-the-shelf methods. Strengths:
    - Easily incorporates features of different types (categorical and numeric).
    - Tolerance to irrelevant features.
    - Some tolerance to correlated inputs.
    - Good with big data.
    - Handling of missing values


---

layout: false
class: inverse, middle, center

# Boston housing data

---

# Boston housing data

* The Boston Housing data is a standard benchmark data set for regression models. It contains data for 506 census tracts of Boston from the 1970 census


* The main objective of the Boston Housing data is to investigate variables associated with predicting the median value of homes (continuous `medv` response) within 506 suburban areas of Boston

* We will use the data contained in the `MASS` package

* We will follow the analysis of [Ehrlinger (2015) ggRandomForests: Random Forests for Regression](https://arxiv.org/pdf/1501.07196.pdf)

---


| Variable | Description | type |
|---|---|---|
| crim | Crime rate by town | numeric |
| zn |  Proportion of residential land zoned for lots over 25,000 sq.ft | numeric |
| indus | Proportion of non-retail business acres per town | numeric |
| chas | Charles River (tract bounds river) | logical |
| nox | Nitrogen oxides concentration (10 ppm) | numeric |
| rm | Number of rooms per dwelling | numeric |
| age | Proportion of units built prior to 1940 | numeric |
| dis | Distances to Boston employment center | numeric |
| rad | Accessibility to highways | integer |
| tax | Property-tax rate per $10,000 | numeric |
| ptratio | Pupil-teacher ratio by town | numeric |
| black | Proportion of blacks by town | numeric |
| lstat | Lower status of the population (percent) | numeric |
| medv | Median value of homes ($1000s) | numeric |

---

```{r}
rm(list=ls())
data(Boston, package = "MASS")
Boston$chas <- as.logical(Boston$chas)
n = nrow(Boston)
```

---

```{r}
# load packages
library("ggplot2") # Graphics engine
library("dplyr") # Better data manipulations

# analysis packages
library("randomForestSRC") # random forests 
library("ggRandomForests") # ggplot2 random forest figures

#default settings 
theme_set(theme_bw()) # A ggplot2 theme with white background
```

---

```{r, echo=FALSE}
# EDA variable plots
library(reshape2)
dta <- melt(Boston, id.vars=c("medv","chas"))
# plot panels for each covariate colored by the logical chas variable.
ggplot(dta, aes(x=medv, y=value, color=chas))+
  geom_point(alpha=.4)+
  geom_rug(data=dta %>% filter(is.na(value)))+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)
```

---


```{r}
library(tree)
set.seed(123)
obs = sample(1:n, size = n, replace=TRUE)
# OOB MSE 
mean( ( predict(
tree(medv ~ ., data=Boston[obs,], 
      control=tree.control(nobs=n,mindev = 0)
      ), newdata = Boston[-obs,]) - Boston$medv[-obs] )^2 )
# OOB MSE with permuted predictor
pBoston = Boston
pBoston$lstat = sample(pBoston$lstat)
mean( ( predict(
tree(medv ~ ., data=Boston[obs,], 
      control=tree.control(nobs=n, mindev = 0)
      ), newdata = pBoston[-obs,]) - Boston$medv[-obs] )^2 )
```


---

```{r}
fit = rfsrc(medv~., data=Boston, 
            nsplit=0, 
            ntree = 1000, 
            importance=TRUE, 
            block.size=1)
fit
```

---

```{r}
# Plot the OOB errors against the growth of the forest
gg_e <- gg_error(fit)
plot(gg_e)
```


---

```{r}
# Plot the variable importance
plot(gg_vimp(fit))
```

---

```{r}
# Variable dependence plot
gg_v <- gg_variable(fit)
plot(gg_v, 
     xvar=c("lstat","rm","nox","crim","pratio","dis","indus"), 
     panel=TRUE, se=.95, span=1.2, alpha=.4)
```



---
layout: false
class: inverse, middle, center

# Spam data

---

```{r}
#Load the data and split into training and test sets.
rm(list=ls())
spam <- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
testy = spam[spam$testid,1]
test = spam[spam$testid, -c(1,2)]
n = nrow(train)
m = nrow(test)
```

---

```{r}
library("caret")
cv10 <- trainControl(
  method = "cv",
  number = 10,
  ## Estimate class probabilities
  classProbs = TRUE,
  ## Evaluate performance using 
  ## the following function
  summaryFunction = twoClassSummary
)
```

---

```{r, message=FALSE, warning=FALSE}
tree <- train(
  spam ~ ., train,
  method = "rpart2",
  tuneLength = 9,
  trControl=cv10, 
  ## Specify which metric to optimize
  metric = "ROC")
```

---

```{r}
tree
```


---

```{r, message=FALSE, warning=FALSE}
mtryGrid = data.frame(mtry=c(2,29,57))
rf <- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  preProcess = c('center', 'scale'),
  tuneGrid = mtryGrid,
  localImp = TRUE,
  trControl=cv10, 
  metric = "ROC")
```

---

```{r}
rf
```

---

```{r}
plot(varImp(rf))
```

---

```{r, message=FALSE, warning=FALSE}
models = list(
  tree=tree,
  rf=rf)
resamps = resamples(models)
bwplot(resamps)
```

---

```{r, message=FALSE, warning=FALSE}
yhat.tree = predict(tree, newdata=test)
confusionMatrix(yhat.tree, testy, positive='email')
```

---

```{r, message=FALSE, warning=FALSE}
yhat.rf = predict(rf, newdata=test)
confusionMatrix(yhat.rf, testy)
```

---

```{r}
phats.all <- extractProb(models, testX = test, testY = testy)
phats <- subset(phats.all, dataType == "Test")
plotClassProbs(phats)
```



