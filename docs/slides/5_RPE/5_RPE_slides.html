<!DOCTYPE html>
<html>
  <head>
    <title>Random-X prediction error</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Random-X prediction error
### Aldo Solari

---




# Outline

* Random-X prediction error
* Random-X Mallows Cp
* Gaussian example

---
layout: false
class: inverse, middle, center

# Random-X prediction error

---

# Random-X setting

Earlier we have seen the Fixed-X setting, where predictor values are assumed to be nonrandom. In most modern predictive modeling applications, it is more reasonable to take a Random-X view, where the predictor values (both those used in training and for future predictions) are random.

We assume 

* Response `\(Y\)`
* Predictors `\(X=(X_1,\ldots,X_p)^\mathsf{T}\)`
* `\((X,Y)\)` have some unknown joint distribution
* Given `\(X=x\)`, we have
`$$(Y|X=x) = f(x) + \varepsilon$$`
where the conditional expectation `\(f(x)=\mathbb{E}(Y|X=x)\)` is called the __regression function__ and `\(\varepsilon\)` is the error term independent from `\(X\)` with `\(\mathbb{E}(\varepsilon)=0\)` and `\(\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2\)`. Note that the conditional variance 
`\(\mathbb{V}\mathrm{ar}(Y|X=x) = \sigma^2\)` is constant (__homoscedasticity__ assumption). 
* Training set: `\(n\)` observations `\((x_1,y_1),\ldots,(x_n,y_n)\)` i.i.d. from `\((X,Y)\)`
* Test set: `\(m\)` observations `\((x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)\)` i.i.d. from `\((X,Y)\)`

---

# Random-X prediction error

The __Random-X prediction error__ is given by
`$$\mathrm{ErrR}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}(Y^*_i - \hat{f}(X^*_i))^2\right] = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2]$$`
where the last equality follows by exchangeability and the expectation of the last term is with respect to the training set `\((X_1,Y_1),\ldots,(X_n,Y_n)\)` and to the new test point `\((X^*_1,Y^*_1)\)`

---

# Bias-variance decomposition

In the Random-X setting, the bias-variance decomposition is given by
`$$\mathrm{ErrR}= \sigma^2 + \mathbb{E}\left\{ \left[\mathbb{E}(\hat{f}(x^*_1)|x_1,\ldots,x_n,x^*_1) - f(x_1^*)\right]^2 \right\} + \mathbb{E}\left\{\mathbb{V}\mathrm{ar}(\hat{f}(x^*_1)|x_1,\ldots,x_n,x^*_1)\right\} = \mathrm{Irreducible} + \mathrm{Bias}^2 + \mathrm{Variance}$$`
---
layout: false
class: inverse, middle, center

# Random-X Mallows Cp

---

# Random-X Optimism

In the Random-X setting, the optimism is given by
`$$\mathrm{OptR}= \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}})- \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2 - (Y_1 - \hat{f}(X_1))^2]$$`
where the last expectation is with respect to to the training set `\((X_1,Y_1),\ldots,(X_n,Y_n)\)` and to the new test point `\((X^*_1,Y^*_1)\)`.


---

# Random-X Optimism for linear regression

* Assume that `\(X = (X_1,\ldots,X_p)^\mathsf{T} \sim N(0,\Sigma)\)` where `\(\Sigma\)` is invertible and `\(p&lt;n-1\)`. 

* Assume also that `\(f(x)= x^\mathsf{T}\beta\)`, a linear function of `\(x\)`. 

* Then, for the least squares regression estimator,
`$$\mathrm{OptR}= \mathrm{OptF} + \frac{\sigma^2p}{n}\left( \frac{p+1}{n-p-1}\right)=  \frac{2\sigma^2p}{n} + \frac{\sigma^2p}{n}\left( \frac{p+1}{n-p-1}\right) = \frac{\sigma^2p}{n}\left(2 + \frac{p+1}{n-p-1}\right)$$`

* Note that the assumptions above hold if we assume that `\((X,Y)\)` is jointly Gaussian, which implies the linear model to be unbiased.

---

# Random-X version of Mallows' Cp 

* Under the previous assumptions, we can estimate ErrR by
`$$\widehat{\mathrm{ErrR}} = \mathrm{MSE}_{\mathrm{Tr}} + \frac{\sigma^2p}{n}\left(2 + \frac{p+1}{n-p-1}\right)$$`

* If we use  `\(\hat{\sigma}^2 = \mathrm{RSS}/(n-p)\)` in place of `\(\sigma^2\)`, we obtain a Random-X version of Mallows' Cp 
`$$\mathrm{RCp} = \mathrm{Cp} + \frac{\hat{\sigma}^2p}{n}\left( \frac{p+1}{n-p-1}\right) = \frac{\mathrm{RSS}(n-1)}{(n-p)(n-p-1)}$$`

---
layout: false
class: inverse, middle, center

# Gaussian example

---

# Gaussian example

For example, suppose that 
`$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
\mu_y \\
\mu_x \\
\end{array}\right), \left(\begin{array}{cc} 
\sigma^2_y &amp; \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  &amp; \sigma^2_x \\
\end{array}\right)\right)$$`
so that the conditional distribution of `\(Y\)` given `\(X=x\)` is
`$$(Y|X=x) \sim N\Big(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x-\mu_x), \sigma^2_y (1-\rho^2)\Big)$$`
where `\(f(x)= \mathbb{E}(Y|X=x) = \left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right) + \left(\rho\frac{\sigma_y}{\sigma_x}\right)x = \alpha + \beta x\)`.

This corresponds to generate the training data as follows. For `\(i=1,\ldots,n\)`:

1. `\(x_i\)` is the realization of `\(X_i \sim N(\mu_x,\sigma^2_x)\)`, the marginal distribution of `\(X\)`
2. `\(y_i\)` is the realization of `\((Y_i|X_i=x_i) = f(x_i) + \varepsilon_i\)`, the conditional distribution of `\(Y\)` given `\(X=x\)`, where 
    - `\(f(x_i)=\alpha + \beta x_i\)` with `\(\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)\)` and `\(\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)\)`
    - `\(\varepsilon_i \sim N(0,\sigma^2)\)` with `\(\sigma^2=\sigma^2_y (1-\rho^2)\)`

and analogously for the test data.

---

* Suppose that `\(\mu_x=\mu_y=0\)`, `\(\rho=0.5\)`, `\(\sigma_x=1\)`, `\(\sigma_y=2\)`, i.e. 
`$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
0 \\
0 \\
\end{array}\right), \left(\begin{array}{cc} 
2^2 &amp; 1 \\
1  &amp; 1^2 \\
\end{array}\right)\right)$$`
thus `\(X\sim N(0,1)\)`, `\(Y|X=x \sim N(\beta x,\sigma^2)\)` with `\(\sigma^2=3\)` and `\(\beta=1\)`.


* Consider the estimator  
`$$\hat{y^*_1} = \hat{f}(x^*_1) =  \hat{\beta} x_1^*$$`
where the estimate of `\(\beta\)` is given by
`$$\displaystyle \hat{\beta} = \frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2 }$$`
where `\((x_1,y_1),\ldots,(x_n,y_n)\)` is the traing set with `\(n&gt;2\)`

* Note that `\(\mathbb{E}(\hat{\beta})=\beta\)`, thus the estimator `\(\mathbb{E}(\hat{f}(x))= f(x)\)` is unbiased.

---

Show that

1. ErrR conditional to the training set is given by
`$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | (X_1,Y_1) = (x_1,y_1), \ldots, (X_n,Y_n) = (x_n,y_n)] = \sigma^2 + (\beta-\hat{\beta})^2$$`

2. ErrR conditional to `\(X_1^*,X_1,\ldots,X_n\)` is given by
`$$\mathrm{E}[(Y^*_1 - \hat{f}(x^*_1))^2 | X^*_1 = x^*_1, X_1 = x_1, \ldots, X_n = x_n] = \sigma^2 + \sigma^2\left(\frac{(x_1^*)^2}{\sum_{i=1}^{n}x^2_i}\right)$$`

3. ErrR is given by
`$$\mathrm{ErrR} = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2] =  \sigma^2 + \frac{\sigma^2}{n-2}$$`

---


```r
n = 20
beta = 1
sigma = sqrt(3)
p = 1
set.seed(123)
# observed training set
x = rnorm(n)
y = beta*x + rnorm(n,0,sigma)
# estimate
hatbeta &lt;- sum(x*y)/sum(x^2)
# observed test point
x1star = rnorm(1)
y1star = beta*x1star + rnorm(1,0,sigma)
# error
( y1star - hatbeta*x1star )^2
```

```
[1] 0.2132094
```

```r
# RCp
RSS = sum( (y - x*hatbeta)^2 )
(RSS*(n-1))/((n-p)*(n-p-1))
```

```
[1] 2.168106
```

---


```r
# 1.
B = 5000
set.seed(123)
x1stars = rnorm(B,0,1)
y1stars = beta*x1stars + rnorm(B,0,sigma)
# empirical
mean( ( y1stars - hatbeta*x1stars )^2 )
```

```
[1] 3.034306
```

```r
# theoretical
sigma^2 + (beta - hatbeta)^2
```

```
[1] 3.021398
```

---


```r
# 2.
set.seed(123)
sim2 = function(n){
  xs = c(x,x1star)
  ys = xs*beta + rnorm(n+1,0,sigma)
  hatbeta &lt;- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim2(n)))
```

```
[1] 3.077386
```

```r
# theoretical
sigma^2 + (sigma^2)*(x1star^2/sum(x^2))
```

```
[1] 3.078788
```

---


```r
# 3.
set.seed(123)
sim3 = function(n){
  xs = rnorm(n+1,0,1)
  ys = beta*xs + rnorm(n+1,0,sigma)
  hatbeta &lt;- sum(xs[1:n]*ys[1:n])/sum(xs[1:n]^2)
  (ys[n+1] - hatbeta*xs[n+1])^2
}
# empirical
mean(replicate(B,sim3(n)))
```

```
[1] 3.194863
```

```r
# theoretical
sigma^2 + (sigma^2)/(n-2)
```

```
[1] 3.166667
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
