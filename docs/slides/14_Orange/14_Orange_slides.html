<!DOCTYPE html>
<html>
  <head>
    <title>Orange data</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Orange data
### Aldo Solari

---





# Outline

* Orange data
* Missing data
* Zero- and near zero-variance predictors
* Encoding missingness
* Scoring categorical predictors

---

# KDD cup

*  The Conference on Knowledge Discovery and Data Mining
(KDD) is the premier conference on machine learning methods

* Every year KDD hosts a data mining cup, where teams analyze a dataset

* The KDD Cup is a huge deal and the inspiration for the famous **Netflix Prize** and even **Kaggle** competitions

* The KDD Cup 2009 provided the **Orange data**, a dataset about customers of the French Telecom company Orange

---

# Orange data

* The goal is to predict the propensity of customers to cancel their account, an event called **churn**

* Other goals were to predict the customers tendency to use
new products and services (called **appetency**), and willingness to respond favorably to marketing pitches (called **upselling**)

* The contest supplied `\(p=230\)` predictors about `\(n=50000\)` credit card accounts

* For privacy reasons, predictors are anonymized: you don’t know the meaning of any of the predictors

* Churn problem: class 7.3% positive class (3672/50000)

* This dataset is an opportunity to deal with a very large database, including 
    - heterogeneous noisy data (numerical and categorical predictors with missing values)
    - Class imbalance

---

# Existing analyses

*  Guyon, Lemaire, Boullé, Dror, Vogel (2009)[ Analysis of the KDD Cup 2009: Fast Scoring on a Large Orange Customer Database](http://proceedings.mlr.press/v7/guyon09/guyon09.pdf)

* Chapter 6 of Zumel and Mount (2014) [Practical Data Science with R](https://www.manning.com/books/practical-data-science-with-r) ,  Manning Publications 

* The support site of Zumel and Mount (code and data) on [GitHub](https://github.com/WinVector/zmPDSwR) 


---

# Data

* Training set with `\(n = 22253\)` observations 

* Test set with `\(m = 27747\)` observations

* Response variable : `churn` = -1 (no churn), +1 (churn)

* Class imbalance: 7% positive class in the train set (1633/22253)

* `\(p=230\)` predictors: `Var1`, `Var2`, .. , `Var230`

* We don't know the meaning of any variable 

* The contest metric is the area under the curve (AUC)

* The winning team achieved an AUC of 0.76

---


```r
# import data
load("/Users/aldosolari/Desktop/Orange.Rdata")
#train &lt;- read.csv("train.csv")
#test &lt;- read.csv("test.csv")
#test$churn = NA
n = nrow(train)
m = nrow(test)
combi = rbind(train,test)
train = combi[1:n,]
test = combi[(n+1):(n+m),]
```


---

layout: false
class: inverse, middle, center

# Missing data

---

# Handling missing data

* Missing data are not rare in real data sets

* The first and most important question when encountering missing data is *why are these values missing?*

* Missing values are generally caused by three mechanisms:
    - A structural deficiency in the data
    - A random occurrence, or
    - A specific cause
    
    
* When the number of observations or predictors exceeds the bounds of effective visualization, then numeric summaries will be better equipped to diagnose the nature and degree of missingness

---


```r
pMiss &lt;- function(x){sum(is.na(x))/length(x)*100}
# predictors by % of missingness
pMiss2 = apply(train,2,pMiss)
hist(pMiss2, xlim=c(0,100))
```

![](14_Orange_slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;


---


```r
# observations by % of missingness
pMiss1 = apply(train,1,pMiss)
hist(pMiss1, xlim=c(0,100))
```

![](14_Orange_slides_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---


```r
# Zero-variance predictors due to complete missingness
vars_miss = which(pMiss2==100)
names(vars_miss)
```

```
 [1] "Var8"   "Var15"  "Var20"  "Var31"  "Var32"  "Var39"  "Var42"  "Var48"  "Var52"  "Var55"  "Var79"  "Var141" "Var167" "Var169" "Var175" "Var185" "Var209" "Var230"
```

---

layout: false
class: inverse, middle, center

# Zero- and Near Zero-Variance Predictors

---

## Zero- and near zero-variance predictors

* In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. **zero-variance predictor**)

* For many models (excluding e.g. tree-based models), this may cause the model to crash or the fit to be unstable

* To identify these types of predictors, the following two metrics can be calculated: 

    - the **frequency ratio**: the frequency of the most prevalent value over the second most frequent value, which would be near one for well-behaved predictors and very large for highly-unbalanced data 

    - the **percent of unique values** is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases

* If the frequency ratio is greater than a pre-specified threshold `freqCut` = 95/5 and the unique value percentage is less than a threshold `uniqueCut` = 10, we might consider a predictor to be __near zero-variance__

---


```r
library(caret)
vars_zv = nearZeroVar(train, freqCut = 95/5, uniqueCut = 10)
setdiff(vars_zv, vars_miss)
```

```
 [1]   2   3   4   5  10  14  19  26  27  29  33  34  36  37  44  49  50  51  53  56  58  59  67  69  70  78  80  84  86  90  93  95  98 106 111 114 116 117 118 122 124 131 138 139 140 143 150 162 165 173 176 177 182 183
[55] 191 194 195 196 201 210 213 215 219 224
```

---

## Type of predictors


```r
#Identify which predictors are categorical and numeric.
vars &lt;- setdiff(names(train),c('churn', 
                               names(train)[vars_zv]
                               ))
table(sapply(train[,vars],class))
```

```

 factor integer numeric 
     28     106      14 
```

---


```r
# categorical predictors
vars_cat &lt;- vars[sapply(train[,vars],class) %in% c('factor','logical')]
vars_cat
```

```
 [1] "Var192" "Var193" "Var197" "Var198" "Var199" "Var200" "Var202" "Var203" "Var204" "Var205" "Var206" "Var207" "Var208" "Var211" "Var212" "Var214" "Var216" "Var217" "Var218" "Var220" "Var221" "Var222" "Var223" "Var225"
[25] "Var226" "Var227" "Var228" "Var229"
```

---


```r
# numerical predictors
vars_num&lt;- vars[sapply(train[,vars],class) %in% c('numeric','integer')]
vars_num
```

```
  [1] "Var1"   "Var6"   "Var7"   "Var9"   "Var11"  "Var12"  "Var13"  "Var16"  "Var17"  "Var18"  "Var21"  "Var22"  "Var23"  "Var24"  "Var25"  "Var28"  "Var30"  "Var35"  "Var38"  "Var40"  "Var41"  "Var43"  "Var45" 
 [24] "Var46"  "Var47"  "Var54"  "Var57"  "Var60"  "Var61"  "Var62"  "Var63"  "Var64"  "Var65"  "Var66"  "Var68"  "Var71"  "Var72"  "Var73"  "Var74"  "Var75"  "Var76"  "Var77"  "Var81"  "Var82"  "Var83"  "Var85" 
 [47] "Var87"  "Var88"  "Var89"  "Var91"  "Var92"  "Var94"  "Var96"  "Var97"  "Var99"  "Var100" "Var101" "Var102" "Var103" "Var104" "Var105" "Var107" "Var108" "Var109" "Var110" "Var112" "Var113" "Var115" "Var119"
 [70] "Var120" "Var121" "Var123" "Var125" "Var126" "Var127" "Var128" "Var129" "Var130" "Var132" "Var133" "Var134" "Var135" "Var136" "Var137" "Var142" "Var144" "Var145" "Var146" "Var147" "Var148" "Var149" "Var151"
 [93] "Var152" "Var153" "Var154" "Var155" "Var156" "Var157" "Var158" "Var159" "Var160" "Var161" "Var163" "Var164" "Var166" "Var168" "Var170" "Var171" "Var172" "Var174" "Var178" "Var179" "Var180" "Var181" "Var184"
[116] "Var186" "Var187" "Var188" "Var189" "Var190"
```


---

## Calibration set

* Split the data into three sets: training, calibration, and test.

* The calibration set is used to simulate the unseen test set during modeling 

* We'll look at performance on the calibration set to estimate if we’re overfitting 

---


```r
set.seed(123)
train.all = train
is.calib &lt;- rbinom(n=nrow(train.all),size=1,prob=0.25)&gt;0
# further split training data into training and calibration.
train = train.all[!is.calib,]
calib = train.all[is.calib,]
```

---

## Single categorical predictor

* `Var218` is a categorical predictor

* Let's see how `churn` varies with the levels of `Var218`


```r
# Tabulate levels of Var218.
table218 &lt;- table(
   Var218=train[,'Var218'],  
   churn=train[,'churn'], 
# Include NA values in tabulation
   useNA='ifany') 	
table218
```

```
      churn
Var218   -1    1
  cJvF 7999  520
  UYBR 7330  641
  &lt;NA&gt;  182   61
```

```r
# Churn rates grouped by Var218 levels
table218[,2]/(table218[,1]+table218[,2])
```

```
      cJvF       UYBR       &lt;NA&gt; 
0.06104003 0.08041651 0.25102881 
```

---

# Var218

* Levels frequencies: cJvF = 11314 (51%), UYBR = 10612 (48%), &lt;NA&gt; = 327 (1%)

* When variable 218 takes on 
    - cJvF, then 6% of the customers churn
    - UYBR, then 8% of the customers churn
    - not available (NA), then 27% of the customers churn
    
* Variable 218 seems like a predictor that's easy to use and helpful with prediction


---

layout: false
class: inverse, middle, center

# Scoring categorical predictors

---

## Scoring categorical predictors

* We also need to design a strategy for what to do if a new level not seen during training were to occur during model use

* We'll build a function that converts NA to a level (as it seems to be pretty informative) and also treats novel values as uninformative

---

# Null model


```r
# positive class %
mean(train$churn=="1")
```

```
[1] 0.07302934
```

```r
# null model
phat.null = rep(mean(train$churn=="1"),m)
library(pROC)
roc.null &lt;- roc(
    response = as.factor(test$churn),
    predictor = phat.null,
    levels = c("1","-1")
    )
auc(roc.null)
```

```
Area under the curve: 0.5
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
