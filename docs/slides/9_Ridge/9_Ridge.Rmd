---
title: "Data Mining"
subtitle: "Ridge regression"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Outline

* Alternatives to least squares
* Ridge regression
* Boston housing data

---

# Matrix notation

* Vector of response: 
$$\underset{n\times 1}{\mathbf{y}} = 
\left[
\begin{array}{c}
y_1   \\
\cdots\\
y_i  \\
\cdots\\
y_n \\
\end{array}\right]$$

* Design matrix:
$$\underset{n\times p}{\mathbf{X}} = [\mathbf{x}_1 \cdots \mathbf{x}_p] =  \left[
\begin{array}{cccccc}
x_{1}^\mathsf{T}   \\
x_{2}^\mathsf{T}  \\
\cdots   \\
x_{i}^\mathsf{T}    \\
\cdots\\
x_{n}^\mathsf{T}\\
\end{array}\right] = \left[
\begin{array}{cccccc}
x_{11}  & x_{12}  & \cdots   &  x_{1j}  & \cdots   &   x_{1p}  \\
x_{21}  & x_{22} & \cdots   &  x_{2j}  & \cdots   &   x_{2p}  \\
\cdots   & \cdots   &  \cdots & \cdots   &  \cdots  \\
x_{i1}  & x_{i2} & \cdots   &  x_{ij}& \cdots   & x_{ip}    \\
\cdots   & \cdots   &  \cdots  &  \cdots   &  \cdots\\
x_{n1}   & x_{n2} & \cdots   & x_{nj}    &  \cdots   &   x_{np}\\
\end{array}\right]$$

---

# Linear regression

* Assume $\mathrm{rank}(\mathbf{X}) = p$

* Least squares problem: 
$$\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2$$
where  $\| \underset{p \times 1}{ \mathbf{v}} \|_{2} = (\mathbf{v}^\mathsf{T}\mathbf{v})^{1/2} = \sqrt{\sum_{j=1}^{p} v_j^2}$ denotes the $L_2$ norm

* OLS estimator: $$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$

* Fitted values: $\underset{n\times 1}{\hat{\mathbf{y}} } = \mathbf{X}\hat{\boldsymbol{\beta}}$

* Test data: $\underset{m\times 1}{\mathbf{y}^*}$, $\underset{m\times p}{\mathbf{X}^*}$, prediction on test data: $\underset{m\times 1}{\hat{\mathbf{y}}^*} = \mathbf{X}^*\hat{\boldsymbol{\beta}}$


---

layout: false
class: inverse, middle, center

# Alternatives to least squares

---

# Why consider alternatives to least squares?

* In the Fixed-X setting, the variance of the OLS estimator is
$$\frac{\sigma^2 p}{n}$$
    - If $n \gg p$, then the least squares estimates tend to have low variance 

    - If $n>p$ but $n \approx p$, then there can be a lot of variability in the least squares fit, resulting in overfitting and poor predictions

    - If $n=p$, then the least squares estimates fit the data exactly, i.e. $\hat{y}_i = y_i$, resulting in complete overfitting and horrible predictions

    - If $n<p$, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all

* We need to keep the variance in check

---

# Three classes of methods

1. **Subset Selection** <br> We identify a subset of the $p$ predictors
that we believe to be related to the response. We then fit a
model using least squares on the reduced set of variables.

2. **Dimension Reduction** <br> We project the $p$ predictors into a $q$-dimensional subspace, where $q < p$. Then these $q$ projections are used as predictors to fit a linear regression model by least squares

3. **Regularization** <br> We fit a model involving all $p$ predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates.  By *constraining* or *shrinking* the estimated coefficients, we can substantially reduce the variance at the cost of a neglige increase in bias


---

# Constrained estimation

```{r, echo=FALSE, fig.align = 'center', out.width = '50%', out.height = '50%'}
knitr::include_graphics("images/l2.jpg")
```

From ISLR, Figure 6.7

---

layout: false
class: inverse, middle, center

# Ridge regression

---

# Constrained estimation

* Solve the constrained minimization problem

\begin{aligned}
& \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2\\ & \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t
\end{aligned}

* The constrained problem can be solved by means of the Karuch-Kuhn-Tucker (KTT) multiplier method, which minimizes a function subject to inequality constraint

---

# Penalized estimation

* Penalized least squares
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2 + \lambda\sum_{j=1}^{p}\beta_j^2 = \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \underbrace{ \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 }_{\mathrm{RSS} }+ \underbrace{\lambda\| \boldsymbol{\beta}\|^2_2}_{\mathrm{penalty}}$$

* Here $\lambda \in [0,\infty)$ is the __tuning parameter__ which controls the strenght of the penalty term

* The minimum of the RSS is attained at $\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}$ while the minimum of the ridge penalty is attained at $\boldsymbol{\beta}=\mathbf{0}_p$. The effect of the penalty in this balancing act is to __shrink__ the
coefficient estimates towards zero

---

# Ridge estimator


* The solution of the minimization problem is the __ridge estimator__ :
$$\hat{\boldsymbol{\beta}}(\lambda) = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$
where $\mathbf{I}_p$ is the $p\times p$ identity matrix

* For any design matrix $\mathbf{X}$, the quantity
$(\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}$ is always invertible provided that $\lambda > 0$; thus,
there is always a unique solution $\hat{\boldsymbol{\beta}}(\lambda)$

---

# Solution path

* As $\lambda \rightarrow 0$, $\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \hat{\boldsymbol{\beta}}$

* As $\lambda \rightarrow \infty$, $\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \mathbf{0}_p$

* __Solution path__ of the ridge estimator:
$$\{\hat{\boldsymbol{\beta}}(\lambda): \lambda \in [0,\infty) \}$$

* All regression coefficients are shrunken towards zero as the tuning parameter $\lambda$ increases

* This behaviour is not strictly monotone in $\lambda$: $\lambda_a > \lambda_b$ does not necessarily imply $|\hat{\beta}_j(\lambda_a)| < |\hat{\beta}_j(\lambda_b)|$

---

# Intercept term

* Usually the *intercept term* is not included in the penalty

* If we center the columns of $\mathbf{X}$ to have mean 0 before ridge regression is performed, 
then the intercept estimate ends up just being $\hat{\beta}_0=\bar{y}$

---

# Standardization

* The linear model is *equivariant* under scale changes of the predictors. What it means is that the space of fits using linear combinations of the predictors is the same as the space of linear combinations using scaled versions 

* Some care is needed in the application of ridge regression

* The penalty term $\| \boldsymbol{\beta}\|^2_2 = \sum_{j=1}^{p}\beta^2_j$ treats all the coefficients as equals. This penalty is most natural if all predictors are measured on the same scale

* Therefore, it is best to apply ridge regression after standardizing the predictors  to have mean 0 and standard deviation 1

* This can be accomplished without any loss of generality because the coefficients can be returned on the original scale

* Usually the response is also standardized to have mean zero and standard deviation 1

---

layout: false
class: inverse, middle, center

# Boston housing data

---

# Boston housing data

* The Boston Housing data is a standard benchmark data set for regression models. It contains data for 506 census tracts of Boston from the 1970 census

* The main objective of the Boston Housing data is to investigate variables associated with predicting the median value of homes (continuous `medv` response) within 506 suburban areas of Boston

* We will use the data contained in the `MASS` package

---

| Variable | Description | type |
|---|---|---|
| crim | Crime rate by town | numeric |
| zn |  Proportion of residential land zoned for lots over 25,000 sq.ft | numeric |
| indus | Proportion of non-retail business acres per town | numeric |
| chas | Charles River (tract bounds river) | logical |
| nox | Nitrogen oxides concentration (10 ppm) | numeric |
| rm | Number of rooms per dwelling | numeric |
| age | Proportion of units built prior to 1940 | numeric |
| dis | Distances to Boston employment center | numeric |
| rad | Accessibility to highways | integer |
| tax | Property-tax rate per $10,000 | numeric |
| ptratio | Pupil-teacher ratio by town | numeric |
| black | Proportion of blacks by town | numeric |
| lstat | Lower status of the population (percent) | numeric |
| medv | Median value of homes ($1000s) | numeric |

---

```{r}
rm(list=ls())
library(MASS)
data(Boston)

# least squares
fit.lm = lm(medv ~ ., Boston)
# lm.ridge (MASS)
fit.ridge = lm.ridge(medv ~ ., Boston, lambda=1) 
# coefficients
round(cbind(coef(fit.ridge), coef(fit.lm)),4)
```



---


```{r}
# design matrix and response
X = model.matrix(fit.lm)
y = Boston$medv
n = nrow(X)

# standardization
Xstd = scale(X[,-1])[,]*sqrt((n-1)/n)
ystd = scale(y)[,]*sqrt((n-1)/n)

# ridge solution path
lambdas = c(0,exp(seq(-4,12,length.out = 100)))
hatbetas =sapply(lambdas, function(lambda)
  solve(t(Xstd)%*%Xstd + lambda*diag(ncol(Xstd))) %*% t(Xstd) %*% ystd
)
```

---

```{r}
matplot(log(lambdas), t(hatbetas), type="l", lty=1, ylab="standardized coefficients")
```

---

# glmnet()

```{r, eval=F}
glmnet(X, y,
family="gaussian",
alpha = 0, # default = 1 (LASSO), 0 = ridgde
nlambda = 100, # default
standardize = TRUE, # default
intercept=TRUE # default
)
```

* See the [glmnet vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)

---

```{r}
library(glmnet)
fit = glmnet(Xstd, ystd, alpha = 0, family="gaussian", lambda=lambdas/n)
# solution path
plot(fit, xvar="lambda")
```

---

```{r}
# Original scale
fit = glmnet(X, y, alpha = 0, family="gaussian")
plot(fit, xvar="lambda")
```

---

# Choice of the penalty parameter

* Throughout the introduction of ridge regression and the
subsequent discussion of its properties the penalty parameter is considered known or given

* In practice, it is unknown and the user needs to make an informed decision on its value

* Usually by cross-validation or generalized cross-validation

---

```{r}
# K-fold cross validation
K <- 5
fit.cv <-cv.glmnet(X[,-1],y, alpha=0, nfolds = K, grouped=FALSE)
plot(fit.cv)
```

---

```{r}
# lambda selected by K fold cross validation
hatlambda <-fit.cv$lambda.min
hatlambda
predict(fit.cv, s=hatlambda, type ="coefficients")
```

---

```{r}
rm(list=ls())
data(Boston)
m = 156
n = nrow(Boston)-m
set.seed(123)
test.id = sample(nrow(Boston),m)
test = Boston[test.id,]
train = Boston[-test.id,]
```

---



```{r, echo=F}
lambda.vals = exp(seq(-15,10,by=0.1))
op <- par(mfrow=c(2,3))
colnow=1
for (nuse in c(50,100,350)){
  use.id = sample(n,nuse)
  fits = lm.ridge(medv~.,data=train[use.id,], lambda=lambda.vals)
  preds = as.matrix(test[,-14]) %*% t(coef(fits)[,-1]) +  rep(1,m) %o% coef(fits)[,1]
  resids = matrix(data=test$medv, nrow=dim(preds)[1], ncol=dim(preds)[2], byrow=F)-preds
  RSSs = apply(resids^2, 2, sum)
  par(mfg=c(1,colnow)) # plot on top row
  plot(fits$lambda, sqrt(RSSs/m), main=paste("Ridge using ", nuse, " training data",sep=""),ylab="RMSE",xlab="lambda",log="x", type="l", ylim=c(5.3, 7))
  points(fits$lambda[which.min(RSSs)], sqrt(min(RSSs)/m))
  par(mfg=c(2,colnow)) # bottom row
  plot(apply(fits$coef^2, 2, sum), sqrt(RSSs/m), ylab="RMSE",xlab="L2 norm", type="l", ylim=c(5.3, 7), xlim=c(0,60))
  points(apply(fits$coef^2, 2, sum)[which.min(RSSs)], sqrt(min(RSSs)/m))
  colnow=colnow+1
}
par(op)
```

---

layout: false
class: inverse, middle, center

# Bias-variance trade-off

---


# Ridge regression bias-variance trade-off

* Consider the Fixed-X setting. Assume that the true model is linear 
$$\mathbf{y} =  \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
where the the distribution of the errors is Gaussian, i.e. $\boldsymbol{\varepsilon} \sim N(\mathbf{0}_n,\sigma^2 \mathbf{I}_{n})$

* The linear operator 
$$\mathbf{W}^{\lambda} = [\mathbf{I}_{p\times p} + \lambda(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}]^{-1}$$
transforms the OLS estimator into the ridge estimator 
$$\mathbf{W}^{\lambda} \hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}^{\lambda}$$

* Bias $$\mathbb{E}(\hat{\boldsymbol{\beta}}^{\lambda}) =  \mathbf{X}^\mathsf{T}\mathbf{X}(\lambda\mathbf{I}_{p}  + \mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \boldsymbol{\beta}$$


* Variance
$$\mathbb{V}\mathrm{ar}(\hat{\boldsymbol{\beta}}^{\lambda}) = \sigma^2 \mathbf{W}^{\lambda} (\mathbf{X}^\mathsf{T}\mathbf{X} )^{-1}(\mathbf{W}^{\lambda})^\mathsf{T}$$


---

# Shrinkage

* Estimates are shrunk towards zero

* Consequence
    - Introduces bias
    - Reduces variance

---

# Theorem 2 of Theobald, 1974

* There exists $\lambda > 0$ such that $(\mathbb{B}\mathrm{ias} )^2 +   \mathbb{V}\mathrm{ar}$ of the ridge regression estimator is lower than the OLS estimator 

* For a certain value of $\lambda$, the decrease in variance of the ridge
regression estimator exceeds the increase in its bias

* The optimal choice of $\lambda$ depends on the quantities $\boldsymbol{\beta}$ and $\sigma^2$. These are unknown in practice

---

```{r, echo=FALSE}
f <- function(rho, lam, n) {
  G <- matrix(c(1, rho, rho,1), 2, 2)

  ## OLS
  bias.ols <- rep(0, 2)
  var.ols <- solve(G)/n
  mse.ols <- sum(diag(var.ols)) + crossprod(bias.ols)

  ## ridge
  W <- solve(G + lam/n*diag(2))
  var.ridge <- W %*% G %*% W / n
  bias.ridge <- lam/n * W %*% matrix(c(1,1), 2, 1)
  mse.ridge <- sum(diag(var.ridge)) + crossprod(bias.ridge)
  val <- c(mse.ols, mse.ridge, crossprod(bias.ridge), sum(diag(var.ridge)))
  names(val) <- c("mse.ols", "mse.ridge", "bias.ridge", "var.ridge")
  val
}
lam <- c(0, exp(seq(log(0.001), log(10), length=99)))
Y <- matrix(NA, 100, 4)
for (i in 1:100) {
  Y[i,] <- f(0.5, lam[i], 20)
}
matplot(lam, Y, type="l", col=c("gray50", 1:3), lwd=3, lty=1, xlab=expression(lambda), las=1, ylab="", bty="n")
text(5.5, 0.10, "MSE", xpd=T)
text(9, 0.03, "Var", xpd=T)
text(9, 0.085, expression(Bias^2), xpd=T)
```

---

# Predicting a Gaussian random variable

* $Y = \mu + \varepsilon$ with $\varepsilon \sim N(0,\sigma^2)$

* Training data: $Y_1,\ldots,Y_n$

* Sample mean: $\bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i$

* Bias-Variance decomposition 
$$
\begin{aligned}
\mathbb{E}\{[Y - \bar{Y} ]^2 \} & = \sigma^2 + [\mathbb{B}\mathrm{ias}( \bar{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \bar{Y} )\\
& = \sigma^2 + 0 + \sigma^2/n
\end{aligned}
$$

* Rao-Blackwell theorem says that $\bar{Y}$ has lower variance than any other unbiased estimator

* Is $\bar{Y}$ the optimal prediction for $Y$ ?

---

# Shrunken sample mean

* Shrunken sample mean: $\hat{Y} = \lambda \bar{Y}$ with $\lambda \in [0,1]$ 

* Bias-Variance decomposition 
$$
\begin{aligned}
\mathbb{E}\{[Y - \hat{Y} ]^2 \} & = \sigma^2 + [\mathbb{B}\mathrm{ias}( \hat{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \hat{Y} )\\
& = \sigma^2 + [\mu - \lambda \mu]^2 + \lambda^2(\sigma^2/n)
\end{aligned}
$$

* Solving $\displaystyle \frac{\partial}{\partial \lambda} \mathbb{E}\{[Y - \hat{Y} ]^2 \}  = 0$ gives the value
$$\lambda=\frac{\mu^2}{\mu^2 + \sigma^2/n}$$ 
that minimizes $\mathbb{E}\{[Y - \hat{Y} ]^2 \}$

* However, the optimal $\lambda$ depends on unknown parameters
