<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Ridge regression
### Aldo Solari

---





# Outline

* Alternatives to least squares

* Ridge regression

* Ridge bias-variance trade-off

* Principal components regression

---

# Matrix notation

* Vector of response: 
`$$\underset{n\times 1}{\mathbf{y}} = 
\left[
\begin{array}{c}
y_1   \\
\cdots\\
y_i  \\
\cdots\\
y_n \\
\end{array}\right]$$`

* Design matrix:
`$$\underset{n\times p}{\mathbf{X}} = [\mathbf{x}_1 \cdots \mathbf{x}_p] =  \left[
\begin{array}{cccccc}
x_{1}^\mathsf{T}   \\
x_{2}^\mathsf{T}  \\
\cdots   \\
x_{i}^\mathsf{T}    \\
\cdots\\
x_{n}^\mathsf{T}\\
\end{array}\right] = \left[
\begin{array}{cccccc}
x_{11}  &amp; x_{12}  &amp; \cdots   &amp;  x_{1j}  &amp; \cdots   &amp;   x_{1p}  \\
x_{21}  &amp; x_{22} &amp; \cdots   &amp;  x_{2j}  &amp; \cdots   &amp;   x_{2p}  \\
\cdots   &amp; \cdots   &amp;  \cdots &amp; \cdots   &amp;  \cdots  \\
x_{i1}  &amp; x_{i2} &amp; \cdots   &amp;  x_{ij}&amp; \cdots   &amp; x_{ip}    \\
\cdots   &amp; \cdots   &amp;  \cdots  &amp;  \cdots   &amp;  \cdots\\
x_{n1}   &amp; x_{n2} &amp; \cdots   &amp; x_{nj}    &amp;  \cdots   &amp;   x_{np}\\
\end{array}\right]$$`

---

# Linear regression

* Assume `\(\mathrm{rank}(\mathbf{X}) = p\)`

* Least squares problem: 
`$$\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2$$`
where  `\(\| \underset{p \times 1}{ \mathbf{v}} \|_{2} = (\mathbf{v}^\mathsf{T}\mathbf{v})^{1/2} = \sqrt{\sum_{j=1}^{p} v_j^2}\)` denotes the `\(\ell_2\)` norm

* OLS estimator: `$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`

* Fitted values: `\(\underset{n\times 1}{\hat{\mathbf{y}} } =  \mathbf{X}\hat{\boldsymbol{\beta}}\)`

* Test data: `\(\underset{m\times 1}{\mathbf{y}^*}\)`, `\(\underset{m\times p}{\mathbf{X}^*}\)`, prediction on test data: `\(\underset{m\times 1}{\hat{\mathbf{y}}^*} = \mathbf{X}^*\hat{\boldsymbol{\beta}}\)`

---

# The Geometry of Least Squares

![](images/projection.png)

From AS, Figure A.2

* `\(\hat{\boldsymbol{\mu}}  = \hat{\mathbf{y}} =  \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{H} \mathbf{y}\)` : projection of `\(\mathbf{y}\)` on column space of `\(\mathbf{X}\)`, i.e. `\(\mathcal{C}(\mathbf{X})\)`

* `\(\mathbf{H} = \mathbf{X} (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\)` : projection (hat) matrix on `\(\mathcal{C}(\mathbf{X})\)`

* `\(\mathbf{y} - \hat{\boldsymbol{\mu}} = (\mathbf{I}_n - \mathbf{H})\mathbf{y}\)` : residuals

---

layout: false
class: inverse, middle, center

# Alternatives to least squares

---

# Why consider alternatives to least squares?

* In the Fixed-X setting, the variance of the OLS estimator is
`$$\frac{\sigma^2 p}{n}$$`
    - If `\(n \gg p\)`, then the least squares estimates tend to have low variance 

    - If `\(n&gt;p\)` but `\(n \approx p\)`, then there can be a lot of variability in the least squares fit, resulting in overfitting and poor predictions

    - If `\(n=p\)`, then the least squares estimates fit the data exactly, i.e. `\(\hat{y}_i = y_i\)`, resulting in complete overfitting and horrible predictions

    - If `\(n&lt;p\)`, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all

* We need to keep the variance in check

---

# `\(n \approx p\)`

* `\(n=10\)` and `\(p=9\)`

* `\(Y = \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_9X_9 +  \varepsilon\)`

* True `\(\boldsymbol{\beta} = (1,0,0,\ldots,0)^\mathsf{T}\)`


```r
n = 10
p = 9
set.seed(1793)
X = matrix(rnorm(n*p), nrow=n, ncol=p)
y = X[,1] + rnorm(n,0,0.5)
fit = lm(y~ 0 + X)
coef(fit)
```

```
        X1         X2         X3         X4         X5         X6         X7         X8         X9 
 13.527953  -5.466373 -24.512290  -6.610876   2.732160 -10.670639  15.472750  16.831498 -11.331475 
```

Large estimates of regression coefficients (in absolute value) are often an indication of overfitting

---



```r
yhat = predict(fit)
plot(X[,1],y, xlab="x1")
ix = sort(X[,1], index.return=T)$ix
lines(X[ix,1], yhat[ix])
abline(a=0,b=1, col=4)
```

![](9_Ridge_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---

# The failure of least squares in high dimensions

* When `\(\mathrm{rank}(\mathbf{X}) &lt; p\)`, there are infinitely many solutions in the least square problem

* Suppose `\(p &gt; n = \mathrm{rank}(\mathbf{X})\)`. Let `\(\mathcal{C}(\mathbf{X})\)` be the `\(n\)`-dimensional space spanned by the columns of `\(\mathbf{X}\)` and `\(\mathcal{V}=\mathcal{C}(\mathbf{X})^\bot\)` the `\(p-n\)` dimensional space orthogonal complement of `\(\mathcal{C}(\mathbf{X})\)`, i.e.  the non-trivial null space of `\(\mathbf{X}\)`

* Then `\(\mathbf{X}\mathbf{v} = \mathbf{0}_p\)` for all `\(\mathbf{v} \in \mathcal{V}\)`, and `\(\mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{v} = \mathbf{X}^\mathsf{T}\mathbf{0}_p = \mathbf{0}_n\)`, the solution of the normal equations `\(\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T}\mathbf{y}\)` is
`$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-}\mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{v} \quad \forall\,\, \mathbf{v} \in \mathcal{V}$$`
where `\(\mathbf{A}^-\)` denotes the Moore-Penrose inverse of the matrix `\(\mathbf{A}\)`

---

# Three classes of methods

1. **Subset Selection** &lt;br&gt; We identify a subset of the `\(p\)` predictors
that we believe to be related to the response. We then fit a
model using least squares on the reduced set of variables.

2. **Dimension Reduction** &lt;br&gt; We project the `\(p\)` predictors into a `\(q\)`-dimensional subspace, where `\(q &lt; p\)`. Then these `\(q\)` projections are used as predictors to fit a linear regression model by least squares

3. **Regularization** &lt;br&gt; We fit a model involving all `\(p\)` predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates.  By *constraining* or *shrinking* the estimated coefficients, we can substantially reduce the variance at the cost of a neglige increase in bias


---

# Constrained estimation

ISLR, Figure 6.7

&lt;img src="images/l2.jpg" width="40%" height="40%" style="display: block; margin: auto;" /&gt;



---

layout: false
class: inverse, middle, center

# Ridge regression

---

# Constrained estimation

* Solve the constrained minimization problem

`\begin{aligned}
&amp; \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2\\ &amp; \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t
\end{aligned}`

* The constrained problem can be solved by means of the Karuch-Kuhn-Tucker (KTT) multiplier method, which minimizes a function subject to inequality constraint


---

# Penalized estimation

* Penalized least squares
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \left\{ \sum_{i=1}^{n}(y_i - x_i^\mathsf{T}\boldsymbol{\beta} )^2 + \lambda\sum_{j=1}^{p}\beta_j^2 \right\} = \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \left\{ \underbrace{ \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 }_{\mathrm{RSS} }+ \underbrace{\lambda\| \boldsymbol{\beta}\|^2_2}_{\mathrm{penalty}} \right\}$$`

* Here `\(\lambda \in [0,\infty)\)` is the __tuning parameter__ which controls the strenght of the penalty term

* The minimum of the RSS is attained at `\(\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}\)` while the minimum of the ridge penalty is attained at `\(\boldsymbol{\beta}=\mathbf{0}_p\)`. The effect of the penalty in this balancing act is to __shrink__ the
coefficient estimates towards zero

---

# Ridge estimate


`\begin{aligned}
&amp; \frac{\partial}{\partial \boldsymbol{\beta}} \left( \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|^2_2 \right)\\
&amp; -2 \mathbf{X}^\mathsf{T} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + 2\lambda \boldsymbol{\beta} = \mathbf{0}_p
\end{aligned}`

* The solution of the minimization problem is the __ridge estimate__ :
`$$\hat{\boldsymbol{\beta}}(\lambda) = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`
where `\(\mathbf{I}_p\)` is the `\(p\times p\)` identity matrix

* For any design matrix `\(\mathbf{X}\)`, the quantity
`\((\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )\)` is always invertible provided that `\(\lambda &gt; 0\)`; thus,
there is always a unique solution `\(\hat{\boldsymbol{\beta}}(\lambda)\)`

---

# Solution path

* As `\(\lambda \rightarrow 0\)`, `\(\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \hat{\boldsymbol{\beta}}\)`

* As `\(\lambda \rightarrow \infty\)`, `\(\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \mathbf{0}_p\)`

* __Solution path__ of the ridge estimator:
`$$\{\hat{\boldsymbol{\beta}}(\lambda): \lambda \in [0,\infty) \}$$`

* All regression coefficients are shrunken towards zero as the tuning parameter `\(\lambda\)` increases

* This behaviour is not strictly monotone in `\(\lambda\)`: `\(\lambda_a &gt; \lambda_b\)` does not necessarily imply `\(|\hat{\beta}_j(\lambda_a)| &lt; |\hat{\beta}_j(\lambda_b)|\)`

---

# Intercept term

* Usually the *intercept term* is not included in the penalty

`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \left\{ \sum_{i=1}^{n}\Big(y_i - \beta_0 -  \sum_{j=1}^{p}x_{ij}\beta_j \Big)^2 + \lambda\sum_{j=1}^{p}\beta_j^2 \right\}$$`

* If we center the columns of `\(\mathbf{X}\)` to have mean 0 before ridge regression is performed, i.e.
`$$\tilde{\mathbf{x}}_{j} = \mathbf{x}_{j} - \mathbf{1}_n \bar{x}_j$$` where
`\(\bar{x}_j = \frac{1}{n}\sum_{i=1}^{n}x_{ij}/n\)` and `\(\mathbf{1}_n\)` is the `\(n\)`-vector of 1s, then the intercept estimate ends up just being `\(\hat{\beta}_0=\bar{y}\)`

---

# Standardization

* The linear model is *equivariant* under scale changes of the predictors. What it means is that the space of fits using linear combinations of the predictors is the same as the space of linear combinations using scaled versions 

* Some care is needed in the application of ridge regression

* The penalty term `\(\| \boldsymbol{\beta}\|^2_2 = \sum_{j=1}^{p}\beta^2_j\)` treats all the coefficients as equals. This penalty is most natural if all predictors are measured on the same scale

* Therefore, it is best to apply ridge regression after standardizing the predictors to have mean 0 and standard deviation 1, i.e.
`\(\mathbf{z}_{j} = (\mathbf{x}_{j} - \mathbf{1}_n \bar{x}_j )/s_j\)` where `\(s_j = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (x_{ij}-\bar{x}_j)^2}\)`

* This can be accomplished without any loss of generality because the coefficients can be returned on the original scale

* Usually the response is also standardized to have mean zero and standard deviation 1

---


# Hitters data

A data frame with 322 observations of major league players on the following 20 variables

.pull-left[
AtBat
Number of times at bat in 1986

Hits
Number of hits in 1986

HmRun
Number of home runs in 1986

Runs
Number of runs in 1986

RBI
Number of runs batted in in 1986

Walks
Number of walks in 1986

Years
Number of years in the major leagues

CAtBat
Number of times at bat during his career

CHits
Number of hits during his career

CHmRun
Number of home runs during his career

CRuns
Number of runs during his career
]

.pull-right[
CRBI
Number of runs batted in during his career

CWalks
Number of walks during his career

League
A factor with levels A and N indicating player's league at the end of 1986

Division
A factor with levels E and W indicating player's division at the end of 1986

PutOuts
Number of put outs in 1986

Assists
Number of assists in 1986

Errors
Number of errors in 1986

**Salary**
1987 annual salary on opening day in thousands of dollars

NewLeague
A factor with levels A and N indicating player's league at the beginning of 1987
]

---


```r
rm(list=ls())
library(ISLR)
data(Hitters)
# remove rows with missing values
Hitters =na.omit(Hitters )
# least squares
fit.lm = lm(Salary ~ ., Hitters)
# lm.ridge (MASS)
library(MASS)
fit.ridge = lm.ridge(Salary ~ ., Hitters, lambda=1) 
```

---


```r
# coefficients
round(data.frame(lm=coef(fit.lm), ridge=coef(fit.ridge)),4)
```

```
                   lm     ridge
(Intercept)  163.1036  159.9382
AtBat         -1.9799   -1.8396
Hits           7.5008    6.5845
HmRun          4.3309    2.0781
Runs          -2.3762   -1.1535
RBI           -1.0450   -0.3568
Walks          6.2313    5.7407
Years         -3.4891   -8.0870
CAtBat        -0.1713   -0.0987
CHits          0.1340    0.1961
CHmRun        -0.1729    0.4764
CRuns          1.4543    0.9687
CRBI           0.8077    0.4969
CWalks        -0.8116   -0.7000
LeagueN       62.5994   62.1389
DivisionW   -116.8492 -120.5144
PutOuts        0.2819    0.2813
Assists        0.3711    0.3277
Errors        -3.3608   -3.6038
NewLeagueN   -24.7623  -27.4197
```

---


```r
# design matrix and response
X = model.matrix(fit.lm)
y = Hitters$Salary
n = nrow(X)

# standardization
Xstd = scale(X[,-1])[,]*sqrt((n-1)/n)
ystd = scale(y)[,]*sqrt((n-1)/n)

# ridge solution path
lambdas = c(0,exp(seq(-4,12,length.out = 100)))
hatbetas =sapply(lambdas, function(lambda)
  solve(t(Xstd)%*%Xstd + lambda*diag(ncol(Xstd))) %*% t(Xstd) %*% ystd
)
```

---


```r
matplot(log(lambdas), t(hatbetas), type="l", lty=1, ylab="standardized coefficients")
```

![](9_Ridge_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---

# glmnet()


```r
glmnet(X, y,
family="gaussian",
alpha = 0, # default = 1 (LASSO), 0 = ridgde
nlambda = 100, # default
standardize = TRUE, # default
intercept=TRUE # default
)
```

See the [glmnet vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)

---


```r
library(glmnet)
fit = glmnet(Xstd, ystd, alpha = 0, family="gaussian", lambda=lambdas/n)
# solution path
plot(fit, xvar="lambda")
```

![](9_Ridge_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

---


```r
# Original scale
fit = glmnet(X, y, alpha = 0, family="gaussian")
plot(fit, xvar="lambda")
```

![](9_Ridge_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---

# Choice of the penalty parameter

* Throughout the introduction of ridge regression and the
subsequent discussion of its properties the penalty parameter is considered known or given

* In practice, it is unknown and the user needs to make an informed decision on its value

* Usually by cross-validation or generalized cross-validation

---


```r
# K-fold cross validation
K &lt;- 5
fit.cv &lt;-cv.glmnet(X[,-1],y, alpha=0, nfolds = K, grouped=FALSE)
plot(fit.cv)
```

![](9_Ridge_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---


```r
# lambda selected by K fold cross validation
hatlambda &lt;-fit.cv$lambda.min
hatlambda
```

```
[1] 25.52821
```

---

# LOOCV shortcut

The ridge hat matrix is defined as
`$$\mathbf{H}(\lambda) = \mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T}$$`

It can be proven that
`$$\frac{1}{n}\sum_{i=1}^{n}\Big[ y_i - x_i^\mathsf{T} \hat{\boldsymbol{\beta}}^{-i}(\lambda) \Big]^2 = \frac{1}{n}\sum_{i=1}^{n} \left[ \frac{y_i - x_i^\mathsf{T} \hat{\boldsymbol{\beta}}(\lambda)}{1-\{\mathbf{H}(\lambda)\}_{ii}} \right]^2$$`
where `\(\hat{\boldsymbol{\beta}}^{-i}(\lambda)\)` is the ridge estimate obtained by excluding the `\(i\)`th observation and `\(\{\mathbf{H}(\lambda)\}_{ii}\)` is the `\(i\)`th diagonal element of `\(\mathbf{H}(\lambda)\)`


Hence, the prediction performance for a given `\(\lambda\)`
can be assessed directly from the ridge hat matrix `\(\mathbf{H}(\lambda)\)` and the response vector without the recalculation of
the `\(n\)` leave-one-out ridge estimators

Computationally, this is a considerable gain

---


```r
rm(list=ls())
data(Hitters)
Hitters = na.omit(Hitters)
Hitters = data.frame(model.matrix(lm(Salary ~ ., Hitters))[,-1], Salary=Hitters$Salary)
m = 100
n = nrow(Hitters)-m
set.seed(123)
test.id = sample(nrow(Hitters),m)
test = Hitters[test.id,-20]
ytest = Hitters[test.id,20]
train = Hitters[-test.id,]

lambda.vals = exp(seq(-10,10,by=0.1))
fits = lm.ridge(Salary~.,data=train, lambda=lambda.vals)
preds = as.matrix(test) %*% t(coef(fits)[,-1]) +  rep(1,m) %o% coef(fits)[,1]
resids = matrix(data=ytest, nrow=dim(preds)[1], ncol=dim(preds)[2], byrow=F)-preds
RMSEs = sqrt( apply(resids^2, 2, sum)/m )
```

---


```r
plot(fits$lambda, RMSEs, type="l", xlab="lambda", ylab="RMSE", log="x")
points(fits$lambda[which.min(RMSEs)],min(RMSEs))
```

![](9_Ridge_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

---

layout: false
class: inverse, middle, center

# Bias-variance trade-off

---


# Ridge regression bias-variance trade-off

* Consider the Fixed-X setting

* Assume that the true model is 
`\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)`
with  `\(\boldsymbol{\varepsilon} \sim N(\mathbf{0}_n,\sigma^2 \mathbf{I}_{n})\)`

* The linear operator 
`$$\mathbf{W}(\lambda) = [\mathbf{I}_{p\times p} + \lambda(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}]^{-1}$$`
transforms the OLS estimator into the ridge estimator 
`$$\mathbf{W}(\lambda) \hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}(\lambda)$$`

* Bias `$$\mathbb{E}[\hat{\boldsymbol{\beta}}(\lambda)] =  \mathbf{X}^\mathsf{T}\mathbf{X}(\lambda\mathbf{I}_{p}  + \mathbf{X}^\mathsf{T}\mathbf{X})^{-1} \boldsymbol{\beta}$$`


* Variance
`$$\mathbb{V}\mathrm{ar}[\hat{\boldsymbol{\beta}}(\lambda)] = \sigma^2 \mathbf{W}(\lambda) (\mathbf{X}^\mathsf{T}\mathbf{X} )^{-1}\mathbf{W}(\lambda)^\mathsf{T}$$`

---

# Does ridge regression improve over least squares?

**Theorem 2 of Theobald (1974)**

&gt; Consider the Fixed-X setting. 
Assume that the true model is 
`\(\mathbf{y} =  \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}\)` with `\(\boldsymbol{\varepsilon} \sim N(\mathbf{0}_n,\sigma^2 \mathbf{I}_{n})\)`. 
Then there exists `\(\lambda &gt; 0\)` such that `$$(\mathbb{B}\mathrm{ias} )^2 +   \mathbb{V}\mathrm{ar}$$` of the ridge regression estimator is lower than the OLS estimator 

* For a certain value of `\(\lambda\)`, the decrease in variance of the ridge regression estimator exceeds the increase in its bias

* The optimal choice of `\(\lambda\)` depends on the quantities `\(\boldsymbol{\beta}\)` and `\(\sigma^2\)`. These are unknown in practice

---

![](9_Ridge_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

---

# Predicting a Gaussian random variable

* `\(Y = \beta + \varepsilon\)` with `\(\varepsilon \sim N(0,\sigma^2)\)`

* Training data: `\(Y_1,\ldots,Y_n\)`

* Sample mean: `\(\bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i\)`

* Bias-Variance decomposition 
$$
`\begin{aligned}
\mathbb{E}\{[Y - \bar{Y} ]^2 \} &amp; = \sigma^2 + [\mathbb{B}\mathrm{ias}( \bar{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \bar{Y} )\\
&amp; = \sigma^2 + 0 + \sigma^2/n
\end{aligned}`
$$

* Rao-Blackwell theorem says that `\(\bar{Y}\)` has lower variance than any other unbiased estimator

* Is `\(\bar{Y}\)` the optimal prediction for `\(Y\)` ?

---

# Shrunken sample mean

* Shrunken sample mean: `\(\hat{Y} = \lambda \bar{Y}\)` with `\(\lambda \in [0,1]\)` 

* Bias-Variance decomposition 
$$
`\begin{aligned}
\mathbb{E}\{[Y - \hat{Y} ]^2 \} &amp; = \sigma^2 + [\mathbb{B}\mathrm{ias}( \hat{Y} ) ]^2 + \mathbb{V}\mathrm{ar}( \hat{Y} )\\
&amp; = \sigma^2 + [\beta - \lambda \beta]^2 + \lambda^2\frac{\sigma^2}{n)}
\end{aligned}`
$$

* Solving `\(\displaystyle \frac{d}{d \lambda} \mathbb{E}\{[Y - \hat{Y} ]^2 \}  = 0\)` gives the value
`$$\lambda=\frac{\beta^2}{\beta^2 + \sigma^2/n}$$` 
that minimizes `\(\mathbb{E}\{[Y - \hat{Y} ]^2 \}\)`

* However, the optimal `\(\lambda\)` depends on unknown parameters

---

layout: false
class: inverse, middle, center

# Principal components regression

---

# Singular value decomposition

* Consider singular value decomposition (SVD) of the centered matrix `\(\mathbf{X}\)` with `\(\mathrm{rank}(\mathbf{X})=p\)`, i.e. 
`$$\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}$$`

* `\(\underset{n\times p}{\mathbf{U}}\)` with orthonormal columns, i.e. `\(\mathbf{U}^\mathsf{T}\mathbf{U}= \mathbf{I}_p\)`

*  `\(\underset{p\times p}{\mathbf{V}}\)` orthogonal matrix, i.e. `\(\mathbf{V}^\mathsf{T}\mathbf{V}=\mathbf{V}\mathbf{V}^\mathsf{T} = \mathbf{I}_p\)`

*  `\(\underset{p\times p}{\mathbf{D}}=\mathrm{diag}(d_1,\ldots,d_p)\)` where `\(d_1\geq \ldots d_p\geq 0\)` are the singular values 

---

# Principal components analysis

* Principal components analysis (PCA) is a popular unsupervised learning approach for deriving a low-dimensional set of features from a large set of variables

* From a centered matrix `\(\mathbf{X}\)`, construct the `\(p\)` principal components
`\(\mathbf{z}_1, \ldots, \mathbf{z}_p\)`, i.e. the `\(p\)` columns of 
`\(\mathbf{Z} = \mathbf{X}\mathbf{V}\)`
and then use the first `\(q&lt;p\)` components
as the predictors in a linear regression model

* The first principal component is that (normalized) linear combination of the variables with the largest variance
`$$\mathbf{z}_1 = \mathbf{X} \mathbf{v}_1 = \mathbf{u}_1 d_1, \qquad \mathbb{V}\mathrm{ar}(\mathbf{z}_1)=\frac{(d_1)^2}{n}$$`
where `\(\mathbf{v}_1,\ldots,\mathbf{v}_p\)` are the columns of `\(\mathbf{V}\)`

* The second principal component has largest variance, subject to being uncorrelated with the first

`$$\mathbf{z}_2 = \mathbf{X} \mathbf{v}_2 = \mathbf{u}_2 d_2, \qquad \mathbb{V}\mathrm{ar}(\mathbf{z}_1)=\frac{(d_2)^2}{n}, \qquad \mathbf{z}_2 \bot \mathbf{z}_1$$`

* And so on

---

# Comparing pc regression to least squares and ridge


* Least squares regression

$$
\hat{\boldsymbol{\beta}}  = \mathbf{V}\mathrm{\,\,diag}\Big(\frac{1}{d_1},\ldots,\frac{1}{d_p}\Big) \mathbf{U}^\mathsf{T}\mathbf{y}
$$

* Ridge regression

$$
\hat{\boldsymbol{\beta}}(\lambda)  = \mathbf{V}\mathrm{\,\,diag}\Big(\frac{d_1}{d^2_1 + \lambda},\ldots,\frac{d_p}{d^2_p + \lambda}\Big) \mathbf{U}^\mathsf{T}\mathbf{y}
$$

* Principal components regression

$$
\hat{\boldsymbol{\beta}}(q)  = \mathbf{V}\mathrm{\,\,diag}\Big(\frac{1}{d_1},\ldots,\frac{1}{d_q},0,\ldots,0\Big) \mathbf{U}^\mathsf{T}\mathbf{y}
$$

* Both ridge and principal components regression operate on the singular values of the (centered) design matrix `\(\mathbf{X}\)`

* But where principal component regression
thresholds the singular values of `\(\mathbf{X}\)`, ridge regression shrinks them (depending on their size). Note that since `\(\lambda\geq 0\)`, we have `\(\frac{d^2_j}{d^2_j + \lambda}\leq 1\)`

---


```r
rm(list=ls())
data(Hitters)
Hitters = na.omit(Hitters)
X = model.matrix(lm(Salary ~ ., Hitters))
n = nrow(X)
Z = scale(X[,-1])[,]*sqrt((n-1)/n)
# SVD
U = svd(Z)$u
D = diag(svd(Z)$d)
V = svd(Z)$v
```

---


```r
library(pls)
fit.pcr=pcr(Salary ~ ., data=Hitters, scale=TRUE, validation ="CV")
validationplot(fit.pcr, val.type="MSEP")
```

![](9_Ridge_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
