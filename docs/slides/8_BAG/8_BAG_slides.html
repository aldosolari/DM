<!DOCTYPE html>
<html>
  <head>
    <title>Bagging</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Bagging
### Aldo Solari

---





# Outline

* Introduction
* Bagging
* Spam data

---
layout: false
class: inverse, middle, center

# Introduction

---

# Ensemble methods

* Ensemble methods are techniques that create multiple
models and then combine them to produce improved results

* These models, when used as inputs of ensemble methods,
are often called __base learners__ or __weak learners__

* Ensemble learning is appealing because that it is able to
boost weak learners which are slightly better than random
guess to strong learners which can make very accurate
predictions

* However, ensemble methods increases __computation time__ and
reduces __interpretability__

* For example, classification and regression trees are simple and useful for interpretation. However they  are typically not competitive with other approaches in terms of prediction accuracy

* We will see that ensemble methods such as __bagging__ and __random forests__ grow multiple trees which are then combined to yield a single prediction. Combining a large number of trees can often result in
dramatic improvements in prediction accuracy, at the
expense of some interpretation loss

---

# Instability of trees

* The primary disadvantage of trees is that they are rather
unstable (high variance)

* In other words, a small change in the data often results in a completely different tree

* One major reason for this instability is that if a split changes, all the splits under it change as well, thereby propagating the variability

* We will learn how to  control the variance, or __stabilize__ the predictions made by trees

* In  doing so, we can greatly improve prediction accuracy but we suffer in terms of interpretability

---

# The boostrap 

* The __bootstrap__ is an useful resampling tool in statistics

* A bootstrap sample of size `\(n\)` from the training data is
`$$(\tilde{x}_1, \tilde{y}_1), (\tilde{x}_2, \tilde{y}_2), \ldots, (\tilde{x}_n, \tilde{y}_n)$$`
where each `\((\tilde{x}_i, \tilde{y}_i)\)` are drawn from uniformly at random from `$$(x_1, y_1), (x_2, y_2), \ldots, (x_n, x_n)$$` with __replacement__

* Not all of the training points are represented in a bootstrap
sample, and some are represented more than once. The probability for one observation not to be drawn in one draw is `\(1 - \frac{1}{n}\)`

* For large `\(n\)`, the probability for one observation not to be drawn in any of the `\(n\)` draws is
`$$\lim_{n\rightarrow \infty} \left(1- \frac{1}{n}\right)^n = \frac{1}{e} \approx 0.368$$`

* We can expect `\(\approx 1/3\)` of the `\(n\)` original observations to be __out-of-bag__ (OOB)

---


```r
rm(list=ls())
n = 1000
original = 1:n
set.seed(123)
bagged = sample(original, size=n, replace=TRUE)
# proportion of OOB observations
length(setdiff(original,bagged))/n
```

```
[1] 0.365
```


---
layout: false
class: inverse, middle, center

# Bagging

---

# Bagging trees

* Bootstrap AGGregation, or __bagging__, is a general procedure for reducing the variance of a model and it is particularly useful in the case of regression or classification trees

1. Generate `\(B\)` different bootstrapped training sets
`$$(\tilde{x}_1^b, \tilde{y}_1^b), (\tilde{x}_2^b, \tilde{y}_2^b), \ldots, (\tilde{x}_n^b, \tilde{y}_n^b), \qquad b=1,\ldots,B$$`

2. Fit a regression tree `\(\hat{f}^{b}\)` or a classification tree `\(\hat{c}^{b}\)` for each bootstrapped training set (simple strategy: grow fairly large trees on each bootstrapped training set, with no pruning)

3. Average all the predictions:
`$$\bar{f}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{b}(x)$$`
for regression trees and
`$$\bar{c}(x) = \mathrm{Mode}\{\hat{c}^{b}(x),b=1,\ldots,B\}$$`
for classification trees (consensus)

---

.pull-left[
![](images/bootstrap.jpg)
]

.pull-right[
Hastie, Tibshirani and Friedman (2009) [The Elements of Statical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/) (ESL) p. 284: `\(n = 30\)` training data points, `\(p = 5\)`
predictors, and `\(K = 2\)` classes. No pruning used in growing trees
]

---


ESL p. 285: Bagging helps decrease the misclassification rate of the classifier
(evaluated on a large independent test set). Look at the orange
curve:

![](images/improved.jpg)



---

# Bagging estimates of class probabilities

* For a classification problem with `\(K\)` classes, we may want to estimate the class probabilities out of our bagging procedure. What about using the proportion of votes that were for each class? This is generally not a good estimate

* Classification trees already gives us a set of
predicted class probabilities at `\(x\)`: `\(\hat{p}_k^b(x), k=1,\ldots,K\)`. These are simply the proportion of points in the appropriate region that
are in each class

* The bagging estimates of class probabilities are given by
`$$\bar{p}_k(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{p}_k^{b}(x), \quad k=1,\ldots,K$$`

* The final bagged classifier just chooses the class with the highest
probability

* This form of bagging is preferred if it is desired to get estimates of
the class probabilities. Also, it can sometimes help the overall
prediction accuracy

---

# Breiman (1996) Bagging predictors

Example from the original Breiman paper on bagging: comparing
the misclassification error of the CART tree (pruning performed by
cross-validation) and of the bagging classifier (with `\(B = 50\)`):

| Data set | CART | Bagging | Decrease |
|---|---|----|---|
| waveform | 29.1 | 19.3 | 34% |
| heart | 4.9 | 2.8 | 43% |
| breast cancer | 5.9 | 3.7 | 37% |
| ionosphere | 11.2 | 7.9 | 29% |
| diabetes | 25.3 | 23.9 | 6% |
| glass | 30.4 | 23.6 | 22% |
| soybean | 8.6 | 6.8 | 21% |

---

# Why is bagging working?

* Why is bagging working? Here is a simplified setup with `\(K = 2\)`
classes "1" and "2" to help understand the basic phenomenon

* Suppose that for a given `\(x\)`, we have an odd number `\(B\)` of independent classifiers `\(\hat{c}^b(x), b=1,\ldots,B\)`, and each classifier has a misclassification rate of 40%. Assume without loss of generality that the true class at `\(x\)` is "1", so
`$$\mathrm{Pr}(\hat{c}^b(x) = 2 ) = 0.4$$`

* Let `\(Z = \sum_{b=1}^{B}I\{\hat{c}^b(x) = 2\}\)` be the number of votes for class "2", and note that 
`$$Z \sim \mathrm{Binomial}(B,0.4)$$`

* Therefore the misclassification rate of the bagged classifier is
`$$\mathrm{Pr}(\bar{c}^b(x) = 2 ) = \mathrm{Pr}(Z \geq (B+1)/2)$$`
which goes `\(\rightarrow 0\)` as `\(B\rightarrow \infty\)`. In other words, the
bagged classifier has perfect predictive accuracy as the number of
sampled data sets `\(B\rightarrow \infty\)`

* Of course, the caveat here is __independence__. The classifiers that we
use in practice, `\(\hat{c}^b\)` are clearly not independent, because they
are fit on very similar data sets (bootstrap samples from the same
training set)

---

# Wisdom of crowds

* The wisdom of crowds is a concept popularized outside of statistics
to describe the same phenomenon. It is the idea that the collection
of knowledge of an __independent__ group of people can exceed the
knowledge of any one person individually

* Assume that we have an odd number `\(n&gt;1\)` of independent individuals, and each individual has probability `\(\pi\)` of making a correct guess

* The number of correct guesses is 
`$$Z \sim \mathrm{Binomial}(n, \pi)$$`

* The majority vote makes the correct guess when `\(Z \geq z\)` with `\(z=(n+1)/2\)`, i.e.
`$$\mathrm{Pr}(Z \geq z) = \sum_{k=z}^{n}{n\choose k}\pi^{k}(1-\pi)^{n-k}$$`

* e.g. if `\(n=3\)` and `\(\pi = 70\%\)`, then the majority vote makes the correct guess with probability `\(78.4\%\)`

---


```r
rm(list=ls())
pi = seq(0,1,by=0.01)
n = 3
z = (n+1)/2
Pr = sapply(pi, function(p) sum(dbinom(z:n, size=n, prob=p) ))
plot(pi, Pr, type="l", xlab="Probability of being correct (individual)", ylab="Probability of being correct (group)")
```

![](8_BAG_slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---

# When will bagging fail?

* Now suppose that we consider the same simplified setup as before
(independent classifiers), but each classifier has a misclassification
rate:
`$$\mathrm{Pr}(\hat{c}^b(x) = 2 ) = 0.6$$`

* Then by the same arguments,  
`$$\lim_{B\rightarrow \infty }\mathrm{Pr}(\bar{c}^b(x) = 2 )  \rightarrow 1$$`
 In other words, the
bagged classifier is perfectly inaccurate as the number of bootstrapped
data sets 

* Again, the independence assumption doesnâ€™t hold with trees, but
the take-away message is clear: bagging a good classifier can
improve predictive accuracy, but bagging a bad one can seriously
degrade predictive accuracy

---

# Disadvantages

It is important to discuss some disadvantages of bagging:

* __Loss of interpretability__: the final bagged classifier is not a
tree, and so we forfeit the clear interpretative ability of a
classification tree

* __Computational complexity__: we are essentially multiplying the
work of growing a single tree by `\(B\)` (especially if we are using
the more involved implementation that prunes and validates
on the original training data)

---

* [ISL, Fig. 8.7](http://www-bcf.usc.edu/~gareth/ISL/Chapter8/8.7.pdf) 

* Top Row: A two-dimensional classification example in which
the true decision boundary is linear, and is indicated by the shaded regions.
A classical approach that assumes a linear boundary (left) will outperform a decision
tree that performs splits parallel to the axes (right)

* Bottom Row: Here the
true decision boundary is non-linear. Here a linear model is unable to capture
the true decision boundary (left), whereas a decision tree is successful (right)

---

* ESL p. 288: Data with two features and two classes, separated by a linear
boundary. The 100 data points shown have two features and two classes, separated by the gray
linear boundary `\(x_1 + x_2 = 1\)`. Consider a classification rule with a single split along either `\(x_1\)` or `\(x_2\)` that produces the largest decrease in training misclassification error. Bagging doesn't help here. The decision boundary obtained from bagging over
`\(B = 50\)` bootstrap samples is shown by the blue curve in the left panel. The test error rate is 0.166

![](images/decrule.jpg)

---

# Out-of-bag error

* Each bagged tree makes use of `\(\approx 2/3\)` of the original observations

* We can predict the response for the `\(i\)`th observation using
each of the bagged trees in which that observation was OOB

* This yields `\(\approx B/3\)` predictions for the `\(i\)`th observation, which
we average (probability or consensus)

* This estimate is essentially the LOOCV error for bagging, if `\(B\)` is large

---

ESL p. 592:

![](images/OOB.jpg)

---


[ISL, Fig. 8.8](http://www-bcf.usc.edu/~gareth/ISL/Chapter8/8.8.pdf) 

Bagging and random forest results for the Heart data. The test error (black and orange) is shown as a function of `\(B\)`, the number of bootstrapped training sets used. Random forests were applied with `\(m=\sqrt{p}\)`. The dashed line indicates the test error resulting from a single classification tree. The green and blue traces show the OOB error, which in this case is considerably lower


---
layout: false
class: inverse, middle, center

# Spam data

---

Dear Google User,

You have been selected as a winner for using our free services !!!

Find attached email with more details. 

Its totally free and you won't be sorry !!!

Congratulations !!!

Matt Brittin. CEO Google UK

---

# Spam data


* 4601 email messages sent to `George` at HP-Labs

* He labeled 1813 of these as `spam`, with the remainder being `good` email 

* The goal is to build a customized spam filter for George: predict whether an e-mail message is `spam` or `good`

* Recorded for each email message is the relative frequency of certain key words (e.g. `business` , `address` , `free` ,  `George`) and certain characters: ( , [ , ! , $ , \#. Included as well are three different recordings of capitalized letters

* Publicly available dataset, available from the [UC Irvine data repository](archive.ics.uci.edu/ml/datasets/Spambase).  More details about the data can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)

* For this problem not all errors are equal; we want to avoid filtering
out good e-mail, while letting spam get through is not desirable but
less serious in its consequences

* We will evaluate predictions in terms of accuracy and __specificity__, i.e.
`$$\frac{\# \mathrm{\,\,correctly\,\,predicted\,\,emails}}{\# \mathrm{\,\,true\,\,emails}}$$`

---


| | george | you | your | hp | free | hpl | ! | our | re | edu | remove |
|--|--|--|--|--|--|--|--|--|--|--|--|
| spam | 0.00 | 2.26 | 1.38 | 0.02 | 0.52 | 0.01 | 0.51 | 0.51 | 0.13 | 0.01 | 0.28 |
| email | 1.27 | 1.27 | 0.44 | 0.90 | 0.07 | 0.43 | 0.11 | 0.18 | 0.42 | 0.29 | 0.01 |

ESL, Table 1.1: Average percentage of words or characters in an email message
equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email.

---


```r
#Load the data and split into training and test sets.
rm(list=ls())
spam &lt;- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test = spam[spam$testid, -2]
n = nrow(train)
m = nrow(test)
```

---


```r
# A function to calculate prediction accuracy and specificity 
score &lt;- function(phat, truth, name="model") {
  ctable &lt;- table(truth=truth,
                  yhat=(phat&gt;0.5))
  accuracy &lt;- sum(diag(ctable))/sum(ctable)
  specificity &lt;- ctable[1,1]/sum(ctable[1,])
  data.frame(model=name, accuracy=accuracy, specificity=specificity)
}
```

---


```r
# null model
phat0 = rep(mean(train$spam=="spam"),m)
yhat0 = ifelse(phat0&gt;.5,"spam","email")
# confusion matrix
table(Predicted=yhat0, True=test$spam)
```

```
         True
Predicted email spam
    email   941  595
```

```r
# score
score(phat0, test$spam, name="null model")
```

```
       model  accuracy specificity
1 null model 0.6126302           1
```

---


```r
library(rpart)
library(rpart.plot)
pnames &lt;- setdiff(colnames(train),"spam")
fml &lt;- as.formula(paste("spam", paste(pnames,collapse=' + '),sep=' ~ '))
fit1 &lt;- rpart(fml, train); rpart.plot(fit1)
```

![](8_BAG_slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---


```r
phat1 = predict(fit1, newdata=test)[,"spam"]
yhat1 = ifelse(phat1&gt;.5,"spam","email")
# confusion matrix
table(pred=yhat1, truth=test$spam)
```

```
       truth
pred    email spam
  email   878   94
  spam     63  501
```

```r
# score 
score(phat1, test$spam, name="tree")
```

```
  model  accuracy specificity
1  tree 0.8977865   0.9330499
```

---


```r
# Obtain B bootstrap samples
set.seed(123)
B &lt;- 50
obs &lt;- sapply(1:B, 
              function(b)
              sample(1:n, size=n, replace=T)
              )
# Train classification trees and return them in a list.
treelist &lt;-lapply(1:B,
                  function(b)
                  rpart(fml, train[obs[,b],])
                  )

# predict.bag (probability)
predict.bag &lt;- function(treelist, newdata) {
  phats &lt;- sapply(1:length(treelist),
                  function(b)
                  predict(treelist[[b]], newdata=newdata)[,"spam"]
                  )
  pbar &lt;- rowMeans(phats)
}
```

---


```r
phat2 = predict.bag(treelist, newdata=test)
yhat2 = ifelse(phat2&gt;.5,"spam","email")
# confusion matrix
table(pred=yhat2, truth=test$spam)
```

```
       truth
pred    email spam
  email   892   95
  spam     49  500
```

```r
# score
score(phat2, test$spam, name="bagging")
```

```
    model accuracy specificity
1 bagging  0.90625   0.9479277
```

---


```r
library(randomForest)
p = ncol(test)
fit &lt;- randomForest(fml, data = train, ntree = B, mtry = p)
# out-of-bag estimate of error
plot(fit)
```

![](8_BAG_slides_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
