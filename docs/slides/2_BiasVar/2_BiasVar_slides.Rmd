---
title: "Bias-variance decomposition"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true     
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=T, R.options=list(width=220),
                      fig.align='center', out.width='75%', fig.asp=.75)
```

# Outline

* Fixed-X setting
* Bias-variance decomposition 
* Simulation
* Bias-variance trade-off


---

# Notation

* Response $Y$
* Predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$
* $(X,Y)$ have some unknown joint distribution
* Model
$$Y = f(X) + \varepsilon$$
where $f(X)=\mathbb{E}(Y|X)$ is the __regression function__ and $\varepsilon$ is the error term independent from $X$ with $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2$
* Observed training set: $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$
* Observed test set: $(x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$ i.i.d. from $(X,Y)$


.pull-left[
Vector of response (training set): 
$$\underset{n\times 1}{\mathbf{y}} = 
\left[
\begin{array}{c}
y_1   \\
\cdots\\
y_i  \\
\cdots\\
y_n \\
\end{array}\right]$$
]

.pull-right[

Design matrix (training set):
$$\underset{n\times p}{\mathbf{X}} = \left[
\begin{array}{cccccc}
x_{11}  & x_{12}  & \cdots   &  x_{1j}  & \cdots   &   x_{1p}  \\
x_{21}  & x_{22} & \cdots   &  x_{2j}  & \cdots   &   x_{2p}  \\
\cdots   & \cdots   &  \cdots & \cdots   &  \cdots  \\
x_{i1}  & x_{i2} & \cdots   &  x_{ij}& \cdots   & x_{ip}    \\
\cdots   & \cdots   &  \cdots  &  \cdots   &  \cdots\\
x_{n1}   & x_{n2} & \cdots   & x_{nj}    &  \cdots   &   x_{np}\\
\end{array}\right]$$

]

---

# Fixed-X setting

This setting combines the following two assumptions about the problem:

1. The covariate values $x_1,\ldots,x_n$ used in training are not random (e.g., designed), and the only randomness in training is due to the responses $y_1,\ldots,y_n$.

2. The covariates $x^*_1,\ldots,x^*_n$ used for prediction exactly match $x_1,\ldots,x_n$, respectively

We assume that for the training set
$$Y_{i}=f(x_i)+\varepsilon_i, \quad i=1,\ldots,n$$
and for the test set
$$Y^*_{i}=f(x_i)+\varepsilon^*_i, \quad i=1,\ldots,n$$
where $f$ is the unknown regression function, 
the predictors $x_1,\ldots,x_n$ are fixed and the errors 
$\varepsilon_1,\ldots,\varepsilon_n$ and $\varepsilon^*_1,\ldots,\varepsilon^*_n$ are i.i.d. $N(0,\sigma^2)$. 

---

# Prediction error

In the fixed-X setting, __prediction error__ $\mathrm{ErrF}$ is defined by
$$\mathrm{ErrF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}})=\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}( Y^*_i - \hat{f}(x_i))^2\right]$$

This is the error that we want to minimize.

---
layout: false
class: inverse, middle, center

# Bias-Variance Decomposition 

---

# Irreducible and reducible error 

* Suppose we want to predict $y^*_i$ by $\hat{y}_i^* = \hat{f}(x_i)$ where $\hat{f}$ is an estimate of $f$ based on the training data.

* The accuracy of $\hat{y}^*_i$ as a prediction for $y^*_i$ depends on two quantities, which we call the __reducible error__ and the __irreducible error__. 

* We have 
$$\mathbb{E}[(Y^*_{i} - \hat{Y}^*_i)^2] = \mathbb{E}[(f(x_i) + \varepsilon^*_i -  \hat{f}(x_i))^2] = \underbrace{\mathbb{E}[(f(x_i)  -  \hat{f}(x_i))^2]}_{\mathrm{Reducible}} + \underbrace{\mathbb{V}\mathrm{ar}(\varepsilon^*_i)}_{\mathrm{Irreducible}}$$
where $\mathbb{V}\mathrm{ar}(\varepsilon^*_i) = \sigma^2$. 



---

# Reducible error


* The reducible error can be decomposed into (squared) __bias__ and __variance__ of the estimator $\hat{f}$, respectively:
$$\mathbb{E}[(f(x_i)  -  \hat{f}(x_i))^2] = \mathbb{E}[(f(x_i)  - \mathbb{E}\hat{f}(x_i) + \mathbb{E}\hat{f}(x_i) - \hat{f}(x_i))^2] =
\underbrace{ [\mathbb{E}\hat{f}(x_i) - f(x_i) ]^2}_{[\mathrm{Bias}(\hat{f}(x_i))]^2} + \underbrace{\mathbb{V}\mathrm{ar}[\hat{f}(x_i)]}_{\mathrm{Variance}(\hat{f}(x_i))}$$

* Now the decomposition of fixed-X prediction error is simply
$$\mathrm{ErrF} =  \sigma^2 + \underbrace{\frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2}_{\mathrm{Bias}^2} + \underbrace{\frac{1}{n}\sum_{i=1}^{n} \mathbb{V}\mathrm{ar}(\hat{f}(x_i))}_{\mathrm{Variance}} = [\mathrm{Bias}(\hat{f})]^2+\mathrm{Variance}(\hat{f})$$

* Bias and variance are conflicting entities, and we cannot
minimize both simultaneously. We must therefore choose a __trade-off__ between bias and variance.

---

# Sources of error

* __Irreducible error__ <br>
Can we ever predict $Y$ from $X$ with zero error? No. Even the true
regression function $f$ cannot do this. Note that, even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\hat{y}^*_i = f(x_i)$, our prediction would still have some error in it. This is because $Y_i^* = f(x_i) + \varepsilon_i^*$

* __Estimation bias__ <br>
What happens if our fitted function $\hat{f}$ belongs to a model class that is far from the true $f$? E.g. we choose to fit a linear model in a setting where the true relationship is far from linear?


* __Estimation variance__ <br>
What happens if our fitted (random) function $\hat{f}$ is itself quite variable? In other words, 
over different copies of the training data, we end up constructing substantially different functions $\hat{f}?$


---
layout: false
class: inverse, middle, center

# Simulation 

---


```{r}
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
plot(x,ftrue, type="l", col=4)
```

---

# Simulation

* Consider the polynomial regression of degree $d$.
We will perform a simulation to compute  $[\mathrm{Bias}(\hat{f})]^2$ and  $\mathrm{Variance}(\hat{f})$


```{r}
# setting
sigmatrue=0.01
n = length(x)
B = 100
# simulation function
{{sim = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
} }}
```

---

# Results (d=3)

.pull-left[

```{r}
d = 3
# results
set.seed(123)
{{yhats = replicate(B,sim(d))}}
matplot(x,yhats, type="l", col="gray", lty=1)
lines(x,ftrue, col=4)
# expected value
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r}
# squared bias
Bias2 = (ftrue - Ehatf)^2
# variance
Var = apply(yhats,1,var)
# plot
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]

---

# Results (d=20)

.pull-left[

```{r}
d = 20
# results
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1)
lines(x,ftrue, col=4)
# expected value
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r}
# squared bias
Bias2 = (ftrue - Ehatf)^2
# variance
Var = apply(yhats,1,var)
# plot
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]


---

# Linear model

* Note that the expected value $\mathbb{E}\hat{f}(x_i)$ can be obtained by fitting the polynomial model using $y_i = f(x_i)$. This allows to compute the squared bias $[\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2$

```{r}
d = 3
# expected value 
Ehatf = fitted(lm(ftrue ~ poly(x,degree=d)))
Bias2 = mean( (ftrue - Ehatf)^2 )
```

* Theoretical results show that for the linear model $$\mathrm{Variance}(\hat{f})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2 p}{n}$$
Importantly, in the linear model the variance is unaffected by the form of $f(x)$. 

```{r}
# true variance
p = d+1
Var = (sigmatrue^2)*p/n
```

* Now we easily compute the decomposition of $\mathrm{ErrF}$ into squared bias and variance of the polynomial regression for all possible degrees from 0 to $n-1$ without using simulations

---

```{r, eval=F}
ds = 1:20
ps = ds+1
Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
  )
Vars = ps*(sigmatrue^2)/n
Reds = Bias2s+Vars 
barplot(Reds, ylab="Prediction error", names.arg=ps)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
# best model size
ps[which.min(Reds)]
```

---

```{r, echo=F}
ds = 1:20
ps = ds+1
Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
  )
Vars = ps*(sigmatrue^2)/n
Reds = Bias2s+Vars 
barplot(Reds, ylab="Reducible error", names.arg=ps)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```

---

```{r}
Irr = rep(sigmatrue^2,length(ps))
ErrFs = Reds + Irr
barplot(ErrFs, ylab="Reducible error", names.arg=ps)
barplot(Irr, add=T, col=1, names.arg=" ")
legend("topright", c("Reducible","Irreducible"), col=c("gray",1), pch=c(19,19))
```


---

# Best model (d=5)

.pull-left[

```{r}
d = 5
# results
set.seed(123)
yhats = replicate(B,sim(d))
matplot(x,yhats, type="l", col="gray", lty=1)
lines(x,ftrue, col=4)
# expected value
Ehatf = apply(yhats,1,mean)
lines(x,Ehatf)
```
]

.pull-right[

```{r}
# squared bias
Bias2 = (ftrue - Ehatf)^2
# variance
Var = apply(yhats,1,var)
# plot
barplot(Bias2+Var, ylab="Bias2 + Var", names.arg=round(x,1))
barplot(Var,add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
```
]


---

```{r}
sigmatrue=0.01
n = length(x)
B = 1000
# simulation function
sim2 = function(d){
y = ftrue + rnorm(n,0,sigmatrue)
fit = lm(y ~ poly(x,degree=d))
yhat = fitted(fit)
ystar = ftrue + rnorm(n,0,sigmatrue)
MSE.te = mean((ystar - yhat)^2)
} 
# MSE.te empirical
d = 5
mean(replicate(B,sim2(d)))
# MSE.te theoretical
Ehatf = fitted(lm(ftrue ~ poly(x,degree=d)))
Bias2 = mean( (ftrue - Ehatf)^2 )
p = d+1
Var = (sigmatrue^2)*p/n
Bias2 + Var + sigmatrue^2
```


---
layout: false
class: inverse, middle, center

# Bias-variance trade-off

---

# Bias-variance trade-off

* Reducible Error = Bias $^2$ + Variance

* Models $\hat{f}$ with low bias tend to have high variance

* Models $\hat{f}$ with low variance tend to have high bias

* We can see that even if our prediction is unbiased, i.e.
$\mathbb{E}\hat{f}(x_i)= f(x_i)$, we can still incur a large error if it is highly
variable 

* On the other hand, $\hat{f}(x_i)=0$ has 0 variance but will be
terribly biased

* To predict well, we need to balance the bias and the variance

---

![](images/tiroasegno.png)

---

# ISLR figures 2.9-2.12 

* [Fig. 2.9](http://www-bcf.usc.edu/~gareth/ISL/Chapter2/2.9.pdf)

* [Fig. 2.10](http://www-bcf.usc.edu/~gareth/ISL/Chapter2/2.10.pdf)

* [Fig. 2.11](http://www-bcf.usc.edu/~gareth/ISL/Chapter2/2.11.pdf)

* [Fig. 2.12](http://www-bcf.usc.edu/~gareth/ISL/Chapter2/2.12.pdf)



