---
title: "Data Mining"
subtitle: "Titanic data"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```


# Outline

* Classification
* Titanic data
    - Missing values
    - EDA and model building
    - Feature engineering

---

layout: false
class: inverse, middle, center

# Classification 

---

# Classification

* Response $Y \in \{0,1\}$

* Predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$

* $(X,Y)$ have some unknown joint distribution

* Note that in this setting the regression function is the conditional probability of $Y=1$ given $x$
$$f(x) = \mathbb{E}(Y|X=x) = \mathrm{Pr}(Y=1|X=x)$$

* The __Bayes' classification rule__ is
$$C(x) = \left\{\begin{array}{ll}
1 & \mathrm{if\,\,} f(x)>1/2 \\
0 & \mathrm{otherwise}\\
\end{array}\right.$$

---

# Bayes error rate

* A __classification rule__ is any function $\hat{C}: x \mapsto \{0,1\}$

* For example, the __plug-in rule__
$$\hat{C}(x) = \left\{\begin{array}{ll}
1 & \mathrm{if\,\,} \hat{f}(x)>1/2 \\
0 & \mathrm{otherwise}\\
\end{array}\right.$$
where $\hat{f}$ is an estimate of $f$ based on training data

* The Bayes classification rule is __optimal__ because it has the smallest error rate:
$$\mathbb{E}\left[\mathrm{Pr}(Y \neq C(x) ) \right]\leq   \mathbb{E}\left[ \mathrm{Pr}(Y \neq \hat{C}(x))\right] \quad \forall \, \hat{C}$$
where the expectation averages the probability over all possible values of
$X$

* The Bayes error rate $\mathbb{E}\left[\mathrm{Pr}(Y \neq C(x) ) \right]$ is analogous to the **irreducible error**

---

# Approximation and estimation errors

* In the classification setting, the concepts of bias and variance generalise to approximation and estimation errors

* Assume $\mathcal{F}$ to be a class of models

* Suppose we've learned a model $\hat{f} \in \mathcal{F}$ learned using some (finite amount of) training data

* Denote by $f^*$ the best possible model in $\mathcal{F}$ assuming infinite amount of training data

* **Approximation error**: Error of best model $f^*$ because of the model class $\mathcal{F}$ being too simple

* **Estimation error**: Error of learned model $\hat{f}$ (relative to $f^*$) because we only had finite training data


$$\mathrm{Reducible\,\,Error} = \underbrace{\mathrm{Err}(f^*) }_{\mathrm{approximation\,\,err.}}+ \underbrace{[\mathrm{Err}(\hat{f})-\mathrm{Err}(f^*)]}_{\mathrm{estimation\,\,err.}}$$


---

# Classification with more than 2 classes


* Response with $K$ classes:  $Y \in \{1,\ldots, K\}$ 

* Conditional class probabilities at $x$:

$$p_k(x)=\mathrm{Pr}(Y=k|X=x), \quad k = 1,\ldots,K$$

*  Then the Bayes optimal classifier at $x$ is

$$C(x)=j \quad \mathrm{if} \quad p_j(x)=\max\{p_1(x),\ldots,p_K(x)\}$$
* The Bayes classifier has the smallest error 
$$1-\mathbb{E}\big(\max_j \Pr(Y=j|X)\big)$$

---


# Methods for classification

* **Discriminative** Seek a model for $\Pr(Y=k|X=x)$
    - Linear regression
    - Logistic regression
    - K-nearest neighbors
    - Support vector classifier 
    - Etc.

* **Generative** Model $\Pr(Y=k|X=x)$ by using the Bayes Theorem
$$\Pr(Y=k|X=x) \propto \Pr(X=x|Y=k) \Pr(Y=k)$$
    - Linear Discriminant Analysis (LDA)
    - Quadratic Discriminant Analysis (QDA)
    - Naive Bayes
    - Etc.

---

# Missclassification rate and accuracy

* Training set: $(x_1, y_1), (x_2,y_2),\ldots, (x_n,y_n)$
* Test set: $(x^*_1, y^*_1), (x^*_2,y^*_2),\ldots, (x^*_m,y^*_m)$


* __Missclassification rate (training set)__

$$\mathrm{Err}_{\mathrm{Tr}} = \frac{1}{n} \sum_{i=1}^{n} I\{y_i \neq \hat{c}(x_i) \}$$

* __Missclassification rate (test set)__
$$\mathrm{Err}_{\mathrm{Te}} = \frac{1}{m} \sum_{i=1}^{m} I\{y^*_i \neq \hat{c}(x^*_i) \}$$

* __Accuracy (test set)__
$$\mathrm{Acc}_{\mathrm{Te}} = 1- \mathrm{Err}_{\mathrm{Te}}$$

---

layout: false
class: inverse, middle, center

# Titanic data


---

# Introduction

On April 15, 1912, during her maiden voyage, the Titanic sank
after colliding with an iceberg, killing 1502 out of 2224
passengers and crew

![](https://vignette.wikia.nocookie.net/foreverknight/images/7/7f/Route_of_Titanic.svg/revision/latest/scale-to-width-down/640?cb=20090303115410)

* Training set of $n = 891$ passengers, each with $p = 10$
predictors

* The goal is to predict a 0 or 1 value for the survived variable
for the $m = 418$ passengers in the test set

---

# References

Read [Varian (2014) Big Data: New Tricks for Econometrics](https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3). In particular 

* Titanic example (section *Classification and Regression Trees*)

* R code (download [Data Set](https://www.aeaweb.org/jep/data/2802/2802-0003_data.zip) in *Additional Materials*)

See the Kaggle competition 
[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic). In particular

* [Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) is a tutorial

* [Tidy TitaRnic
](https://www.kaggle.com/headsortails/tidy-titarnic) provides an extensive EDA

* [Titanic using Name only](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818) gives a nice example of feature engineering

---
```{r}
# import data
PATH <- "/Users/aldosolari/Documents/mygithub/DM/dataset/"
train <- read.csv(paste(PATH,"titanic_tr.csv",sep=""), 
                  header = TRUE, stringsAsFactors = FALSE)
n <- nrow(train)
test <- read.csv(paste(PATH,"titanic_te.csv",sep=""), 
                 header = TRUE, stringsAsFactors = FALSE)
m <- nrow(test)
# combined train and test
combi <- rbind(train, test)
# check type of variables
str(combi)
```

---

# Type of variables

| Name | Description |
|---|----|
| pclass   |       Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) |
| survived    |   Survival (0 = No; 1 = Yes) |
| name      |      Name |
| sex      |       Sex |
| age      |       Age |
| sibsp     |      Number of Siblings/Spouses Aboard |
| parch     |      Number of Parents/Children Aboard |
| ticket    |      Ticket Number |
| fare      |      Passenger Fare |
| cabin     |      Cabin |
| embarked   |     Port of Embarkation  (C = Cherbourg; Q = Queenstown; S = Southampton) |

* See the info file [http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt](http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt)

---

```{r}
# copy of the response as a factor for better readability
combi$survived01 <- combi$survived
combi$survived <- as.factor(combi$survived01)
levels(combi$survived) = c("Death","Alive")
# test survived NA
testsurvived <- combi$survived[(n+1):(n+m)]
combi$survived[(n+1):(n+m)] <- NA
combi$survived01[(n+1):(n+m)] <- NA
# convert pclass, sex, embarked to factors
combi$pclass <- as.factor(combi$pclass)
combi$sex <- as.factor(combi$sex)
combi$embarked <- as.factor(combi$embarked)
```

---

layout: false
class: inverse, middle, center

# Missing values

---

# Where are the missing values?

```{r}
# cabin has missing values coded as "" instead of NA
combi$cabin[combi$cabin==""] <- NA
# where are the missing values?
summary(combi)
```

---

```{r}
library(VIM)
aggr(combi[,-c(2,12)], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```

---

```{r, echo=F}
aggr(combi[,-c(2,12)], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE, plot=F)
```

---

# Fare

```{r}
# 1 missing value in fare
combi[which(is.na(combi$fare)), ]
```

---

# Imputing missing values

```{r}
aggregate(fare ~ pclass + embarked, combi, FUN=median)
combi$fare[which(is.na(combi$fare))] <- 8.0500
```

---

# Embarked

```{r}
# 2 missing in embarked
combi[which(is.na(combi$embarked)), ]
```

---

# Ticket

![](https://upload.wikimedia.org/wikipedia/commons/a/ad/Ticket_for_the_Titanic%2C_1912.jpg)


---

# Ticket frequency

```{r}
# different people have the same ticket number
head(table(combi$ticket))
# number of people who share the same ticket
table(table(combi$ticket))
# ticket frequency
combi$ticketFreq <- ave(1:nrow(combi),combi$ticket,FUN=length)
```

---

# Ticket price

```{r}
combi$price <- combi$fare/combi$ticketFreq
```

```{r, echo=F}
plot(fare ~ ticketFreq, combi, subset=pclass==3)
points(price ~ ticketFreq, combi, subset=pclass==3, col=2)
```

---

```{r}
boxplot(fare ~ pclass + embarked + sex, data=combi, subset=pclass==1 & sex=="female"); abline(h=80)
combi$embarked[which(is.na(combi$embarked))] <- c("C","C")
```

---

# Age

```{r}
# Age by pclass and sex
aggregate(age ~ pclass + sex, combi, FUN=mean)
# Model age as a function of pclass and sex
fit.age <- lm(age ~ sex*pclass, 
          data = combi[!is.na(combi$age),])
# Imputation of age missing values
combi$age[is.na(combi$age)] <- predict(fit.age, 
          newdata=combi[is.na(combi$age),])
```

---

# Back to train and test 

```{r}
train <- combi[1:n,]
test <- combi[(n+1):(n+m),]
```

---

layout: false
class: inverse, middle, center

# EDA and model building

---

# Null model 

* Training set: 38.38% of passengers survived, 61.62% died
* The null model uses $y$ only and predicts 'all-dead'

```{r}
round(mean(train$survived01),2)
yhat <- rep("Death",m)
# confusion matrix
table(Predicted=yhat, True=testsurvived)
# accuracy
mean(yhat == testsurvived)
```

---

# Gender

Women first?

```{r}
plot(survived ~ sex, train)
```

---

```{r}
# gender-only logistic model
fit <- glm(survived ~ sex, train, family="binomial")
phat <- predict(fit, newdata=test, type = "response")
yhat <- ifelse(phat > 0.5, "Alive","Death")
# confusion matrix 
table(Predicted=yhat, True=testsurvived)
# accuracy 
mean(yhat == testsurvived)
```

---

# Age

Children first? What is the relationship between age and survival?

```{r}
summary(glm(survived ~ age, train, family="binomial"))$ coefficients 
```

---

What about this plot?

```{r, echo=FALSE}
ageclass = cut(train$age, breaks = c(0,10,20,30,40,50,60,70,80))
barplot(prop.table(table(train$survived,ageclass),2), xlab="Age class")
```

---

A better visualization

```{r}
plot(survived ~ age, train)
```

---

# Passenger class

Rich people survived at a higher rate?

```{r}
plot(survived~pclass, train)
```

---

```{r}
# pclass-only logistic model
fit <- glm(survived ~ pclass, train, family="binomial")
phat <- predict(fit, newdata=test, type = "response")
yhat <- ifelse(phat > 0.5, "Alive","Death")
# confusion matrix 
table(Predicted=yhat, True=testsurvived)
# accuracy 
mean(yhat == testsurvived)
```

---

# Age and pclass

What about age and pclass combined?

```{r, echo=F}
class.jitter <- as.numeric(train$pclass)+.7*runif(n)
plot(age ~ class.jitter,xlim=c(.95,3.8),cex=1, xlab="Passenger class",xaxt="n", train)
axis(side=1,at=c(1.4,2.4,3.4),label=c("1st","2nd","3rd"))
points(age[survived01==0] ~ class.jitter[survived01==0],pch=19, train)
```

---

# Classification tree 

Binary recursive partitioning of the sample space into smaller rectangles

```{r, echo=F}
class.jitter <- as.numeric(train$pclass)+.7*runif(n)
plot(age ~ class.jitter,xlim=c(.95,3.8),cex=1, xlab="Passenger class",xaxt="n", train)
axis(side=1,at=c(1.4,2.4,3.4),label=c("1st","2nd","3rd"))
points(age[survived01==0] ~ class.jitter[survived01==0],pch=19, train)
abline(v=2.85)
rect(1.85,18,4.0,89,col=rgb(0.5,0.5,0.5,1/4))
rect(2.85,-5,4.0,89,col=rgb(0.5,0.5,0.5,1/4))
```


---

# Decision rule

```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(survived ~ pclass + age, train, control=rpart.control(maxdepth =  3))
```

.pull-left[
```{r}
rpart.plot(fit, type=0, extra=2)
```
]

.pull-right[
| Status | Pr(Death) | Prediction |
|-----------|-----------|------|
| Class 3 |  76% | Death |
| Class 1-2, younger than 18 |  9% | Alive |
| Class 2, older than 18 |  56% | Death |
| Class 1, older than 18 |  39% | Alive | 
]

---


```{r}
yhat <- predict(fit, newdata=test, type="class")
# confusion matrix
table(Predicted=yhat, True=testsurvived)
# accuracy
mean(yhat == testsurvived)
```

---

# Gender and pclass

.pull-left[
```{r}
plot(survived~pclass, train, subset=sex=="male", main="male")
```
]

.pull-right[
```{r}
plot(survived~pclass, train, subset=sex=="female", main="female")
```
]

---

# Gender and age

.pull-left[
```{r}
plot(survived~age, train, subset=sex=="male", main="male")
```
]

.pull-right[
```{r}
plot(survived~age, train, subset=sex=="female", main="female")
```
]


---

# Performance

Logistic model

| Predictors | Acc.Tr | Acc.Te |
|------------|-------|--------|
| -  |   61.6% |  62.2% |
| age | 61.6% | 62.2% |
| pclass | 67.9% | 67.2% |
| sex | 78.7% | 76.6% |
| age + pclass | 69.1% | 65.3% |
| age + sex | 78.7% | 76.6% |
| pclass + sex | 78.7% | 76.5% |
| age + pclass + sex | 79.5% | 75.6% |

---

layout: false
class: inverse, middle, center

# Feature engineering

---

# Title

```{r}
combi$name[1]
library(dplyr)
library(stringr)
combi <- combi %>% 
     mutate(title = str_extract(name, "[a-zA-Z]+\\."))
table(combi$title)
combi$title <- factor(combi$title)
```

---

```{r}
plot(survived ~ title, combi[1:n,])
```

---

# Man, boy or woman?

```{r}
combi$wbm <- "man"
combi$wbm[grep('Master',combi$name)] <- "boy"
combi$wbm[combi$sex=="female"] <- "woman"
combi$wbm <- factor(combi$wbm)
```

---

```{r}
plot(survived ~ wbm, combi[1:n,])
```


---

# Cabin

```{r}
# the first character of cabin is the deck
table(substr(combi$cabin, 1, 1))
```

---

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Titanic_cutaway_diagram.png/322px-Titanic_cutaway_diagram.png)

---

# Family size

```{r}
combi$familysize <- combi$sibsp + combi$parch + 1
plot(survived ~ familysize, combi[1:n,])
```

---

# SurnameFreq

```{r}
combi$surname <- substring(combi$name,0,regexpr(",",combi$name)-1)
combi$surnameFreq <- ave(1:nrow(combi),combi$surname,FUN=length)
plot(survived ~ surnameFreq, combi[1:n,])
```


---

# Surname survival

```{r}
train = combi[1:n,]
plot(survived ~ as.factor(surname), train[train$surnameFreq==5,])
```

---

# Gender surname model

| Rule | Prediction |
|---|----|
| Man | Death |
| Boy and all females and boys in his family live | Alive |
| Boy and not all females and boys in his family live | Death |
| Female and all females and boys in her family die | Death |
| Female and not all females and boys in her family die | Alive |

---

# Models comparison

| Model | Acc.Tr | Acc.Te |
|---|----|----|
| All-dead | 61.6% |  62.2% |
| Gender-only | 78.7% | 76.6% |
| Gender surname | 85.5% | 80.1% |


How I'm doing with my score? See Oscar Takeshita's [post](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)

---

# Public scores

* 0.62679 All-dead submission:  The simplest strategy of guessing that all died since the majority died.
* [0.65550](https://www.kaggle.com/pliptor/optimal-titanic-for-pclass-only-0-65550) Passenger-class only.
* [0.675](https://www.kaggle.com/ccastleberry/titanic-cabin-features/notebook) Cabin-only:  This is a remarkable notebook by [CalebCastleberry](https://www.kaggle.com/ccastleberry).
* [0.71291](https://www.kaggle.com/pliptor/titanic-ticket-only-study) Ticket-only using KNN.
* [0.76555](https://www.kaggle.com/pliptor/optimal-titanic-for-gender-only-0-7655) Gender-only.
* 0.76555 Gender + Passenger-class (Identical as Gender-only).
* [0.77990](https://www.kaggle.com/pliptor/minimalistic-xgb) Gender + Class + Embarked  XGBoost in R.
* [0.77990](https://www.kaggle.com/pliptor/minimalistic-titanic-in-python-lightgbm) Gender + Class + Embarked  LightGBM in Python.
* [0.77990](https://www.kaggle.com/wayne999/a-simple-genetic-programming/code) Using genetic programming by [Yan Wang](https://www.kaggle.com/wayne999) Short and neat complete genetic programming code (it is suboptimal since the Name feature is not used).
* [0.78468](https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot) Name-only text vectorization and PCA with a 3D interactive plot.
* [0.78947](https://www.kaggle.com/pliptor/titanic-in-python-svm) Gender + Class + Embarked + Age using SVM
* [0.78947](https://www.kaggle.com/yildirimarda/decision-tree-visualization-submission) Gender + Class + FamilySize + Age using Decision Tree by [Arda Yildirim](https://www.kaggle.com/yildirimarda)
* [0.79904](https://www.kaggle.com/rafaelvleite/titanic-artificial-neural-network-80-score) Neural network (keras)  by [Rafael Vicente Leite](https://www.kaggle.com/rafaelvleite).
* [0.79904](https://www.kaggle.com/frederikh/12-500-feet-under-the-sea) Using ethnicity feature by [Frederik Hasecke](https://www.kaggle.com/frederikh).
* [0.80382](https://www.kaggle.com/bwboerman/a-quick-dive-into-h2o-with-python) Using H2O by [Bart Boerman](https://www.kaggle.com/bwboerman)
* [0.80861](https://www.kaggle.com/arthurtok/0-808-with-simple-stacking/output) Simple stacking by [Anisotropic](https://www.kaggle.com/arthurtok) stacking is ubiquitous in competitions.

---

# Public scores (cont'd)

* [0.80861](https://www.kaggle.com/nicapotato/titanic-voting-pipeline-stack-and-guide) Voting/ensembling  by [Nick Brooks](https://www.kaggle.com/nicapotato) An impressive number of models is packed in almost one hour of running time!
* [0.80861](https://www.kaggle.com/chucktalbert/titanic-a-tidy-caret-approach-0-8086) Tidy + Caret, KNN impute and Random Forests optimizer by [Chuck Talbert](https://www.kaggle.com/chucktalbert)
* [0.80861](https://www.kaggle.com/reisel/iterative-prediction-of-survival) Iterative Prediction of Survival by [Reinhard](https://www.kaggle.com/reisel)
* [0.80861](https://www.kaggle.com/njmei42/kaggle-titanic-with-tensorflow/notebook) Kaggle Titanic with Tensorflow by [nme-42](https://www.kaggle.com/njmei42) It is quite an interesting kernel.
* [0.81339](https://www.kaggle.com/jack89roberts/titanic-using-ticket-groupings) Titanic Using Ticket Grouping by [Jack Roberts](https://www.kaggle.com/jack89roberts). 
* [0.81339](https://www.kaggle.com/c/titanic/discussion/6821) Random Forests: A great tutorial by [Trevor Stephens](https://www.kaggle.com/trevorstephens)
* [0.82296](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296) Divide and Conquer. 
* [0.82296](https://www.kaggle.com/bisaria/titanic-lasso-ridge-implementation) Lasso Ridge by [Bisaria](https://www.kaggle.com/bisaria) 
* [0.82296](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818) Titanic using Name only [Chris Deotte](https://www.kaggle.com/cdeotte) achieves this great score using nothing but the Name feature! (Note from his kernel: 0.81818 with Name only and 0.82296 by adding Ticket). 
* [0.82296](https://www.kaggle.com/cdeotte/titanic-deep-net-0-82296) Titanic Deep Net by [Chris Deotte](https://www.kaggle.com/cdeotte)] 
* [0.82775](https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster) [Frank Sylla](https://www.kaggle.com/francksylla) engineers
several features.
* [0.83253](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83/notebook) [Konstantin](https://www.kaggle.com/konstantinmasich) brings
attention to feature scaling, which is essential when working with the kNN algorithm.
* [0.84210](https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210) Titanic Mega Model [Chris Deotte](https://www.kaggle.com/cdeotte) ensembles Kaggle's top 6 models. It starts with a neat ensembling diagram.
* [0.84688](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688) Titanic WCG+XGBoost [Chris Deotte](https://www.kaggle.com/cdeotte) Is this the ultimate Titanic model? 

