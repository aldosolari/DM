<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Titanic data
### Aldo Solari

---





# Outline

* Classification
* Titanic data
    - Missing values
    - EDA and model building
    - Feature engineering

---

layout: false
class: inverse, middle, center

# Classification 

---

# Classification

* Response `\(Y \in \{0,1\}\)`

* Predictors `\(X=(X_1,\ldots,X_p)^\mathsf{T}\)`

* `\((X,Y)\)` have some unknown joint distribution

* Note that in this setting the regression function is the conditional probability of `\(Y=1\)` given `\(x\)`
`$$f(x) = \mathbb{E}(Y|X=x) = \mathrm{Pr}(Y=1|X=x)$$`

* The __Bayes' classification rule__ is
`$$C(x) = \left\{\begin{array}{ll}
1 &amp; \mathrm{if\,\,} f(x)&gt;1/2 \\
0 &amp; \mathrm{otherwise}\\
\end{array}\right.$$`

---

# Bayes error rate

* A __classification rule__ is any function `\(\hat{C}: x \mapsto \{0,1\}\)`

* For example, the __plug-in rule__
`$$\hat{C}(x) = \left\{\begin{array}{ll}
1 &amp; \mathrm{if\,\,} \hat{f}(x)&gt;1/2 \\
0 &amp; \mathrm{otherwise}\\
\end{array}\right.$$`
where `\(\hat{f}\)` is an estimate of `\(f\)` based on training data

* The Bayes classification rule is __optimal__ because it has the smallest error rate:
`$$\mathbb{E}\left[\mathrm{Pr}(Y \neq C(x) ) \right]\leq   \mathbb{E}\left[ \mathrm{Pr}(Y \neq \hat{C}(x))\right] \quad \forall \, \hat{C}$$`
where the expectation averages the probability over all possible values of
`\(X\)`

* The Bayes error rate `\(\mathbb{E}\left[\mathrm{Pr}(Y \neq C(x) ) \right]\)` is analogous to the **irreducible error**

---

# Approximation and estimation errors

* In the classification setting, the concepts of bias and variance generalise to approximation and estimation errors

* Assume `\(\mathcal{F}\)` to be a class of models

* Suppose we've learned a model `\(\hat{f} \in \mathcal{F}\)` learned using some (finite amount of) training data

* Denote by `\(f^*\)` the best possible model in `\(\mathcal{F}\)` assuming infinite amount of training data

* **Approximation error**: Error of best model `\(f^*\)` because of the model class `\(\mathcal{F}\)` being too simple

* **Estimation error**: Error of learned model `\(\hat{f}\)` (relative to `\(f^*\)`) because we only had finite training data


`$$\mathrm{Reducible\,\,Error} = \underbrace{\mathrm{Err}(f^*) }_{\mathrm{approximation\,\,err.}}+ \underbrace{[\mathrm{Err}(\hat{f})-\mathrm{Err}(f^*)]}_{\mathrm{estimation\,\,err.}}$$`


---

# Classification with more than 2 classes


* Response with `\(K\)` classes:  `\(Y \in \{1,\ldots, K\}\)` 

* Conditional class probabilities at `\(x\)`:

`$$p_k(x)=\mathrm{Pr}(Y=k|X=x), \quad k = 1,\ldots,K$$`

*  Then the Bayes optimal classifier at `\(x\)` is

`$$C(x)=j \quad \mathrm{if} \quad p_j(x)=\max\{p_1(x),\ldots,p_K(x)\}$$`
* The Bayes classifier has the smallest error 
`$$1-\mathbb{E}\big(\max_j \Pr(Y=j|X)\big)$$`

---


# Methods for classification

* **Discriminative** Seek a model for `\(\Pr(Y=k|X=x)\)`
    - Linear regression
    - Logistic regression
    - K-nearest neighbors
    - Support vector classifier 
    - Etc.

* **Generative** Model `\(\Pr(Y=k|X=x)\)` by using the Bayes Theorem
`$$\Pr(Y=k|X=x) \propto \Pr(X=x|Y=k) \Pr(Y=k)$$`
    - Linear Discriminant Analysis (LDA)
    - Quadratic Discriminant Analysis (QDA)
    - Naive Bayes
    - Etc.

---

# Missclassification rate and accuracy

* Training set: `\((x_1, y_1), (x_2,y_2),\ldots, (x_n,y_n)\)`
* Test set: `\((x^*_1, y^*_1), (x^*_2,y^*_2),\ldots, (x^*_m,y^*_m)\)`


* __Missclassification rate (training set)__

`$$\mathrm{Err}_{\mathrm{Tr}} = \frac{1}{n} \sum_{i=1}^{n} I\{y_i \neq \hat{c}(x_i) \}$$`

* __Missclassification rate (test set)__
`$$\mathrm{Err}_{\mathrm{Te}} = \frac{1}{m} \sum_{i=1}^{m} I\{y^*_i \neq \hat{c}(x^*_i) \}$$`

* __Accuracy (test set)__
`$$\mathrm{Acc}_{\mathrm{Te}} = 1- \mathrm{Err}_{\mathrm{Te}}$$`

---

layout: false
class: inverse, middle, center

# Titanic data


---

# Introduction

On April 15, 1912, during her maiden voyage, the Titanic sank
after colliding with an iceberg, killing 1502 out of 2224
passengers and crew

![](https://vignette.wikia.nocookie.net/foreverknight/images/7/7f/Route_of_Titanic.svg/revision/latest/scale-to-width-down/640?cb=20090303115410)

* Training set of `\(n = 891\)` passengers, each with `\(p = 10\)`
predictors

* The goal is to predict a 0 or 1 value for the survived variable
for the `\(m = 418\)` passengers in the test set

---

# References

Read [Varian (2014) Big Data: New Tricks for Econometrics](https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3). In particular 

* Titanic example (section *Classification and Regression Trees*)

* R code (download [Data Set](https://www.aeaweb.org/jep/data/2802/2802-0003_data.zip) in *Additional Materials*)

See the Kaggle competition 
[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic). In particular

* [Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) is a tutorial

* [Tidy TitaRnic
](https://www.kaggle.com/headsortails/tidy-titarnic) provides an extensive EDA

* [Titanic using Name only](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818) gives a nice example of feature engineering

---

```r
# import data
PATH &lt;- "/Users/aldosolari/Documents/mygithub/DM/dataset/"
train &lt;- read.csv(paste(PATH,"titanic_tr.csv",sep=""), 
                  header = TRUE, stringsAsFactors = FALSE)
n &lt;- nrow(train)
test &lt;- read.csv(paste(PATH,"titanic_te.csv",sep=""), 
                 header = TRUE, stringsAsFactors = FALSE)
m &lt;- nrow(test)
# combined train and test
combi &lt;- rbind(train, test)
# check type of variables
str(combi)
```

```
'data.frame':	1309 obs. of  11 variables:
 $ pclass  : int  3 1 3 1 3 3 1 3 3 2 ...
 $ survived: int  0 1 1 1 0 0 0 0 1 1 ...
 $ name    : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
 $ sex     : chr  "male" "female" "female" "female" ...
 $ age     : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ sibsp   : int  1 1 0 1 0 0 0 3 0 1 ...
 $ parch   : int  0 0 0 0 0 0 0 1 2 0 ...
 $ ticket  : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" "113803" ...
 $ fare    : num  7.25 71.28 7.92 53.1 8.05 ...
 $ cabin   : chr  "" "C85" "" "C123" ...
 $ embarked: chr  "S" "C" "S" "S" ...
```

---

# Type of variables

| Name | Description |
|---|----|
| pclass   |       Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) |
| survived    |   Survival (0 = No; 1 = Yes) |
| name      |      Name |
| sex      |       Sex |
| age      |       Age |
| sibsp     |      Number of Siblings/Spouses Aboard |
| parch     |      Number of Parents/Children Aboard |
| ticket    |      Ticket Number |
| fare      |      Passenger Fare |
| cabin     |      Cabin |
| embarked   |     Port of Embarkation  (C = Cherbourg; Q = Queenstown; S = Southampton) |

* See the info file [http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt](http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt)

---


```r
# copy of the response as a factor for better readability
combi$survived01 &lt;- combi$survived
combi$survived &lt;- as.factor(combi$survived01)
levels(combi$survived) = c("Death","Alive")
# test survived NA
testsurvived &lt;- combi$survived[(n+1):(n+m)]
combi$survived[(n+1):(n+m)] &lt;- NA
combi$survived01[(n+1):(n+m)] &lt;- NA
# convert pclass, sex, embarked to factors
combi$pclass &lt;- as.factor(combi$pclass)
combi$sex &lt;- as.factor(combi$sex)
combi$embarked &lt;- as.factor(combi$embarked)
```

---

layout: false
class: inverse, middle, center

# Missing values

---

# Where are the missing values?


```r
# cabin has missing values coded as "" instead of NA
combi$cabin[combi$cabin==""] &lt;- NA
# where are the missing values?
summary(combi)
```

```
 pclass   survived       name               sex           age       
 1:323   Death:549   Length:1309        female:466   Min.   : 0.17  
 2:277   Alive:342   Class :character   male  :843   1st Qu.:21.00  
 3:709   NA's :418   Mode  :character                Median :28.00  
                                                     Mean   :29.88  
                                                     3rd Qu.:39.00  
                                                     Max.   :80.00  
                                                     NA's   :263    
     sibsp            parch          ticket               fare        
 Min.   :0.0000   Min.   :0.000   Length:1309        Min.   :  0.000  
 1st Qu.:0.0000   1st Qu.:0.000   Class :character   1st Qu.:  7.896  
 Median :0.0000   Median :0.000   Mode  :character   Median : 14.454  
 Mean   :0.4989   Mean   :0.385                      Mean   : 33.295  
 3rd Qu.:1.0000   3rd Qu.:0.000                      3rd Qu.: 31.275  
 Max.   :8.0000   Max.   :9.000                      Max.   :512.329  
                                                     NA's   :1        
    cabin           embarked     survived01    
 Length:1309        C   :270   Min.   :0.0000  
 Class :character   Q   :123   1st Qu.:0.0000  
 Mode  :character   S   :914   Median :0.0000  
                    NA's:  2   Mean   :0.3838  
                               3rd Qu.:1.0000  
                               Max.   :1.0000  
                               NA's   :418     
```

---


```r
library(VIM)
aggr(combi[,-c(2,12)], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```

![](8_Titanic_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

```

 Variables sorted by number of missings: 
 Variable Count
    cabin  1014
      age   263
 embarked     2
     fare     1
   pclass     0
     name     0
      sex     0
    sibsp     0
    parch     0
   ticket     0
```

---


```

 Missings in variables:
 Variable Count
      age   263
     fare     1
    cabin  1014
 embarked     2
```

---

# Fare


```r
# 1 missing value in fare
combi[which(is.na(combi$fare)), ]
```

```
     pclass survived               name  sex  age sibsp parch ticket fare
1282      3     &lt;NA&gt; Storey, Mr. Thomas male 60.5     0     0   3701   NA
     cabin embarked survived01
1282  &lt;NA&gt;        S         NA
```

---

# Imputing missing values


```r
aggregate(fare ~ pclass + embarked, combi, FUN=median)
```

```
  pclass embarked    fare
1      1        C 76.7292
2      2        C 15.3146
3      3        C  7.8958
4      1        Q 90.0000
5      2        Q 12.3500
6      3        Q  7.7500
7      1        S 52.0000
8      2        S 15.3750
9      3        S  8.0500
```

```r
combi$fare[which(is.na(combi$fare))] &lt;- 8.0500
```

---

# Embarked


```r
# 2 missing in embarked
combi[which(is.na(combi$embarked)), ]
```

```
    pclass survived                                      name    sex age
62       1    Alive                       Icard, Miss. Amelie female  38
830      1    Alive Stone, Mrs. George Nelson (Martha Evelyn) female  62
    sibsp parch ticket fare cabin embarked survived01
62      0     0 113572   80   B28     &lt;NA&gt;          1
830     0     0 113572   80   B28     &lt;NA&gt;          1
```

---

# Ticket

![](https://upload.wikimedia.org/wikipedia/commons/a/ad/Ticket_for_the_Titanic%2C_1912.jpg)


---

# Ticket frequency


```r
# different people have the same ticket number
head(table(combi$ticket))
```

```

110152 110413 110465 110469 110489 110564 
     3      3      2      1      1      1 
```

```r
# number of people who share the same ticket
table(table(combi$ticket))
```

```

  1   2   3   4   5   6   7   8  11 
713 132  49  16   7   4   5   2   1 
```

```r
# ticket frequency
combi$ticketFreq &lt;- ave(1:nrow(combi),combi$ticket,FUN=length)
```

---

# Ticket price


```r
combi$price &lt;- combi$fare/combi$ticketFreq
```

![](8_Titanic_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---


```r
boxplot(fare ~ pclass + embarked + sex, data=combi, subset=pclass==1 &amp; sex=="female"); abline(h=80)
```

![](8_Titanic_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

```r
combi$embarked[which(is.na(combi$embarked))] &lt;- c("C","C")
```

---

# Age


```r
# Age by pclass and sex
aggregate(age ~ pclass + sex, combi, FUN=mean)
```

```
  pclass    sex      age
1      1 female 37.03759
2      2 female 27.49922
3      3 female 22.18533
4      1   male 41.02927
5      2   male 30.81538
6      3   male 25.96226
```

```r
# Model age as a function of pclass and sex
fit.age &lt;- lm(age ~ sex + pclass, 
          data = combi[!is.na(combi$age),])
# Imputation of age missing values
combi$age[is.na(combi$age)] &lt;- predict(fit.age, 
          newdata=combi[is.na(combi$age),])
```

---

# Back to train and test 


```r
train &lt;- combi[1:n,]
test &lt;- combi[(n+1):(n+m),]
```

---

layout: false
class: inverse, middle, center

# EDA and model building

---

# Null model 

* Training set: 38.38% of passengers survived, 61.62% died
* The null model uses `\(y\)` only and predicts 'all-dead'


```r
round(mean(train$survived01),2)
```

```
[1] 0.38
```

```r
yhat &lt;- rep("Death",m)
# confusion matrix
table(Predicted=yhat, True=testsurvived)
```

```
         True
Predicted Death Alive
    Death   260   158
```

```r
# accuracy
mean(yhat == testsurvived)
```

```
[1] 0.6220096
```

---

# Gender

Women first?


```r
plot(survived ~ sex, train)
```

![](8_Titanic_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---


```r
# gender-only logistic model
fit &lt;- glm(survived ~ sex, train, family="binomial")
phat &lt;- predict(fit, newdata=test, type = "response")
yhat &lt;- ifelse(phat &gt; 0.5, "Alive","Death")
# confusion matrix 
table(Predicted=yhat, True=testsurvived)
```

```
         True
Predicted Death Alive
    Alive    46   106
    Death   214    52
```

```r
# accuracy 
mean(yhat == testsurvived)
```

```
[1] 0.7655502
```

---

# Age

Children first? What is the relationship between age and survival?


```r
summary(glm(survived ~ age, train, family="binomial"))$ coefficients 
```

```
                Estimate  Std. Error   z value   Pr(&gt;|z|)
(Intercept) -0.194344575 0.166959581 -1.164022 0.24441522
age         -0.009590806 0.005264725 -1.821711 0.06849887
```

---

What about this plot?

![](8_Titanic_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---

A better visualization


```r
plot(survived ~ age, train)
```

![](8_Titanic_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---

# Passenger class

Rich people survived at a higher rate?


```r
plot(survived~pclass, train)
```

![](8_Titanic_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---


```r
# pclass-only logistic model
fit &lt;- glm(survived ~ pclass, train, family="binomial")
phat &lt;- predict(fit, newdata=test, type = "response")
yhat &lt;- ifelse(phat &gt; 0.5, "Alive","Death")
# confusion matrix 
table(Predicted=yhat, True=testsurvived)
```

```
         True
Predicted Death Alive
    Alive    43    64
    Death   217    94
```

```r
# accuracy 
mean(yhat == testsurvived)
```

```
[1] 0.6722488
```

---

# Age and pclass

What about age and pclass combined?

![](8_Titanic_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;

---

# Classification tree 

Binary recursive partitioning of the sample space into smaller and smaller rectangles

![](8_Titanic_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;


---

# Decision rule


```r
library(rpart)
library(rpart.plot)
fit &lt;- rpart(survived ~ pclass + age, train, control=rpart.control(maxdepth =  3))
```

.pull-left[

```r
rpart.plot(fit, type=0, extra=2)
```

![](8_Titanic_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;
]

.pull-right[
| Status | Pr(Death) | Prediction |
|-----------|-----------|------|
| Class 3 |  76% | Death |
| Class 1-2, younger than 18 |  9% | Alive |
| Class 2, older than 18 |  56% | Death |
| Class 1, older than 18 |  39% | Alive | 
]

---



```r
yhat &lt;- predict(fit, newdata=test, type="class")
# confusion matrix
table(Predicted=yhat, True=testsurvived)
```

```
         True
Predicted Death Alive
    Death   215    86
    Alive    45    72
```

```r
# accuracy
mean(yhat == testsurvived)
```

```
[1] 0.6866029
```

---

# Gender and pclass

.pull-left[

```r
plot(survived~pclass, train, subset=sex=="male", main="male")
```

![](8_Titanic_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
plot(survived~pclass, train, subset=sex=="female", main="female")
```

![](8_Titanic_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

---

# Gender and age

.pull-left[

```r
plot(survived~age, train, subset=sex=="male", main="male")
```

![](8_Titanic_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
plot(survived~age, train, subset=sex=="female", main="female")
```

![](8_Titanic_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
]


---

# Performance

Logistic model

| Predictors | Acc.Tr | Acc.Te |
|------------|-------|--------|
| -  |   61.6% |  62.2% |
| age | 61.6% | 62.2% |
| pclass | 67.9% | 67.2% |
| sex | 78.7% | 76.6% |
| age + pclass | 70.9% | 67.2% |
| age + sex | 78.7% | 76.6% |
| pclass + sex | 78.7% | 76.5% |
| age + pclass + sex | 80.2% | 76.6% |

---

layout: false
class: inverse, middle, center

# Feature engineering

---

# Title


```r
combi$name[1]
```

```
[1] "Braund, Mr. Owen Harris"
```

```r
library(dplyr)
library(stringr)
combi &lt;- combi %&gt;% 
     mutate(title = str_extract(name, "[a-zA-Z]+\\."))
table(combi$title)
```

```

    Capt.      Col. Countess.      Don.     Dona.       Dr. Jonkheer. 
        1         4         1         1         1         8         1 
    Lady.    Major.   Master.     Miss.     Mlle.      Mme.       Mr. 
        1         2        61       260         2         1       757 
     Mrs.       Ms.      Rev.      Sir. 
      197         2         8         1 
```

```r
combi$title &lt;- factor(combi$title)
```

---


```r
plot(survived ~ title, combi[1:n,])
```

![](8_Titanic_files/figure-html/unnamed-chunk-33-1.png)&lt;!-- --&gt;

---

# Man, boy or woman?


```r
combi$wbm &lt;- "man"
combi$wbm[grep('Master',combi$name)] &lt;- "boy"
combi$wbm[combi$sex=="female"] &lt;- "woman"
combi$wbm &lt;- factor(combi$wbm)
```

---


```r
plot(survived ~ wbm, combi[1:n,])
```

![](8_Titanic_files/figure-html/unnamed-chunk-35-1.png)&lt;!-- --&gt;


---

# Cabin


```r
# the first character of cabin is the deck
table(substr(combi$cabin, 1, 1))
```

```

 A  B  C  D  E  F  G  T 
22 65 94 46 41 21  5  1 
```

---

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Titanic_cutaway_diagram.png/322px-Titanic_cutaway_diagram.png)

---

# Family size


```r
combi$familysize &lt;- combi$sibsp + combi$parch + 1
plot(survived ~ familysize, combi[1:n,])
```

![](8_Titanic_files/figure-html/unnamed-chunk-37-1.png)&lt;!-- --&gt;

---

# Surname


```r
combi$surname &lt;- substring(combi$name,0,regexpr(",",combi$name)-1)
combi$surnameFreq &lt;- ave(1:nrow(combi),combi$surname,FUN=length)
plot(survived ~ surnameFreq, combi[1:n,])
```

![](8_Titanic_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;


---

# Same surname


```r
plot(survived ~ as.factor(surname), combi[1:n &amp; combi$surnameFreq==5,])
```

![](8_Titanic_files/figure-html/unnamed-chunk-39-1.png)&lt;!-- --&gt;

---

# Gender surname model

| Rule | Prediction |
|---|----|
| Man | Death |
| Boy and all females and boys in his family live | Alive |
| Boy and not all females and boys in his family live | Death |
| Female and all females and boys in her family die | Death |
| Female and not all females and boys in her family die | Alive |

---

# Models comparison

| Model | Acc.Tr | Acc.Te |
|---|----|----|
| All-dead | 61.6% |  62.2% |
| Gender-only | 78.7% | 76.6% |
| Gender surname | 85.5% | 80.1% |


How I'm doing with my score? See Oscar Takeshita's [post](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)

---

# Public scores

* 0.62679 All-dead submission:  The simplest strategy of guessing that all died since the majority died.
* [0.65550](https://www.kaggle.com/pliptor/optimal-titanic-for-pclass-only-0-65550) Passenger-class only.
* [0.675](https://www.kaggle.com/ccastleberry/titanic-cabin-features/notebook) Cabin-only:  This is a remarkable notebook by [CalebCastleberry](https://www.kaggle.com/ccastleberry).
* [0.71291](https://www.kaggle.com/pliptor/titanic-ticket-only-study) Ticket-only using KNN.
* [0.76555](https://www.kaggle.com/pliptor/optimal-titanic-for-gender-only-0-7655) Gender-only.
* 0.76555 Gender + Passenger-class (Identical as Gender-only).
* [0.77990](https://www.kaggle.com/pliptor/minimalistic-xgb) Gender + Class + Embarked  XGBoost in R.
* [0.77990](https://www.kaggle.com/pliptor/minimalistic-titanic-in-python-lightgbm) Gender + Class + Embarked  LightGBM in Python.
* [0.77990](https://www.kaggle.com/wayne999/a-simple-genetic-programming/code) Using genetic programming by [Yan Wang](https://www.kaggle.com/wayne999) Short and neat complete genetic programming code (it is suboptimal since the Name feature is not used).
* [0.78468](https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot) Name-only text vectorization and PCA with a 3D interactive plot.
* [0.78947](https://www.kaggle.com/pliptor/titanic-in-python-svm) Gender + Class + Embarked + Age using SVM
* [0.78947](https://www.kaggle.com/yildirimarda/decision-tree-visualization-submission) Gender + Class + FamilySize + Age using Decision Tree by [Arda Yildirim](https://www.kaggle.com/yildirimarda)
* [0.79904](https://www.kaggle.com/rafaelvleite/titanic-artificial-neural-network-80-score) Neural network (keras)  by [Rafael Vicente Leite](https://www.kaggle.com/rafaelvleite).
* [0.79904](https://www.kaggle.com/frederikh/12-500-feet-under-the-sea) Using ethnicity feature by [Frederik Hasecke](https://www.kaggle.com/frederikh).
* [0.80382](https://www.kaggle.com/bwboerman/a-quick-dive-into-h2o-with-python) Using H2O by [Bart Boerman](https://www.kaggle.com/bwboerman)
* [0.80861](https://www.kaggle.com/arthurtok/0-808-with-simple-stacking/output) Simple stacking by [Anisotropic](https://www.kaggle.com/arthurtok) stacking is ubiquitous in competitions.

---

# Public scores (cont'd)

* [0.80861](https://www.kaggle.com/nicapotato/titanic-voting-pipeline-stack-and-guide) Voting/ensembling  by [Nick Brooks](https://www.kaggle.com/nicapotato) An impressive number of models is packed in almost one hour of running time!
* [0.80861](https://www.kaggle.com/chucktalbert/titanic-a-tidy-caret-approach-0-8086) Tidy + Caret, KNN impute and Random Forests optimizer by [Chuck Talbert](https://www.kaggle.com/chucktalbert)
* [0.80861](https://www.kaggle.com/reisel/iterative-prediction-of-survival) Iterative Prediction of Survival by [Reinhard](https://www.kaggle.com/reisel)
* [0.80861](https://www.kaggle.com/njmei42/kaggle-titanic-with-tensorflow/notebook) Kaggle Titanic with Tensorflow by [nme-42](https://www.kaggle.com/njmei42) It is quite an interesting kernel.
* [0.81339](https://www.kaggle.com/jack89roberts/titanic-using-ticket-groupings) Titanic Using Ticket Grouping by [Jack Roberts](https://www.kaggle.com/jack89roberts). 
* [0.81339](https://www.kaggle.com/c/titanic/discussion/6821) Random Forests: A great tutorial by [Trevor Stephens](https://www.kaggle.com/trevorstephens)
* [0.82296](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296) Divide and Conquer. 
* [0.82296](https://www.kaggle.com/bisaria/titanic-lasso-ridge-implementation) Lasso Ridge by [Bisaria](https://www.kaggle.com/bisaria) 
* [0.82296](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818) Titanic using Name only [Chris Deotte](https://www.kaggle.com/cdeotte) achieves this great score using nothing but the Name feature! (Note from his kernel: 0.81818 with Name only and 0.82296 by adding Ticket). 
* [0.82296](https://www.kaggle.com/cdeotte/titanic-deep-net-0-82296) Titanic Deep Net by [Chris Deotte](https://www.kaggle.com/cdeotte)] 
* [0.82775](https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster) [Frank Sylla](https://www.kaggle.com/francksylla) engineers
several features.
* [0.83253](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83/notebook) [Konstantin](https://www.kaggle.com/konstantinmasich) brings
attention to feature scaling, which is essential when working with the kNN algorithm.
* [0.84210](https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210) Titanic Mega Model [Chris Deotte](https://www.kaggle.com/cdeotte) ensembles Kaggle's top 6 models. It starts with a neat ensembling diagram.
* [0.84688](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688) Titanic WCG+XGBoost [Chris Deotte](https://www.kaggle.com/cdeotte) Is this the ultimate Titanic model?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
