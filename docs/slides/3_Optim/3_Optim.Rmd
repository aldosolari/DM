---
title: "Data Mining"
subtitle: "Optimism and information criteria"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```

# Outline

* Optimism
* Information criteria
* Random-X view

---

layout: false
class: inverse, middle, center

# Optimism 

---

# Optimism

Optimism is the **expected** difference of the test error and training error

\begin{aligned}
\mathrm{Opt} &= \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) - \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}})
\end{aligned}

---

# Fixed-X optimism

* For the Fixed-X setting, we have

$$\mathrm{OptF} = \mathbb{E}\left[ \frac{1}{n}\sum_{i=1}^{n}( y^*_i - \hat{f}(x_i))^2 - \frac{1}{n}\sum_{i=1}^{n}( y_i - \hat{f}(x_i))^2 \right]$$

* $\mathrm{OptF}$ can be also expressed as 

$$\mathrm{OptF} =  \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))$$

So the higher the correlation between $y_i$ and its fitted value $\hat{f}(x_i)$, the greater the optimism

---

```{r, echo=FALSE, fig.align = 'center', out.width = '80%', out.height = '80%'}
rm(list=ls())
sigmatrue = 0.01
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
ds = 1:15
ps = ds + 1
n =30
Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
)
Vars = ps*(sigmatrue^2)/n
EMSE.te = Bias2s + Vars + sigmatrue^2
OptF = 2*(sigmatrue^2)*ps/n
EMSE.tr = EMSE.te - OptF
plot(ps, EMSE.tr, type="b", xlab="p", ylab="E(MSE)", lwd=2)
lines(ps, EMSE.te, type="b", col=2, lwd=2)
legend("topright",c("E(MSE.te)","E(MSE.tr)"), col=c(1,2), pch=19)
```


---

# Optimism for linear models

* Prediction error is given by $\mathrm{ErrF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) = \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) + \mathrm{OptF}$

* Optimism is important because it leads to an estimate of $\mathrm{ErrF}$:
\begin{aligned}
\widehat{\mathrm{ErrF}} &= \mathrm{MSE}_{\mathrm{Tr}} + \mathrm{OptF}\\
\end{aligned}

* For linear models, it can be shown that
$$\mathrm{OptF} =  \frac{2\sigma^2 p}{n}$$

---


```{r}
rm(list=ls())
library(readr)
df <- read_table2("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat")[-31,]
train <- data.frame(x=df$x, y=df$y.yesterday)
# compute MSE.tr
n <- nrow(train)
ds = 1:15
ps = ds + 1
x = seq(.5,3,length=30)
fun <- function(d) if (d==0) lm(y~1, train) else lm(y~poly(x,degree=d), train)
fits <- lapply(ds, fun)
MSEs.tr <- unlist( lapply(fits, deviance) )/n
# compute ErrF
sigmatrue = 0.01
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
x = seq(.5,3,length=30)
Bias2s = sapply(ps, function(p) 
  mean( ( ftrue - fitted(lm(ftrue ~ poly(x,degree=(p-1)))) )^2 )
)
Vars = ps*(sigmatrue^2)/n
ErrFs = Bias2s + Vars + sigmatrue^2
```

---

```{r}
{{hatErrFs = MSEs.tr + (2*sigmatrue^2*ps)/n}}
plot(ps, MSEs.tr, type="b", xlab="p", ylab="ErrF")
lines(ps, hatErrFs, type="b", col=2)
lines(ps, ErrFs, type="b", col=4)
legend("topright",c("ErrF","MSE.tr","MSE.tr + Opt"), col=c(4,1,2), pch=19)
ps[which.min(hatErrFs)]
```

---

# Mallows Cp

* $\displaystyle \mathrm{OptF}=\frac{2\sigma^2p}{n}$ requires that $\sigma^2$ is known, which is not realistic

* We can plug-in its estimate 

$$\hat{\sigma}^2= \frac{\mathrm{RSS}}{n-p}= \frac{\sum_{i=1}^{n}( y_i - \hat{f}(x_i))^2}{n-p}$$
where $\mathrm{RSS} = n \mathrm{MSE}_{\mathrm{Tr}}$ is the __residual sum of squares__

* Then we can estimate the prediction error as

\begin{aligned}
\widehat{\mathrm{ErrF}} &= \mathrm{MSE}_{\mathrm{Tr}} + \widehat{\mathrm{OptF}}\\
\end{aligned}

* This estimator it is known as __Mallows' Cp__ : 

$$\mathrm{Cp} = \mathrm{MSE}_{\mathrm{Tr}}  + \frac{2 \hat{\sigma}^2 p}{n}$$

---


```{r}
{{hatsigma2 = (n*MSEs.tr)/(n-ps)}}
Cps = MSEs.tr + (2*hatsigma2*ps)/n 
plot(ps, MSEs.tr, type="b", xlab="p", ylab="ErrF")
lines(ps, Cps, type="b", col=2)
lines(ps, ErrFs, type="b", col=4)
legend("topright",c("ErrF","MSE.tr","MSE.tr + hatOpt"), col=c(4,1,2), pch=19)
ps[which.min(Cps)]
```

---

```{r}
plot(ps, hatsigma2, type="b", xlab="p", ylab="Sigma2")
abline(h=sigmatrue^2, col=4)
```

---

layout: false
class: inverse, middle, center

# Information criteria

---

# AIC and BIC

* __AIC__ is given by
$$\mathrm{AIC} =  -2 \cdot \mathrm{loglikelihood}(\hat{\beta},\hat{\sigma}^2) + 2p$$
where for the linear model $-2 \cdot \mathrm{loglikelihood}(\hat{\beta},\hat{\sigma}^2) = n \log(\mathrm{MSE}_{\mathrm{Tr}})$

* For linear models, Cp and AIC are proportional to each other, and the lowest Cp corresponds to the lowest AIC. 

* __BIC__ is given by
$$\mathrm{BIC} =  -2 \cdot \mathrm{loglikelihood}(\hat{\beta},\hat{\sigma}^2) + \log(n)p$$

* Since $\log(n) > 2$ for any $n > 7$,
the BIC statistic generally results in the selection of smaller models than AIC

---

```{r}
AICs <- unlist( lapply(fits, AIC) )
BICs <- unlist( lapply(fits, BIC) )
plot(ps, AICs, type="b", col=5, ylab="Information Criteria", xlab="p")
lines(ps, BICs, type="b", col=6)
legend("topright",c("AIC","BIC"), col=c(5,6), lty=1)
```

---
layout: false
class: inverse, middle, center

# Random-X prediction error

---

# Random-X view

* Earlier we have assumed Fixed-X view, where predictor values are assumed to be nonrandom

* In most modern predictive modeling applications, it is more reasonable to take a Random-X view, where the predictor values (both those used in training and for future predictions) are random

---

# Random-X setting

We assume 

* Random response $Y$ and random predictors $X=(X_1,\ldots,X_p)^\mathsf{T}$
* $(X,Y)$ have some unknown joint distribution
* Conditional to $X=x$, we have
$$(Y|X=x) = f(x) + \varepsilon$$
where 
    - the conditional expectation $f(x)=\mathbb{E}(Y|X=x)$ is called the __regression function__ 
    - $\varepsilon$ is the error term independent from $X$ with $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}\mathrm{ar}(\varepsilon)=\sigma^2$
* Note that the conditional variance 
$\mathbb{V}\mathrm{ar}(Y|X=x) = \sigma^2$ is constant (__homoscedasticity__ assumption)

* Training set: $n$ observations $(x_1,y_1),\ldots,(x_n,y_n)$ i.i.d. from $(X,Y)$
* Test set: $m$ observations $(x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$ i.i.d. from $(X,Y)$

---

# Gaussian example

For example, suppose that 
$$\left(\begin{array}{c} 
Y \\
X \\
\end{array}\right) \sim N\left(\left(\begin{array}{c} 
\mu_y \\
\mu_x \\
\end{array}\right), \left(\begin{array}{cc} 
\sigma^2_y & \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  & \sigma^2_x \\
\end{array}\right)\right)$$
so that the conditional distribution of $Y$ given $X=x$ is

$$(Y|X=x) \sim N\Big(\mu_y + \rho \frac{\sigma_y}{\sigma_x}(x-\mu_x), \sigma^2_y (1-\rho^2)\Big)$$

where $f(x)= \mathbb{E}(Y|X=x) = \left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right) + \left(\rho\frac{\sigma_y}{\sigma_x}\right)x = \alpha + \beta x$.

---

# Random-X simulation

In the Random-X setting, we generate the training data as follows. 


For $i=1,\ldots,n$:

1. $x_i$ is the realization of $X_i \sim N(\mu_x,\sigma^2_x)$, the marginal distribution of $X$

2. $y_i$ is the realization of $(Y_i|X_i=x_i) = f(x_i) + \varepsilon_i$, the conditional distribution of $Y$ given $X=x$, where 
    - $f(x_i)=\alpha + \beta x_i$ with $\alpha=\left(\mu_y - \rho\frac{\sigma_y}{\sigma_x}\mu_x\right)$ and $\beta= \left(\rho\frac{\sigma_y}{\sigma_x}\right)$
    - $\varepsilon_i \sim N(0,\sigma^2)$ with $\sigma^2=\sigma^2_y (1-\rho^2)$

and analogously for the test data

---

# Random-X prediction error

* The __Random-X prediction error__ is given by
$$\mathrm{ErrR}= \mathbb{E}(\mathrm{MSE_{\mathrm{Te}}}) = \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m}(Y^*_i - \hat{f}(X^*_i))^2\right] = \mathbb{E}[(Y^*_1 - \hat{f}(X^*_1))^2]$$
where the last equality follows by exchangeability and the expectation of the last term is with respect to the training set $(X_1,Y_1),\ldots,(X_n,Y_n)$ and to the new test point $(X^*_1,Y^*_1)$


---

# Random-X optimism for linear models

* Assume that $X = (X_1,\ldots,X_p)^\mathsf{T} \sim N(0,\Sigma)$ where $\Sigma$ is invertible and $p<n-1$

* Assume also that $f(x)= x^\mathsf{T}\beta$, a linear function of $x$ 

* Note that the assumptions above hold if we assume that $(X,Y)$ is jointly Gaussian

* Then, for linear models

$$\mathrm{OptR}= \mathrm{OptF} + \frac{\sigma^2p}{n}\left( \frac{p+1}{n-p-1}\right)$$

---

# Random-X Mallows Cp 

* We have $$\mathrm{ErrR} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) +  \mathrm{OptR}$$

*  We can estimate ErrR by
$$\widehat{\mathrm{ErrR}} = \mathrm{MSE}_{\mathrm{Tr}} + \frac{\sigma^2p}{n}\left(2 + \frac{p+1}{n-p-1}\right)$$

* If we use  $\hat{\sigma}^2 = \mathrm{RSS}/(n-p)$ in place of $\sigma^2$, we obtain a Random-X version of Mallows' Cp 
$$\mathrm{RCp} = \mathrm{Cp} + \frac{\hat{\sigma}^2p}{n}\left( \frac{p+1}{n-p-1}\right) = \frac{\mathrm{RSS}(n-1)}{(n-p)(n-p-1)}$$



