---
title: "Non-parametric regression"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```

# Outline

* Non-parametric regression
* the caret package

---

# Nonparametric methods

* Non-parametric methods do not make explicit assumptions about the functional
form of $f$ (e.g. polynomials)

* Leave data __speak for themselves__ in a free way

* Advantage: by avoiding the assumption of a particular functional form for $f$, they have a wider range of possible shapes for $f$.

* Disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations is required

---
layout: false
class: inverse, middle, center

# k-nearest neighbors 

---

# k-nearest neighbors

* A very simple and quite commonly used method is __k-nearest neighbors__ (kNN)

* Suppose we want to make a prediction at some $x^*_1$. Define the neighbourhood $N_k(x^*_1)$ to be the set of $k$ training observations having values $x_i$ closest to $x^*_1$ in Euclidean norm $\| x_i - x^*_1 \|=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)}$

$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$
* The number $k$ is a __tuning parameter__ 

* Small $k$ corresponds to a more flexible fit

* Large k corresponds to a less flexible fit

* Since we are computing a distance, usually we __center__ and __scale__ the predictors

---

# Exercise

Consider the $n=250$ points version of the yesterday-tomorrow data:

* the true $f$, named `f.true250`, is given [here](http://azzalini.stat.unipd.it/Book-DM/f_true.R)

* the true $\sigma$ is $0.01$

* the $x$ values are given by `x = seq(0.5,3,length=250)`

```{r, echo=F}
rm(list=ls())
n = 250
sigmatrue = 0.01
ftrue <- c(
   0.4342,0.4402,0.4460,0.4515,0.4568,0.4618,0.4665,0.4712,0.4755,0.4797,
   0.4837,0.4875,0.4911,0.4945,0.4978,0.5009,0.5039,0.5067,0.5094,0.5119,
   0.5143,0.5166,0.5187,0.5208,0.5227,0.5245,0.5262,0.5278,0.5293,0.5307,
   0.5321,0.5333,0.5345,0.5355,0.5365,0.5375,0.5383,0.5391,0.5399,0.5405,
   0.5411,0.5417,0.5422,0.5426,0.5430,0.5434,0.5437,0.5440,0.5442,0.5444,
   0.5445,0.5447,0.5447,0.5448,0.5448,0.5448,0.5448,0.5447,0.5446,0.5445,
   0.5444,0.5442,0.5441,0.5439,0.5437,0.5434,0.5432,0.5429,0.5427,0.5424,
   0.5421,0.5418,0.5415,0.5412,0.5408,0.5405,0.5401,0.5398,0.5394,0.5390,
   0.5387,0.5383,0.5379,0.5375,0.5371,0.5367,0.5363,0.5359,0.5355,0.5351,
   0.5347,0.5343,0.5339,0.5335,0.5330,0.5326,0.5322,0.5318,0.5314,0.5310,
   0.5306,0.5302,0.5298,0.5294,0.5290,0.5286,0.5282,0.5278,0.5274,0.5270,
   0.5267,0.5263,0.5259,0.5255,0.5251,0.5248,0.5244,0.5240,0.5237,0.5233,
   0.5230,0.5226,0.5223,0.5219,0.5216,0.5213,0.5209,0.5206,0.5203,0.5200,
   0.5197,0.5193,0.5190,0.5187,0.5184,0.5181,0.5178,0.5176,0.5173,0.5170,
   0.5167,0.5164,0.5162,0.5159,0.5156,0.5154,0.5151,0.5149,0.5146,0.5144,
   0.5141,0.5139,0.5136,0.5134,0.5132,0.5130,0.5127,0.5125,0.5123,0.5121,
   0.5119,0.5117,0.5115,0.5113,0.5111,0.5109,0.5107,0.5105,0.5103,0.5101,
   0.5100,0.5098,0.5096,0.5094,0.5093,0.5091,0.5090,0.5088,0.5086,0.5085,
   0.5083,0.5082,0.5080,0.5079,0.5077,0.5076,0.5075,0.5073,0.5072,0.5071,
   0.5069,0.5068,0.5067,0.5066,0.5065,0.5063,0.5062,0.5061,0.5060,0.5059,
   0.5058,0.5057,0.5056,0.5055,0.5054,0.5053,0.5052,0.5051,0.5050,0.5049,
   0.5048,0.5047,0.5046,0.5045,0.5044,0.5044,0.5043,0.5042,0.5041,0.5041,
   0.5040,0.5039,0.5038,0.5038,0.5037,0.5036,0.5035,0.5035,0.5034,0.5034,
   0.5033,0.5032,0.5032,0.5031,0.5030,0.5030,0.5029,0.5029,0.5028,0.5028,
   0.5027,0.5027,0.5026,0.5026,0.5025,0.5025,0.5024,0.5024,0.5023,0.5023)
x = seq(0.5,3,length=n)
```

Obtain the training set and test set by

```{r}
set.seed(123)
y = ftrue + rnorm(n, 0, sigmatrue)
ystar = ftrue + rnorm(n, 0, sigmatrue)
train = data.frame(x=x, y=y)
test = data.frame(x=x, y=ystar)
```

---

Install the R package `kknn` and perform kNN with $k=21$ by using the function 
`kknn(y ~ x, train, test, kernel = "rectangular", k = 21)`


```{r, echo=F}
library(kknn)
k = 21
fit = kknn(y ~ x, train, test, kernel = "rectangular", k = k)
yhat = fit$fitted.values
plot(y ~ x, train, main=paste("k = ", k))
lines(x,yhat, col=4, type="s")
```

---

Compute MSE.tr and MSE.te for $k=21$

```{r, echo=F}
MSE.tr = mean( (train$y - yhat)^2 )
MSE.tr
MSE.te = mean( (test$y - yhat)^2 )
MSE.te
```

---

Compute MSE.tr and MSE.te for $k=1,2,\ldots,40$ and find the $k$ corresponding to the minimum MSE.te

```{r, echo=F}
ks = 1:40
MSEs.tr = sapply(ks, function(k)
mean( 
  (train$y - kknn(y ~ x, train, test, kernel = "rectangular", k = k)$fitted.values )^2
))
MSEs.te = sapply(ks, function(k)
mean( 
  (test$y - kknn(y ~ x, train, test, kernel = "rectangular", k = k)$fitted.values)^2
))
op <- par(mfrow = c(1, 2)) 
plot(1/ks, MSEs.tr, type="b", ylab="MSE.tr", xlab="1/k")
plot(1/ks,MSEs.te,type="b", ylab="MSE.te", col=4, xlab="1/k")
par(op)
```

---

In the fixed-X setting, theoretical results show that for kNN
$$\mathrm{OptF} = \mathbb{E}(\mathrm{MSE}_{\mathrm{Te}}) - \mathbb{E}(\mathrm{MSE}_{\mathrm{Tr}}) = \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))= \frac{2\sigma^2}{k}$$
Find the $k$ corresponding to the minimum $$\widehat{\mathrm{Err}} = \mathrm{MSE}_{\mathrm{Tr}}  + \frac{2\sigma^2}{k}$$

```{r, echo=F}
hatErr = MSEs.tr + (2*sigmatrue^2)/ks
ks[which.min(hatErr)]
```

---

* In the fixed-X setting, theoretical results show that for kNN
$$\mathrm{Variance}(\hat{f})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{V}\mathrm{ar}[\hat{f}(x_i)] = \frac{\sigma^2}{k}$$

* Remember that the expected value $\mathbb{E}\hat{f}(x_i)$ can be obtained by fitting the kNN model using $y_i = f(x_i)$. This allows to compute the squared bias $[\mathrm{Bias}(\hat{f})]^2= \frac{1}{n}\sum_{i=1}^{n}(\mathbb{E}\hat{f}(x_i) - f(x_i) )^2$

* Compute the redubile error $[\mathrm{Bias}(\hat{f})]^2 + \mathrm{Variance}(\hat{f})$ for $k=1,2,\ldots,40$ and find the $k$ that minimize it.

---

```{r, echo=F}
Vars = sigmatrue^2/ks
Bias2s = sapply(ks, function(k) 
  mean( 
    ( ftrue - kknn(ftrue ~ x, train, test, kernel = "rectangular", k = k)$fitted.values )^2 
      )
  )
Reds = Bias2s+Vars 
barplot(Reds, ylab="Reducible error", names.arg=ks)
barplot(Vars, add=T, col=1, names.arg=" ")
legend("topright", c("Bias2","Var"), col=c("gray",1), pch=c(19,19))
ks[which.min(Reds)]
```

---
layout: false
class: inverse, middle, center

# The caret package

---

# The caret package

The [caret](http://topepo.github.io/caret/index.html) package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

as well as other functionality.

---

# Install and help

* The package utilizes a number of R packages but tries not to load them all at package start-up. The package "suggests" field includes 76 packages

* __Not now!__ but full installation by 
```{r, eval=F}
 install.packages("caret", dependencies = c("Depends", "Suggests"))
```

* The main help pages for the package are at [https://topepo.github.io/caret/](https://topepo.github.io/caret/)

* The book [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) can be view as an introduction to the `caret` package.

---

# train function pseudocode

![](https://i2.wp.com/www.quintuitive.com/wp-content/uploads/2016/09/TrainAlgo.png)


---


```{r}
library(caret)
fit.knn = train(
  y ~ ., train,
  method = "knn")
# result
fit.knn
```

---

.pull-left[
```{r}
rcv <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2)

fit.knn = train(
  y ~ ., train,
  method = "knn",
  trControl=rcv,
  tuneLength = 10)
```
]

.pull-right[
```{r}
plot(fit.knn)
fit.knn$bestTune
```
]

---

.pull-left[
```{r}
fit.poly = train(
  y ~ poly(x,degree=6), train,
  method = "lm",
  trControl=rcv)
models = list(
    knn=fit.knn,
    poly=fit.poly
  )
resamps = resamples(models)
```
]

.pull-right[
```{r}
bwplot(resamps, metric = "RMSE")
yhats = predict(models, newdata=test)
lapply(yhats, function(yhat) sqrt( mean( (yhat - test$y)^2) ) )
```
]




