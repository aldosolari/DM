<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## La regressione ridge
### Aldo Solari

---






# Il teorema di Gauss-Markov 

Si consideri il setting Fixed-X con `\(\mathbf{X}\)` a rango pieno, i.e. `\(\mathrm{rank}(\mathbf{X}) = p\)`, e si assuma che i dati siano generati secondo il modello 
`$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta}^0 + \boldsymbol{\varepsilon}$$`
con `\(\mathbb{E} \boldsymbol{\varepsilon} = \mathbf{0}\)` e `\(\mathbb{V}\mathrm{ar}(\boldsymbol{\varepsilon}) = \sigma^2 \mathbf{I}\)`

Il teorema di Gauss-Markov dimostra che lo stimatore OLS
`$$\hat{\boldsymbol{\beta}}^{\mathrm{OLS}} = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`
è il migliore stimatore lineare tra i non distorti: per qualsiasi altro stimatore non distorto  `\(\tilde{\boldsymbol{\beta}}\)` lineare in `\(\mathbf{y}\)` (ovvero della forma `\(\tilde{\boldsymbol{\beta}} = \mathbf{A} \mathbf{y}\)` per una qualche matrice `\(\mathbf{A}\)`), abbiamo che
`$$\mathbb{V}\mathrm{ar}(\tilde{\boldsymbol{\beta}}) - \mathbb{V}\mathrm{ar}(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}})$$`
è positiva semidefinita. 


---

# La geometria dei minimi quadrati

![](images/projection.png)


* `\(\hat{\boldsymbol{\mu}}\)` indica `\(\hat{\mathbf{y}} =  \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{H} \mathbf{y}\)`, la proiezione di `\(\mathbf{y}\)` su `\(\mathcal{C}(\mathbf{X})\)`, lo spazio generato dalle colonne di `\(\mathbf{X}\)` 

* `\(\mathbf{H} = \mathbf{X} (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\)` indica la matrice  di proiezione ("hat") su `\(\mathcal{C}(\mathbf{X})\)`

* `\(\mathbf{y} - \hat{\boldsymbol{\mu}}\)` indicano i residui `\((\mathbf{I}_n - \mathbf{H})\mathbf{y}\)` 

---

# Perchè considerare stimatori alternativi a OLS?

Se consideriamo la funzione di perdita quadratica, gli stimatori non distorti di `\(\boldsymbol{\beta}_0\)` vengono valutati sulla base della loro varianza.

Per uno stimatore distorto `\(\tilde{\boldsymbol{\beta}}\)`, la quantità rilevante è 
$$
`\begin{aligned}
\mathbb{E}\{(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0)(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0)\}^\mathsf{T} &amp; = \mathbb{V}\mathrm{ar}(\tilde{\boldsymbol{\beta}}) + \{ \mathbb{E}(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0) \} \{ \mathbb{E}(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0) \}^\mathsf{T}
\end{aligned}`
$$
che somma distorsione al quadrato e varianza. 

Un punto cruciale per l'ottimalità per gli stimatori OLS (e MLE) è la loro non distorsione.

Esistono stimatori distorti per cui la loro varianza è ridotta rispetto a OLS in modo tale che l'errore di previsione complessivo sia inferiore? Sì! 

In effetti, l'uso di stimatori distorti è essenziale per trattare contesti in cui il numero di parametri `\(p\)` da stimare è elevato rispetto al numero di osservazioni `\(n\)`

---

# Setting ad elevata dimensionalità


Se `\(n&gt;p\)` ma `\(n \approx p\)`, allora lo stimatore OLS è molto variabile, con conseguente sovra-adattamento e previsioni scadenti

Se `\(n=p\)`, allora lo stimatore OLS si sovra-adatta completamente ai dati, i.e. `\(\hat{y}_i = y_i\)`, e le previsioni saranno inaccettabili 

Se `\(n&lt;p\)` (*high-dimensional setting*), non esiste più una stimatore OLS unico, quindi il metodo non può essere utilizzato

---

# `\(n \approx p\)`

* `\(n=10\)` e `\(p=9\)`

* Si consideri il modello `\(Y = \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_9X_9 +  \varepsilon\)`

* Il vero `\(\boldsymbol{\beta}\)` è `\((1,0,0,\ldots,0)^\mathsf{T}\)`


```r
n = 10
p = 9
set.seed(1793)
X = matrix(rnorm(n*p), nrow=n, ncol=p)
y = X[,1] + rnorm(n,0,0.5)
fit = lm(y~ 0 + X)
coef(fit)
```

```
        X1         X2         X3         X4         X5         X6         X7         X8         X9 
 13.527953  -5.466373 -24.512290  -6.610876   2.732160 -10.670639  15.472750  16.831498 -11.331475 
```

Stime elevate dei coefficienti di regressione (in valore assoluto) sono spesso un'indicazione di sovra-adattamento

---



```r
yhat = predict(fit)
plot(X[,1],y, xlab="x1")
ix = sort(X[,1], index.return=T)$ix
lines(X[ix,1], yhat[ix])
abline(a=0,b=1, col=4)
```

![](Ridge_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---

# OLS in setting ad elevata dimensionalità

Quando `\(\mathrm{rank}(\mathbf{X}) &lt; p\)`, ci sono infinite soluzioni nel problema dei minimi quadrati

Supponiamo `\(p &gt; n = \mathrm{rank}(\mathbf{X})\)`. Sia `\(\mathcal{C}(\mathbf{X})\)` lo spazio `\(n\)`-dimensionale formato dalle colonne di `\(\mathbf{X}\)` e `\(\mathcal{V}=\mathcal{C}(\mathbf{X})^\bot\)` lo spazio `\(p-n\)` dimensionale ortogonale a `\(\mathcal{C}(\mathbf{X})\)`, i.e. lo spazio nullo di `\(\mathbf{X}\)`

* Allora `\(\mathbf{X}\mathbf{v} = \mathbf{0}_p\)` per qualunque `\(\mathbf{v} \in \mathcal{V}\)`, e `\(\mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{v} = \mathbf{X}^\mathsf{T}\mathbf{0}_p = \mathbf{0}_n\)`, la soluzione delle equazioni normali `\(\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T}\mathbf{y}\)` è
`$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-}\mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{v} \quad \forall\,\, \mathbf{v} \in \mathcal{V}$$`
dove `\(\mathbf{A}^-\)` indica l'inversa di Moore-Penrose della matrice `\(\mathbf{A}\)`


---

# La regressione Ridge 

Un modo per ridurre la varianza di 
 `\(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}\)` è di contrarre (*shrink*) le stime dei coefficienti verso il valore zero.

La regressione Ridge (Hoerl and Kennard, 1970) fa proprio questi risolvendo il seguente problema di ottimizzazione
`$$(\hat{\mu}_\lambda^R, \hat{\boldsymbol{\beta}}^R_\lambda) = \arg\min_{(\mu,\boldsymbol{\beta}) \in \mathbb{R} \times \mathbb{R}^p} \left\{ \| \mathbf{y} - \mu\mathbf{1}_n - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|^2_2 \right\}$$`
Vediamo che la funzione obiettivo OLS è penalizzata da un termine aggiuntivo che è proporzionale a  `\(\|\boldsymbol{\beta}\|^2_2\)`

Il parametro `\(\lambda \geq 0\)`, che controlla l'ammontare della penalità e quindi il grado di contrazione verso lo zero, è detto parametro di regolarizzazione (*regularisation parameter* o *tuning parameter*). 


---

# L'intercetta

Nella formulazione precedente l'intercetta non viene penalizzata. La ragione di ciò è che se cambiamo l'origine dei predittori (ad es. il predittore temperatura è misurato in unità Kelvin anziché Celsius), i valori stimati dal modello non cambiano.

Tuttavia, `\(\mathbf{X}\boldsymbol{\beta}\)` non è invariante a trasformazioni di scala dei predittori, quindi è pratica comune centrare ciascuna colonna di `\(\mathbf{X}\)` (quindi rendendola ortogonale al termine intercetta) e poi scalare i predittori in modo tale che la loro norma Euclidea (o `\(\ell_2\)`) risulti pari `\(\sqrt{n}\)`. 

E' immediato dimostrare che dopo questa standardizzazione di 
`\(\mathbf{X}\)`, `\(\hat{\mu}_\lambda^R = \bar{y} = n^{-1}\sum_{i=1}^{n}y_i\)`, quindi possiamo assumere  `\(\sum_{i=1}^{n}y_i=0\)` sostituendo `\(y_i\)` con `\(y_i - \bar{y}\)` e quindi omettere `\(\mu\)` dalla nostra funzione obiettivo. 

---

# Esercizio

Si consideri la minimizzazione del seguente funzione obiettivo che coinvolge la risposta `\(\mathbf{y}\)` e la matrice del disegno `\(\mathbf{X}\)` su `\((\mu,\boldsymbol{\beta}) \in \mathbb{R}\times \mathbb{R}^p\)`:
`$$\min_{(\mu,\boldsymbol{\beta}) \in \mathbb{R} \times \mathbb{R}^p} \left\{ \| \mathbf{y} - \mu\mathbf{1}_n - \mathbf{X}\boldsymbol{\beta}\|_2^2 + J(\boldsymbol{\beta}) \right\}$$`
Qui `\(J:\mathbb{R}^p \rightarrow \mathbb{R}\)` è una funzione di penalità arbitraria. Si supponga che `\(\sum_{i=1}^{n}x_{ij}=0\)` per `\(j=1,\ldots,p\)`. Assumendo che una soluzione  `\((\hat{\mu},\hat{\boldsymbol{\beta}})\)` esista, si dimostri che `\(\hat{\mu} = \bar{y}\)`. 

Ora si consideri `\(J(\boldsymbol{\beta})= \lambda \|\boldsymbol{\beta}\|^2_2\)` in modo da ottenere la funzione obiettivo della regressione *ridge*. Si dimostri che
`$$\hat{\boldsymbol{\beta}}^R_\lambda = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$`

Da qui in poi, ogni qualvolta ci riferiamo alla regressione *ridge*, assumeremo che `\(\mathbf{X}\)` ha le colonne centrate sulla media. 

Si noti che anche se `\(\mathbf{X}\)` non ha rango pieno, è comunque possibile calcolare lo stimatore. D'altra parte, se `\(\mathbf{X}\)` ha rango pieno, vale il teorema che segue. 

---

# Teorema 

Per `\(\lambda\)` sufficientemente piccolo (che dipende da `\(\boldsymbol{\beta}^0\)` e `\(\sigma^2\)`), 

`$$\mathbb{E}\{(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}- \boldsymbol{\beta}^0)(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}- \boldsymbol{\beta}^0)^\mathsf{T}\} - \mathbb{E}\{(\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}} - \boldsymbol{\beta}^0)(\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}}- \boldsymbol{\beta}^0)^\mathsf{T}\}$$`
è positiva definita. 

(per la dimostrazione, si vedano le note scritte a mano). 

---

Il teorema dice che lo stimatore ridge è  preferibile a OLS in termini di errore di previsione (nel setting Fixed-X) a condizione che `\(\lambda\)` sia scelto in modo appropriato. 

Per essere in grado di utilizzare efficacemente la regressione *ridge*, abbiamo bisogno di un modo per selezionare un buon `\(\lambda\)` - lo faremo con il metodo della convalida incrociata. 

Ciò che il teorema non ci dice veramente è in quali situazioni
ci aspettiamo che la regressione *ridge* abbia buone capacità di previsione. Per capirlo, utilizzeremo la decomposizione ai valori singolari (SVD)

---

# Decomposizione ai valori singolari

la decomposizione ai valori singolari, detta anche SVD (dall'acronimo inglese *Singular Value Decomposition*) è una generalizzazione della decomposizione spettrale (*eigendecomposition*) di una matrice quadrata. Possiamo fattorizzare `\(\mathbf{X} \in \mathbb{R}^{n\times p}\)` in
`$$\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}$$`
dove `\(\mathbf{U} \in \mathbb{R}^{n\times n}\)` e `\(\mathbf{V} \in \mathbb{R}^{p\times p}\)` sono matrici ortogonali e `\(\mathbf{D} \in \mathbb{R}^{n\times p}\)` ha `\(D_{11}\geq D_{22} \geq \ldots \geq D_{mm}\geq 0\)` dove `\(m=\min(n,p)\)` e tutti gli altri elementi pari a 0. 

Le colonna `\(j\)`-sima di `\(\mathbf{U}\)` ( `\(\mathbf{V}\)` ) è denominata `\(j\)`-simo vettore singolare sinistro (destro), e `\(D_{jj}\)` è il `\(j\)`-simo valore singolare. 

Il calcolo di questa decomposizione richiede un numero di operazioni dell'ordine `\(O(np\min(n,p))\)`. 

Tale fattorizzazione è indicata come fattorizzazione SVD completa. Nella versione normalmente utilizzata, denominata forma SVD ridotta, se `\(n&gt;p\)`, la matrice `\(\mathbf{U}\)` è sostituita con le sue prime `\(p\)` colonne, mentre `\(\mathbf{D}\)` con le sue prime `\(p\)` righe. Allora `\(\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}\)` dove `\(\mathbf{U} \in \mathbb{R}^{n\times p}\)` ha colonne ortonormali, i.e. `\(\mathbf{U}^\mathsf{T}\mathbf{U} = \mathbf{I}\)` (ma non è più quadrata) mentre `\(\mathbf{D}\)` è quadrata e diagonale. C'è una versione equivalente per `\(p&gt;n\)`.  

---


Sia `\(\mathbf{X} \in \mathbb{R}^{n\times p}\)` la nostra matrice del disegno, e supponiamo `\(n\geq p\)`. 


Utilizzando la SVD ridotta, possiamo scrivere i valori stimati dalla regressione *ridge* come

`$$\begin{aligned}
\mathbf{X}\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}} &amp;= \mathbf{U} \mathbf{D} (\mathbf{D}^2 + \lambda \mathbf{I})^{-1} \mathbf{D}\mathbf{U}^\mathsf{T}\mathbf{y} \\
&amp;= \sum_{j=1}^{p} \mathbf{U}_j \frac{D^2_{jj}}{D^2_{jj} +\lambda} \mathbf{U}_j^\mathsf{T}\mathbf{y}
\end{aligned}$$`

Per confronto, i valori stimati con OLS (quando `\(\mathbf{X}\)` ha rango pieno) sono
`$$\begin{aligned}
\mathbf{X}\hat{\boldsymbol{\beta}}^{\mathrm{OLS}} &amp;= \mathbf{U} \mathbf{U}^\mathsf{T}\mathbf{y}
\end{aligned}$$`

Si noti che `\(\mathbf{X}\mathbf{V}_1 = D_{11}\mathbf{U}_1\)` corrisponde alla prima componente principale, `\(\mathbf{X}\mathbf{V}_2 = D_{22}\mathbf{U}_2\)` alla seconda (che è ortogonale alla prima), `\(\mathbf{X}\mathbf{V}_3 = D_{33}\mathbf{U}_3\)` alla terza (che è ortogonale alle precedenti), etc.

Lo stimatore *ridge* si può riscrivere come
`$$\begin{aligned}
\hat{\boldsymbol{\beta}}^R_\lambda &amp; = \mathbf{V}\mathrm{diag}\left(\frac{D_{11}^2}{D_{11}^2+\lambda}, \ldots, \frac{D_{pp}^2}{D_{pp}^2+\lambda}\right) \mathbf{U}^\mathsf{T}\mathbf{y}
\end{aligned}$$`

La regressione *ridge* contrae maggiormente `\(\mathbf{y}\)` nelle ultime componenti principali di `\(\mathbf{X}\)`. Quindi funzionerà molto bene quando il segnale è presente nella prima componente principale di `\(\mathbf{X}\)`. 

---


```r
## Generazione dei dati
set.seed(123)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- x1 + 0.5*rnorm(n)
x1 &lt;- (x1 - mean(x1))/sd(x1)
x2 &lt;- (x2 - mean(x2))/sd(x2)
x &lt;- cbind(x1, x2)

## SVD (ridotta) di x
svd_x &lt;- svd(x)
str(svd_x)
```

```
List of 3
 $ d: num [1:2] 13.64 3.47
 $ u: num [1:100, 1:2] -0.0858 -0.0253 0.1551 -0.0083 -0.0174 ...
 $ v: num [1:2, 1:2] 0.707 0.707 0.707 -0.707
```

```r
(svd_x$v)
```

```
          [,1]       [,2]
[1,] 0.7071068  0.7071068
[2,] 0.7071068 -0.7071068
```

```r
## Si noti che sono proporzionali a (1,1) e (-1,1) a causa della standardizzazione
```

---


```r
## diagramma di dispersione (utilizzando la stessa scala)
library(MASS)
eqscplot(x1, x2)
## Aggiungo le linee corrispondenti ai vettori dei pesi delle componenti principali
eqscplot(x1, x2)
abline(0, 1)
abline(0, -1)
```

![](Ridge_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---


```r
## Generazione della risposta
sigma &lt;- 5
errors &lt;- sigma*rnorm(n)
errors &lt;- errors - mean(errors)

sig_easy &lt;- sqrt(n)*svd_x$u[, 1]
sig_hard &lt;- sqrt(n)*svd_x$u[, 2]

y_easy &lt;- sig_easy + errors
y_hard &lt;- sig_hard + errors

## Regressione ridge
lambda_max &lt;- 200
nlambda &lt;- 101
lambda &lt;- seq(from=0, to=lambda_max, length.out=nlambda)
lambda_mat &lt;- rep(lambda, each = 2)
dim(lambda_mat) &lt;- c(2, nlambda)

U &lt;- svd_x$u
U_t &lt;- t(U)
V &lt;- svd_x$v
D_lambda_inv &lt;- svd_x$d^2/(svd_x$d^2 + lambda_mat) # matrice 2 x nlambda

coef_easy &lt;- V %*% (D_lambda_inv * as.numeric((U_t %*% y_easy)))

fit_easy &lt;- U %*% (D_lambda_inv * as.numeric(U_t %*% y_easy))
fit_hard &lt;- U %*% (D_lambda_inv * as.numeric(U_t %*% y_hard))
```

---


```r
## Stima dei coefficienti
matplot(main="Coefficients", ylab=expression(hat(beta)), xlab=expression(lambda),
        x=lambda, y=t(coef_easy), type="l")
```

![](Ridge_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---


```r
## Errore di previsione

nreps &lt;- 500
out_mat_easy &lt;- out_mat_hard &lt;- matrix(nrow=nlambda, ncol=nreps)

for (i in seq_len(nreps)) {
  errors &lt;- sigma*rnorm(n)
  errors &lt;- errors - mean(errors)
  sig_easy &lt;- sqrt(n)*svd_x$u[, 1]
  sig_hard &lt;- sqrt(n)*svd_x$u[, 2]
  y_easy &lt;- sig_easy + errors
  y_hard &lt;- sig_hard + errors
  
  fit_easy &lt;- U %*% (D_lambda_inv * as.numeric(U_t %*% y_easy))
  fit_hard &lt;- U %*% (D_lambda_inv * as.numeric(U_t %*% y_hard))
  
  out_mat_easy[, i] &lt;-  colMeans((fit_easy - sig_easy)^2)
  out_mat_hard[, i] &lt;- colMeans((fit_hard - sig_hard)^2)
}
```


---


```r
op &lt;- par(mfrow=c(2, 1))

plot(main="easy", ylab="MSPE", xlab=expression(lambda),
     x=lambda, y=rowMeans(out_mat_easy), type="l")
plot(main="hard", ylab="MSPE", xlab=expression(lambda),
     x=lambda, y=rowMeans(out_mat_hard), type="l")
```

![](Ridge_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

```r
par(op)
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
