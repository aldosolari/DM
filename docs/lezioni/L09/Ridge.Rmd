---
title: "Data Mining"
subtitle: "La regressione ridge"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


<!-- # Matrix notation -->

<!-- Vector of response:  -->
<!-- $$\underset{n\times 1}{\mathbf{y}} =  -->
<!-- \left[ -->
<!-- \begin{array}{c} -->
<!-- y_1   \\ -->
<!-- \cdots\\ -->
<!-- y_i  \\ -->
<!-- \cdots\\ -->
<!-- y_n \\ -->
<!-- \end{array}\right]$$ -->

<!-- Design matrix: -->
<!-- $$\underset{n\times p}{\mathbf{X}} = [\mathbf{x}_1 \cdots \mathbf{x}_p] =  \left[ -->
<!-- \begin{array}{cccccc} -->
<!-- x_{1}^\mathsf{T}   \\ -->
<!-- x_{2}^\mathsf{T}  \\ -->
<!-- \cdots   \\ -->
<!-- x_{i}^\mathsf{T}    \\ -->
<!-- \cdots\\ -->
<!-- x_{n}^\mathsf{T}\\ -->
<!-- \end{array}\right] = \left[ -->
<!-- \begin{array}{cccccc} -->
<!-- x_{11}  & x_{12}  & \cdots   &  x_{1j}  & \cdots   &   x_{1p}  \\ -->
<!-- x_{21}  & x_{22} & \cdots   &  x_{2j}  & \cdots   &   x_{2p}  \\ -->
<!-- \cdots   & \cdots   &  \cdots & \cdots   &  \cdots  \\ -->
<!-- x_{i1}  & x_{i2} & \cdots   &  x_{ij}& \cdots   & x_{ip}    \\ -->
<!-- \cdots   & \cdots   &  \cdots  &  \cdots   &  \cdots\\ -->
<!-- x_{n1}   & x_{n2} & \cdots   & x_{nj}    &  \cdots   &   x_{np}\\ -->
<!-- \end{array}\right]$$ -->

<!-- --- -->

# Il teorema di Gauss-Markov 

Si consideri il setting Fixed-X con $\mathbf{X}$ a rango pieno, i.e. $\mathrm{rank}(\mathbf{X}) = p$, e si assuma che i dati siano generati secondo il modello 
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta}^0 + \boldsymbol{\varepsilon}$$
con $\mathbb{E} \boldsymbol{\varepsilon} = \mathbf{0}$ e $\mathbb{V}\mathrm{ar}(\boldsymbol{\varepsilon}) = \sigma^2 \mathbf{I}$

Il teorema di Gauss-Markov dimostra che lo stimatore OLS
$$\hat{\boldsymbol{\beta}}^{\mathrm{OLS}} = \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg\min} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$
è il migliore stimatore lineare tra i non distorti: per qualsiasi altro stimatore non distorto  $\tilde{\boldsymbol{\beta}}$ lineare in $\mathbf{y}$ (ovvero della forma $\tilde{\boldsymbol{\beta}} = \mathbf{A} \mathbf{y}$ per una qualche matrice $\mathbf{A}$), abbiamo che
$$\mathbb{V}\mathrm{ar}(\tilde{\boldsymbol{\beta}}) - \mathbb{V}\mathrm{ar}(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}})$$
è positiva semidefinita. 


---

# La geometria dei minimi quadrati

![](images/projection.png)


* $\hat{\boldsymbol{\mu}}$ indica $\hat{\mathbf{y}} =  \mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{H} \mathbf{y}$, la proiezione di $\mathbf{y}$ su $\mathcal{C}(\mathbf{X})$, lo spazio generato dalle colonne di $\mathbf{X}$ 

* $\mathbf{H} = \mathbf{X} (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}$ indica la matrice  di proiezione ("hat") su $\mathcal{C}(\mathbf{X})$

* $\mathbf{y} - \hat{\boldsymbol{\mu}}$ indicano i residui $(\mathbf{I}_n - \mathbf{H})\mathbf{y}$ 

---

# Perchè considerare stimatori alternativi a OLS?

Se consideriamo la funzione di perdita quadratica, gli stimatori non distorti di $\boldsymbol{\beta}_0$ vengono valutati sulla base della loro varianza.

Per uno stimatore distorto $\tilde{\boldsymbol{\beta}}$, la quantità rilevante è 
$$
\begin{aligned}
\mathbb{E}\{(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0)(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0)\}^\mathsf{T} & = \mathbb{V}\mathrm{ar}(\tilde{\boldsymbol{\beta}}) + \{ \mathbb{E}(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0) \} \{ \mathbb{E}(\tilde{\boldsymbol{\beta}}- \boldsymbol{\beta}^0) \}^\mathsf{T}
\end{aligned}
$$
che somma distorsione al quadrato e varianza. 

Un punto cruciale per l'ottimalità per gli stimatori OLS (e MLE) è la loro non distorsione.

Esistono stimatori distorti per cui la loro varianza è ridotta rispetto a OLS in modo tale che l'errore di previsione complessivo sia inferiore? Sì! 

In effetti, l'uso di stimatori distorti è essenziale per trattare contesti in cui il numero di parametri $p$ da stimare è elevato rispetto al numero di osservazioni $n$

---

# Setting ad elevata dimensionalità


Se $n>p$ ma $n \approx p$, allora lo stimatore OLS è molto variabile, con conseguente sovra-adattamento e previsioni scadenti

Se $n=p$, allora lo stimatore OLS si sovra-adatta completamente ai dati, i.e. $\hat{y}_i = y_i$, e le previsioni saranno inaccettabili 

Se $n<p$ (*high-dimensional setting*), non esiste più una stimatore OLS unico, quindi il metodo non può essere utilizzato

---

# $n \approx p$

* $n=10$ e $p=9$

* Si consideri il modello $Y = \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_9X_9 +  \varepsilon$

* Il vero $\boldsymbol{\beta}$ è $(1,0,0,\ldots,0)^\mathsf{T}$

```{r}
n = 10
p = 9
set.seed(1793)
X = matrix(rnorm(n*p), nrow=n, ncol=p)
y = X[,1] + rnorm(n,0,0.5)
fit = lm(y~ 0 + X)
coef(fit)
```

Stime elevate dei coefficienti di regressione (in valore assoluto) sono spesso un'indicazione di sovra-adattamento

---


```{r}
yhat = predict(fit)
plot(X[,1],y, xlab="x1")
ix = sort(X[,1], index.return=T)$ix
lines(X[ix,1], yhat[ix])
abline(a=0,b=1, col=4)
```

---

# OLS in setting ad elevata dimensionalità

Quando $\mathrm{rank}(\mathbf{X}) < p$, ci sono infinite soluzioni nel problema dei minimi quadrati

Supponiamo $p > n = \mathrm{rank}(\mathbf{X})$. Sia $\mathcal{C}(\mathbf{X})$ lo spazio $n$-dimensionale formato dalle colonne di $\mathbf{X}$ e $\mathcal{V}=\mathcal{C}(\mathbf{X})^\bot$ lo spazio $p-n$ dimensionale ortogonale a $\mathcal{C}(\mathbf{X})$, i.e. lo spazio nullo di $\mathbf{X}$

* Allora $\mathbf{X}\mathbf{v} = \mathbf{0}_p$ per qualunque $\mathbf{v} \in \mathcal{V}$, e $\mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{v} = \mathbf{X}^\mathsf{T}\mathbf{0}_p = \mathbf{0}_n$, la soluzione delle equazioni normali $\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\mathsf{T}\mathbf{y}$ è
$$\underset{p\times 1}{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-}\mathbf{X}^\mathsf{T} \mathbf{y} + \mathbf{v} \quad \forall\,\, \mathbf{v} \in \mathcal{V}$$
dove $\mathbf{A}^-$ indica l'inversa di Moore-Penrose della matrice $\mathbf{A}$


---

# La regressione Ridge 

Un modo per ridurre la varianza di 
 $\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}$ è di contrarre (*shrink*) le stime dei coefficienti verso il valore zero.

La regressione Ridge (Hoerl and Kennard, 1970) fa proprio questi risolvendo il seguente problema di ottimizzazione
$$(\hat{\mu}_\lambda^R, \hat{\boldsymbol{\beta}}^R_\lambda) = \arg\min_{(\mu,\boldsymbol{\beta}) \in \mathbb{R} \times \mathbb{R}^p} \left\{ \| \mathbf{y} - \mu\mathbf{1}_n - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|^2_2 \right\}$$
Vediamo che la funzione obiettivo OLS è penalizzata da un termine aggiuntivo che è proporzionale a  $\|\boldsymbol{\beta}\|^2_2$

Il parametro $\lambda \geq 0$, che controlla l'ammontare della penalità e quindi il grado di contrazione verso lo zero, è detto parametro di regolarizzazione (*regularisation parameter* o *tuning parameter*). 


---

# L'intercetta

Nella formulazione precedente l'intercetta non viene penalizzata. La ragione di ciò è che se cambiamo l'origine dei predittori (ad es. il predittore temperatura è misurato in unità Kelvin anziché Celsius), i valori stimati dal modello non cambiano.

Tuttavia, $\mathbf{X}\boldsymbol{\beta}$ non è invariante a trasformazioni di scala dei predittori, quindi è pratica comune centrare ciascuna colonna di $\mathbf{X}$ (quindi rendendola ortogonale al termine intercetta) e poi scalare i predittori in modo tale che la loro norma Euclidea (o $\ell_2$) risulti pari $\sqrt{n}$. 

E' immediato dimostrare che dopo questa standardizzazione di 
$\mathbf{X}$, $\hat{\mu}_\lambda^R = \bar{y} = n^{-1}\sum_{i=1}^{n}y_i$, quindi possiamo assumere  $\sum_{i=1}^{n}y_i=0$ sostituendo $y_i$ con $y_i - \bar{y}$ e quindi omettere $\mu$ dalla nostra funzione obiettivo. 

---

# Esercizio

Si consideri la minimizzazione del seguente funzione obiettivo che coinvolge la risposta $\mathbf{y}$ e la matrice del disegno $\mathbf{X}$ su $(\mu,\boldsymbol{\beta}) \in \mathbb{R}\times \mathbb{R}^p$:
$$\min_{(\mu,\boldsymbol{\beta}) \in \mathbb{R} \times \mathbb{R}^p} \left\{ \| \mathbf{y} - \mu\mathbf{1}_n - \mathbf{X}\boldsymbol{\beta}\|_2^2 + J(\boldsymbol{\beta}) \right\}$$
Qui $J:\mathbb{R}^p \rightarrow \mathbb{R}$ è una funzione di penalità arbitraria. Si supponga che $\sum_{i=1}^{n}x_{ij}=0$ per $j=1,\ldots,p$. Assumendo che una soluzione  $(\hat{\mu},\hat{\boldsymbol{\beta}})$ esista, si dimostri che $\hat{\mu} = \bar{y}$. 

Ora si consideri $J(\boldsymbol{\beta})= \lambda \|\boldsymbol{\beta}\|^2_2$ in modo da ottenere la funzione obiettivo della regressione *ridge*. Si dimostri che
$$\hat{\boldsymbol{\beta}}^R_\lambda = (\mathbf{X}^\mathsf{T}\mathbf{X} + \lambda \mathbf{I}_p )^{-1}\mathbf{X}^\mathsf{T} \mathbf{y}$$

Da qui in poi, ogni qualvolta ci riferiamo alla regressione *ridge*, assumeremo che $\mathbf{X}$ ha le colonne centrate sulla media. 

Si noti che anche se $\mathbf{X}$ non ha rango pieno, è comunque possibile calcolare lo stimatore. D'altra parte, se $\mathbf{X}$ ha rango pieno, vale il teorema che segue. 

---

# Teorema 

Per $\lambda$ sufficientemente piccolo (che dipende da $\boldsymbol{\beta}^0$ e $\sigma^2$), 

$$\mathbb{E}\{(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}- \boldsymbol{\beta}^0)(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}}- \boldsymbol{\beta}^0)^\mathsf{T}\} - \mathbb{E}\{(\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}} - \boldsymbol{\beta}^0)(\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}}- \boldsymbol{\beta}^0)^\mathsf{T}\}$$
è positiva definita. 

(per la dimostrazione, si vedano le note scritte a mano). 

---

Il teorema dice che lo stimatore ridge è  preferibile a OLS in termini di errore di previsione (nel setting Fixed-X) a condizione che $\lambda$ sia scelto in modo appropriato. 

Per essere in grado di utilizzare efficacemente la regressione *ridge*, abbiamo bisogno di un modo per selezionare un buon $\lambda$ - lo faremo con il metodo della convalida incrociata. 

Ciò che il teorema non ci dice veramente è in quali situazioni
ci aspettiamo che la regressione *ridge* abbia buone capacità di previsione. Per capirlo, utilizzeremo la decomposizione ai valori singolari (SVD)

---

# Decomposizione ai valori singolari

la decomposizione ai valori singolari, detta anche SVD (dall'acronimo inglese *Singular Value Decomposition*) è una generalizzazione della decomposizione spettrale (*eigendecomposition*) di una matrice quadrata. Possiamo fattorizzare $\mathbf{X} \in \mathbb{R}^{n\times p}$ in
$$\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}$$
dove $\mathbf{U} \in \mathbb{R}^{n\times n}$ e $\mathbf{V} \in \mathbb{R}^{p\times p}$ sono matrici ortogonali e $\mathbf{D} \in \mathbb{R}^{n\times p}$ ha $D_{11}\geq D_{22} \geq \ldots \geq D_{mm}\geq 0$ dove $m=\min(n,p)$ e tutti gli altri elementi pari a 0. 

Le colonna $j$-sima di $\mathbf{U}$ ( $\mathbf{V}$ ) è denominata $j$-simo vettore singolare sinistro (destro), e $D_{jj}$ è il $j$-simo valore singolare. 

Il calcolo di questa decomposizione richiede un numero di operazioni dell'ordine $O(np\min(n,p))$. 

Tale fattorizzazione è indicata come fattorizzazione SVD completa. Nella versione normalmente utilizzata, denominata forma SVD ridotta, se $n>p$, la matrice $\mathbf{U}$ è sostituita con le sue prime $p$ colonne, mentre $\mathbf{D}$ con le sue prime $p$ righe. Allora $\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}^\mathsf{T}$ dove $\mathbf{U} \in \mathbb{R}^{n\times p}$ ha colonne ortonormali, i.e. $\mathbf{U}\mathbf{U}^\mathsf{T} = \mathbf{I}$ (ma non è più quadrata) mentre $\mathbf{D}$ è quadrata e diagonale. C'è una versione equivalente per $p>n$.  

---


Sia $\mathbf{X} \in \mathbb{R}^{n\times p}$ la nostra matrice del disegno, e supponiamo $n\geq p$. 


Utilizzando la SVD ridotta, possiamo scrivere i valori stimati dalla regressione *ridge* come

$$\begin{aligned}
\mathbf{X}\hat{\boldsymbol{\beta}}_\lambda^{\mathrm{R}} &= \mathbf{U} \mathbf{D} (\mathbf{D}^2 + \lambda \mathbf{I})^{-1} \mathbf{D}\mathbf{U}^\mathsf{T}\mathbf{y} \\
&= \sum_{j=1}^{p} \mathbf{U}_j \frac{D^2_{jj}}{D^2_{jj} +\lambda} \mathbf{U}_j^\mathsf{T}\mathbf{y}
\end{aligned}$$

Per confronto, i valori stimati con OLS (quando $\mathbf{X}$ ha rango pieno) sono
$$\begin{aligned}
\mathbf{X}\hat{\boldsymbol{\beta}}^{\mathrm{OLS}} &= \mathbf{U}^\mathsf{T} \mathbf{U}\mathbf{y}
\end{aligned}$$

Si noti che $\mathbf{X}\mathbf{V}_1 = D_{11}\mathbf{U}_1$ corrisponde alla prima componente principale, $\mathbf{X}\mathbf{V}_2 = D_{22}\mathbf{U}_2$ alla seconda (che è ortogonale alla prima), $\mathbf{X}\mathbf{V}_3 = D_{33}\mathbf{U}_3$ alla terza (che è ortogonale alle precedenti), etc.

La regressione *ridge* contrae maggiormente $\mathbf{y}$ nelle ultime componenti principali di $\mathbf{X}$. Quindi funzionerà molto bene quando il segnale è presente nella componente principale di $\mathbf{X}$. 

<!-- --- -->

<!-- # Solution path -->


<!-- __Solution path__ of the ridge estimator: -->
<!-- $$\{\hat{\boldsymbol{\beta}}(\lambda): \lambda \in [0,\infty) \}$$ -->

<!-- As $\lambda \rightarrow 0$, $\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \hat{\boldsymbol{\beta}}$ -->

<!-- As $\lambda \rightarrow \infty$, $\hat{\boldsymbol{\beta}}(\lambda) \rightarrow \mathbf{0}_p$ -->

<!-- All regression coefficients are shrunken towards zero as the tuning parameter $\lambda$ increases -->

<!-- This behaviour is not strictly monotone in $\lambda$: $\lambda_a > \lambda_b$ does not necessarily imply $|\hat{\beta}_j(\lambda_a)| < |\hat{\beta}_j(\lambda_b)|$ -->




