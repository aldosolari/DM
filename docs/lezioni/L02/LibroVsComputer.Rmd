---
title: "Data Mining"
subtitle: "Libro di testo vs computer"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      countIncrementalSlides: false
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F)
```

# Libro di testo vs computer

* Nei libri di testo viene spesso presentata la soluzione ad un certo problema con una formula matematica. Ad esempio, si consideri la soluzione 
$$\hat{\beta} = (X^\mathsf{T} X)^{-1}Xy$$

* Tuttavia la traduzione diretta di questo tipo di formule in codice non è sempre consigliabile perché ci sono molti aspetti problematici dei computer che semplicemente non sono rilevanti quando si scrivono le cose su carta

* I potenziali problemi computazionali che possono emergere sono

1. **Overflow** Quando i numeri diventano troppo grandi, non possono essere rappresentati su un computer e quindi spesso vengono prodotte NA

2. **Underflow** Simile all'overflow, i numeri possono diventare troppo piccoli per essere rappresentati dai computer, provocando errori o avvisi o calcoli imprecisi

3. **Dipendenza quasi lineare**  la precisione del computer determina se una matrice ha (computazionalmente) colonne linearmente dipendenti

---

# La stima di $\beta$

* Si consideri la soluzione 
$$\hat{\beta} = (X^\mathsf{T} X)^{-1}Xy$$

* Questa soluzione può essere tradotta in codice R come
```{r, eval=F}
betahat <- solve(t(X) %*% X) %*% t(X) %*% y
```

* Tuttavia non è consigliabile calcolare il valore di $\hat{\beta}$ in questo modo

* La ragione principale è che il calcolo dell'inversa di $X^\mathsf{T} X$ è molto costoso dal punto di vista computazionale ed è un'operazione potenzialmente instabile su un computer quando c'è un'elevata collinearità tra i predittori

* Inoltre, per calcolare $\hat{\beta}$ non abbiamo bisogno dell'inversa di
$X^\mathsf{T} X$, quindi perché calcolarla?

---

# Le equazioni normali

* Basta infatti considerare le equazioni normali
$$X^\mathsf{T} X \beta = X^\mathsf{T} y$$
e risolverle direttamente:
```{r, eval=F}
solve(crossprod(X), crossprod(X, y))
```

* Questo approccio ha il vantaggio di essere più stabile numericamente e di essere molto più veloce

---


```{r}
# generiamo dei dati con n = 500 e p = 100
set.seed(123)
n <- 500; p <- 100
X <- matrix(rnorm(n*p), ncol=p)
y <- rnorm(n)

# confrontiamo i due approcci:
library(microbenchmark)
microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, 
            solve(crossprod(X), crossprod(X, y)))

```

---

# Collinearità 


* Per ottenere una situazione di collinearità, possiamo aggiungere una colonna a $X$ che è molto simile (ma non identica) alla prima colonna di $X$

```{r}
W <- cbind(X, X[, 1] + rnorm(n, sd = 1e-10))
```

* L'approccio "diretto" fallisce quando c'è elevata multicollinearità

```{r, eval=F}
solve(crossprod(W), crossprod(W, y))
```
> Error in solve.default(crossprod(W), crossprod(W, y)) : system is computationally singular: reciprocal condition number = 7.94831e-17

* Se $n\geq p$, il *reciprocal condition number* è sostanzialmente dato dal rapporto tra il minimo e il massimo autovalore di $X^\mathsf{T}X$

```{r}
eigvals = eigen(crossprod(W), only.values = T)$values
min(eigvals)/max(eigvals)
```

---

# Solve 

* Le equazioni normali 
$$X^\mathsf{T} X \beta = X^\mathsf{T} y$$
rappresentano un caso particolare di un generico sistema di equazioni
$$A x = b$$

* La funzione `solve(A, b, ...)` risolve questo sistema di equazioni, dove $A$ è una matrice quadrata e $b$ può essere un vettore o una matrice

* Se $b$ non viene specificato, allora diventa la matrice identità $I$, quindi il problema si traduce in $Ax = I$, ovvero trovare l'inversa di $A$

---

# La decomposizione di Cholesky

* Poichè nel nostro caso la matrice $A= X^\mathsf{T} X$ è simmetrica e (di solito) definita positiva, possiamo considerare la decomposizione di Cholesky
$$A = L L^\mathsf{T}$$
dove $L$ è una matrice triangolare inferiore 

* Con questa decomposizione possiamo scrivere 
$$
\begin{aligned}
L L^\mathsf{T} x= b\\
L z = b
\end{aligned}
$$
dove $z = L^\mathsf{T} x$ è la nuova incognita

* Quando nel generico sistema di equazioni $Ax = b$ la matrice $A$ è triangolare inferiore (superiore), si può applicare l'algoritmo di *forwardsolve* (*backsolve*)

---

# Algoritmo di backsolve

* Si consideri la seguente matrice triangolare superiore

$$\left[\begin{array}{ccc}
l_{1,1}  & l_{1,2}  & l_{1,3} \\ 
0  & l_{2,2}  & l_{2,3} \\ 
0  & 0  & l_{3,3} \\ 
\end{array}\right] 
\left[\begin{array}{c}
z_{1}   \\ 
z_{2} \\ 
z_{3} \\ 
\end{array}\right] = \left[\begin{array}{c}
b_{1}   \\ 
b_{2} \\ 
b_{3} \\ 
\end{array}\right]$$

* Dall'ultima riga (equazione) risulta
$$z_{3} = \frac{b_3}{l_{3,3}}$$

* La seconda riga (equazione) coinvolge solamente $z_2$ e $z_3$, quindi 
$$z_2 = \frac{b_2}{l_{2,2}} + \frac{l_{2,3} z_3}{l_{2,2}}$$

* Infine 
$$z_{1} = \frac{b_1}{l_{1,1}} + \frac{l_{1,2} z_2}{l_{1,1}} + \frac{l_{1,3} z_3}{l_{1,1}}$$

---

* L'algoritmo di *forwardsolve* utilizza sostanzialmente la stessa tecnica per matrici triangolari inferiori

* Possiamo quindi scrivere 
$$
\begin{aligned}
X X^\mathsf{T} x = X^\mathsf{T} y\\
L L^\mathsf{T} x = b\\
L z = b
\end{aligned}
$$
 

* Prima si risolve $L z = b$ con
```{r, eval=F}
forwardsolve(L, b)
```

* Poi si risolve $L^\mathsf{T}x  = z$ con 
```{r, eval=F}
backsolve(t(L), forwardsolve(L, b))
```

---

```{r}
# Calcolare la stima OSL con la decomposizione di Cholesky
#
# Argomenti:
# X: la matrice del disegno
# y: il vettore risposta
#
# Ritorna:
# Il vettore hatbeta di lunghezza ncol(X).
ols_chol <-
function(X, y)
{
XtX <- crossprod(X)
Xty <- crossprod(X, y)
L <- t(chol(XtX))
betahat <- backsolve(t(L), forwardsolve(L, Xty))
betahat
}
```


---

```{r}
# generiamo dei dati con n = 1000 e p=4
n <- 1e4; p <-4
X <- matrix(rnorm(n*p), ncol=p)
beta = 1:4
epsilon <- rnorm(n)
y <- X %*% beta + epsilon 

# stimiamo beta
ols_chol(X,y)

# verifichiamo con lm
coef(lm(y ~ X - 1))
```

---

# La decomposizione QR

* La funzione `lm` adotta un approccio diverso: utilizza la decomposizione QR, che non è così veloce, ma ha l'ulteriore vantaggio di essere in grado di rilevare e gestire automaticamente le colonne collineari

* La decomposizione QR (ridotta) prevede
$$\underset{n \times p}{X} = \underset{n \times p}{Q}\underset{p \times p}{R}$$
dove $Q$ è una matrice ortogonale tale che $Q^\mathsf{T} Q = I$, ed $R$ è una matrice triangolare superiore, quindi
$$
\begin{aligned}
X^\mathsf{T}X \beta= X^\mathsf{T} y\\
R^\mathsf{T} Q^\mathsf{T} Q R = R^\mathsf{T} Q^\mathsf{T} y\\
R^\mathsf{T} R = R^\mathsf{T} Q^\mathsf{T} y\\
 R =  Q^\mathsf{T} y\\
\end{aligned}
$$

* Possiamo quindi risolvere il sistema con l'algoritmo *backsolve* senza dover calcolare $X^\mathsf{T} X$, che potrebbe risultare numericamente instabile

---

```{r}
# Calcolare le stime OLS con la decomposizione QR 
#
# Argomenti:
# X: la matrice del disegno
# y: il vettore risposta
#
# Ritorna:
# Il vettore hatbeta di lunghezza ncol(X).
ols_qr <-
function(X, y)
{
qr_obj <- qr(X)
Q <- qr.Q(qr_obj)
R <- qr.R(qr_obj)
Qty <- crossprod(Q, y)
betahat <- backsolve(R, Qty)
betahat
}
```

---

```{r}
# stimiamo beta
ols_qr(X,y)

# verifichiamo con lm
coef(lm(y ~ X - 1))
```

---

```{r}
X <- matrix(c(10^9, -1, -1, 10^(-5)), 2, 2)
beta <- c(1,1)
y <- X %*% beta
# visto che in questo esempio X è una matrice quadrata, possiamo risolvere con
solve(X, y)
# il reciprocal condition number di X
eigvals = eigen(X)$values
min(eigvals)/max(eigvals)
```

Il valore è molto piccolo, ma non inferiore alla soglia di tolleranza del computer di $\approx 10^{-16}$
```{r}
.Machine$double.eps
```

---

```{r, eval=FALSE}
# se proviamo con le equazioni normali
solve( crossprod(X), crossprod(X, y) )
```

> Error in solve.default(crossprod(X), crossprod(X, y)) : 
  system is computationally singular: reciprocal condition number = 9.998e-29

```{r}
eigvals = eigen(crossprod(X))$values
min(eigvals)/max(eigvals)
```

Ora il valore è inferiore alla soglia di tolleranza $\approx 10^{-16}$, 

```{r}
solve(crossprod(X), crossprod(X, y), tol = 0)
```

La soluzione purtroppo è sbagliata. Se evitiamo di calcolare $X^\mathsf{T}X$, la soluzione risulta corretta

```{r}
ols_qr(X, y)
```

---

```{r}
# generiamo dei dati collineari con n = 500 e p = 100
set.seed(123)
n <- 500; p <- 100
X <- matrix(rnorm(n*p), ncol=p)
y <- rnorm(n)
W <- cbind(X, X[, 1] + rnorm(n, sd = 1e-10))
eigvals = eigen(crossprod(W), only.values = T)$values
min(eigvals)/max(eigvals)

qr(W)$rank
```





