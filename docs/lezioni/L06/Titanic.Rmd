---
title: "Data Mining"
subtitle: "I dati Titanic"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=F, message=F, warning=F, error=F, comment=NA, cache=F)
```



# Problemi di classificazione

* Variabile risposta $Y \in \{0,1\}$ (a volte è conveniente $Y \in \{-1,1\}$)

* Predittori $X=(X_1,\ldots,X_p)^\mathsf{T}$

* $(X,Y)$ hanno una certa distribuzione congiunta (non nota)

* Si noti che in questo caso la funzione di regressione diventa la probabilità condizionata di $Y=1$ dato $x$
$$f(x) = \mathbb{E}(Y|X=x) = \mathrm{Pr}(Y=1|X=x)$$

* La regola di classificazione di Bayes (*Bayes classifier*) è definita da
$$c(x) = \left\{\begin{array}{ll}
1 & \mathrm{se\,\,} f(x)>1/2 \\
0 & \mathrm{altrimenti}\\
\end{array}\right.$$

---

# Il tasso di errore di Bayes

* Una regola di classificazione è una funzione
$\hat{c}: x \mapsto \{0,1\}$

* Per esempio, possiamo utilizzare il principio di sostituzione (*plug-in rule*)
$$\hat{c}(x) = \left\{\begin{array}{ll}
1 & \mathrm{if\,\,} \hat{f}(x)>1/2 \\
0 & \mathrm{otherwise}\\
\end{array}\right.$$
dove $\hat{f}$ è una stima di $f$ che si basa sui dati di training

* La regola di classificazione di Bayes è ottima nel senso che minimizza l'errore di classificazione:
$$\mathbb{E}\left[\mathrm{Pr}(Y \neq c(x) ) \right]\leq   \mathbb{E}\left[ \mathrm{Pr}(Y \neq \hat{c}(x))\right] \quad \forall \, \hat{c}$$
dove il valore atteso è rispetto ad $X$

* Il tasso di errore di Bayes $\mathbb{E}\left[\mathrm{Pr}(Y \neq c(x) ) \right]$ è analogo all'errore irriducibile per un problema di classificazione

---

# Errori di approssimazione e di stima

* Per un problema di classificazione, i concetti di distorsione e varianza si generalizzano in errori di **approssimazione** e di **stima**

* Sia $\mathcal{F}$ la classe di modelli considerata

* Supponiamo di aver stimato un modello $\hat{f}$ utilizzando un numero (finito) di osservazioni di training

* Sia $f^*$ il miglior modello possibile in $\mathcal{F}$ ipotizzando un numero infinito di osservazioni di training

* **Errore di approssimazione**: L'errore di $f^*$ dovuto al fatto che la classe $\mathcal{F}$ è limitata

* **Errore di stima**: Errore del modello stimato $\hat{f}$ (relativamente ad $f^*$) dovuto al fatto che abbiamo un numero finito di osservazioni di training


$$\mathrm{Errore\,\,riducibile} = \underbrace{\mathrm{Err}(f^*) }_{\mathrm{err.\,\,di\,\,approssimazione}}+ \underbrace{[\mathrm{Err}(\hat{f})-\mathrm{Err}(f^*)]}_{\mathrm{err.\,\,di\,\,stima}}$$


---

# Classificazione con più di due classi


* Variabile risposta con $K$ classi:  $Y \in \{1,\ldots, K\}$ 

* Probabilità di appartenenza ad una certa classe condizionata al valore di $x$:

$$p_k(x)=\mathrm{Pr}(Y=k|X=x), \quad k = 1,\ldots,K$$

* Il classificatore di Bayes dato $x$ risulta
$$c(x)=j \quad \mathrm{se} \quad p_j(x)=\max\{p_1(x),\ldots,p_K(x)\}$$
* Il classificatore di Bayes minimizza l'errore 
$$1-\mathbb{E}\big(\max_j \Pr(Y=j|X)\big)$$

---


# Approcci 

* **Discriminativo** Si cerca di modellare direttamente $\Pr(Y=k|X=x)$
    - Regressione lineare o logistica
    - $k$-NN
    - *Support vector machine*
    - Etc.

* **Generativo** Si cerca di modellare $\Pr(Y=k|X=x)$ sfruttando il Teorema di Bayes:
$$\Pr(Y=k|X=x) \propto \Pr(X=x|Y=k) \Pr(Y=k)$$
    - Analisi Discriminante Lineare o Quadratica
    - *Naive Bayes*
    - Etc.

---

# Errore di classificazione e accuratezza

* Training set: $(x_1, y_1), (x_2,y_2),\ldots, (x_n,y_n)$
* Test set: $(x^*_1, y^*_1), (x^*_2,y^*_2),\ldots, (x^*_m,y^*_m)$


* __Errore di classificazione (training set)__

$$\mathrm{Err}_{\mathrm{Tr}} = \frac{1}{n} \sum_{i=1}^{n} I\{y_i \neq \hat{c}(x_i) \}$$

* __Errore di classificazione (test set)__
$$\mathrm{Err}_{\mathrm{Te}} = \frac{1}{m} \sum_{i=1}^{m} I\{y^*_i \neq \hat{c}(x^*_i) \}$$

* __Accuratezza (test set)__
$$\mathrm{Acc}_{\mathrm{Te}} = 1- \mathrm{Err}_{\mathrm{Te}}$$

---

layout: false
class: inverse, middle, center

# I dati Titanic


---

# Introduzione

l 15 aprile 1912, durante il suo viaggio inaugurale, il Titanic affondò dopo essersi scontrato con un iceberg, causando la morte di 1502 persone (su 2224 tra passeggeri ed equipaggio)

![](https://vignette.wikia.nocookie.net/foreverknight/images/7/7f/Route_of_Titanic.svg/revision/latest/scale-to-width-down/640?cb=20090303115410)

* Training set di $n = 891$ passeggeri, sui quali sono state misurate $10$ variabili (predittori)

* L'obiettivo è prevedere la sorte (1 = soppravissuto, 0 = non sopravvissuto) di $m = 418$ passeggeri del test set


---

# Letture consigliate

Si consiglia la lettura di [Varian (2014) Big Data: New Tricks for Econometrics](https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3). In particolare

* l'esempio Titanic (sezione *Classification and Regression Trees*)

* il codice R utilizzata (potete scaricare i [Data Set](https://www.aeaweb.org/jep/data/2802/2802-0003_data.zip) nella sezione *Additional Materials*)

La competizione Kaggle 
[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic). In particolare

* [Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic) è un buon tutorial da cui partire

* [Tidy TitaRnic
](https://www.kaggle.com/headsortails/tidy-titarnic) fornisce un buon esempio di EDA

* [Titanic using Name only](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818) fornisce un buon esempio di *feature engineering*

---

```{r}
# importazione dei dati
PATH <- "/Users/aldosolari/Documents/mygithub/DM/dataset/"
train <- read.csv(paste(PATH,"titanic_tr.csv",sep=""), 
                  header = TRUE, stringsAsFactors = FALSE)
n <- nrow(train)
test <- read.csv(paste(PATH,"titanic_te.csv",sep=""), 
                 header = TRUE, stringsAsFactors = FALSE)
m <- nrow(test)
# unisco il training set con il test set
combi <- rbind(train, test)
# controllo la tipologia delle variabili
str(combi)
```

---

# Tipologia delle variabili

| Name | Description |
|---|----|
| pclass   |       Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) |
| survived    |   Survival (0 = No; 1 = Yes) |
| name      |      Name |
| sex      |       Sex |
| age      |       Age |
| sibsp     |      Number of Siblings/Spouses Aboard |
| parch     |      Number of Parents/Children Aboard |
| ticket    |      Ticket Number |
| fare      |      Passenger Fare |
| cabin     |      Cabin |
| embarked   |     Port of Embarkation  (C = Cherbourg; Q = Queenstown; S = Southampton) |

* Si veda il file informativo:  [http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt](http://biostat.mc.vanderbilt.edu/twiki/pub/Main/DataSets/titanic3info.txt)

---

```{r}
# copia della risposta codificata come factor 
combi$survived01 <- combi$survived
combi$survived <- as.factor(combi$survived01)
levels(combi$survived) = c("Death","Alive")
# test survived NA
testsurvived <- combi$survived[(n+1):(n+m)]
combi$survived[(n+1):(n+m)] <- NA
combi$survived01[(n+1):(n+m)] <- NA
# ricodifica di pclass, sex, embarked in factor
combi$pclass <- as.factor(combi$pclass)
combi$sex <- as.factor(combi$sex)
combi$embarked <- as.factor(combi$embarked)
```

---

layout: false
class: inverse, middle, center

# Valori mancanti

---

# Dove sono i valori mancanti?

```{r}
# cabin contiene valori mancanti codificati "" invece di NA
combi$cabin[combi$cabin==""] <- NA
# dove sono i valori mancanti?
summary(combi)
```


---

```{r}
library(VIM)
aggr(combi[,-c(2,12)], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)
```


---

```{r, echo=F}
aggr(combi[,-c(2,12)], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE, plot=F)
```

---

# Tariffa (*fare*)

```{r}
# 1 valore mancante in fare
combi[which(is.na(combi$fare)), ]
```

---

# Sostituzione dei valori mancanti

```{r}
aggregate(fare ~ pclass + embarked, combi, FUN=median)
combi$fare[which(is.na(combi$fare))] <- 8.0500
```

---

# Porto di imbarcazione (*embarked*)

```{r}
# 2 valori mancanti in embarked
combi[which(is.na(combi$embarked)), ]
```

---

# Biglietto (*ticket*)

![](https://upload.wikimedia.org/wikipedia/commons/a/ad/Ticket_for_the_Titanic%2C_1912.jpg)


---

# Numero di copie dello stesso biglietto

```{r}
# persone diverse hanno lo stesso numero di biglietto
head(table(combi$ticket))
# numero di persone che condividono lo stesso biglietto
table(table(combi$ticket))
# numero di copie dello stesso biglietto
combi$ticketFreq <- ave(1:nrow(combi),combi$ticket,FUN=length)
```

---

# Prezzo del biglietto

.pull-left[
```{r}
combi$price <- combi$fare/combi$ticketFreq
```
]

.pull-right[
```{r, echo=F}
plot(fare ~ ticketFreq, combi, subset=pclass==3)
points(price ~ ticketFreq, combi, subset=pclass==3, col=2)
```
]

---

```{r}
boxplot(fare ~ pclass + embarked + sex, data=combi, subset=pclass==1 & sex=="female"); abline(h=80)
combi$embarked[which(is.na(combi$embarked))] <- c("C","C")
```

---

# Età (*age*)

```{r}
# Età come funzione di pclass e sex
aggregate(age ~ pclass + sex, combi, FUN=mean)
# Modello lineare
fit.age <- lm(age ~ sex*pclass, 
          data = combi[!is.na(combi$age),])
# Sostituzione dei valori mancanti
combi$age[is.na(combi$age)] <- predict(fit.age, 
          newdata=combi[is.na(combi$age),])
```

---

# Divisione in training and test 

```{r}
train <- combi[1:n,]
test <- combi[(n+1):(n+m),]
```

---

layout: false
class: inverse, middle, center

# EDA e costruzione del modello

---

# Modello nullo 

* Training set: 38.38% di passeggeri è sopravvissuto, 61.62% no
* Il modello nullo utilizza solo $y$ e prevede 'nessun sopravvissuto'

```{r}
round(mean(train$survived01),2)
yhat <- rep("Death",m)
# matrice di confusione
table(Predicted=yhat, True=testsurvived)
# accuratezza
mean(yhat == testsurvived)
```

---

# Genere (*sex*)

.pull-left[

Prima le donne?

```{r, eval=F}
plot(survived ~ sex, train)
```
]

.pull-right[

```{r, echo=F}
plot(survived ~ sex, train)
```
]

---

```{r}
# modello logistico con solo il genere
fit <- glm(survived ~ sex, train, family="binomial")
phat <- predict(fit, newdata=test, type = "response")
yhat <- ifelse(phat > 0.5, "Alive","Death")
# matrice di confusione 
table(Predicted=yhat, True=testsurvived)
# accuratezza
mean(yhat == testsurvived)
```

---

# Età

Prima i bambini? Qual è la relazione tra età e sopravvivenza?

```{r}
summary(glm(survived ~ age, train, family="binomial"))$ coefficients 
```

---


.pull-left[
```{r, echo=FALSE}
ageclass = cut(train$age, breaks = c(0,10,20,30,40,50,60,70,80))
barplot(prop.table(table(train$survived,ageclass),2), xlab="Age class")
```
]



.pull-right[
```{r, echo=F}
plot(survived ~ age, train)
```
]

---

# Classe

.pull-left[

Le presone benestanti hanno maggiore probabilità di sopravvivenza?

```{r, eval=F}
plot(survived~pclass, train)
```
]

.pull-right[
```{r, echo=F}
plot(survived~pclass, train)
```
]

---

```{r}
# modello logistico con solo pclass
fit <- glm(survived ~ pclass, train, family="binomial")
phat <- predict(fit, newdata=test, type = "response")
yhat <- ifelse(phat > 0.5, "Alive","Death")
# matrice di confusione
table(Predicted=yhat, True=testsurvived)
# accuratezza
mean(yhat == testsurvived)
```

---

# Età e classe

```{r, echo=F}
class.jitter <- as.numeric(train$pclass)+.7*runif(n)
plot(age ~ class.jitter,xlim=c(.95,3.8),cex=1, xlab="Passenger class",xaxt="n", train)
axis(side=1,at=c(1.4,2.4,3.4),label=c("1st","2nd","3rd"))
points(age[survived01==0] ~ class.jitter[survived01==0],pch=19, train)
```

---

# Albero di classificazione

```{r, echo=F}
class.jitter <- as.numeric(train$pclass)+.7*runif(n)
plot(age ~ class.jitter,xlim=c(.95,3.8),cex=1, xlab="Passenger class",xaxt="n", train)
axis(side=1,at=c(1.4,2.4,3.4),label=c("1st","2nd","3rd"))
points(age[survived01==0] ~ class.jitter[survived01==0],pch=19, train)
abline(v=2.85)
rect(1.85,18,4.0,89,col=rgb(0.5,0.5,0.5,1/4))
rect(2.85,-5,4.0,89,col=rgb(0.5,0.5,0.5,1/4))
```


---

# Regola decisionale

.pull-left[
```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(survived ~ pclass + age, train, control=rpart.control(maxdepth =  3))
rpart.plot(fit, type=0, extra=2)
```
]

.pull-right[
| Status | Pr(Death) | Prediction |
|-----------|-----------|------|
| Class 3 |  76% | Death |
| Class 1-2, younger than 18 |  9% | Alive |
| Class 2, older than 18 |  56% | Death |
| Class 1, older than 18 |  39% | Alive | 
]

---


```{r}
yhat <- predict(fit, newdata=test, type="class")
# matrice di confusione
table(Predicted=yhat, True=testsurvived)
# accuratezza
mean(yhat == testsurvived)
```

---

# Genere e classe

.pull-left[
```{r, echo=F}
plot(survived~pclass, train, subset=sex=="male", main="male")
```
]

.pull-right[
```{r, echo=F}
plot(survived~pclass, train, subset=sex=="female", main="female")
```
]

---

# Genere e età

.pull-left[
```{r ,echo=F}
plot(survived~age, train, subset=sex=="male", main="male")
```
]

.pull-right[
```{r, echo=F}
plot(survived~age, train, subset=sex=="female", main="female")
```
]


---

# Bontà delle previsioni

Logistic model

| Predictors | Acc.Tr | Acc.Te |
|------------|-------|--------|
| -  |   61.6% |  62.2% |
| age | 61.6% | 62.2% |
| pclass | 67.9% | 67.2% |
| sex | 78.7% | 76.6% |
| age + pclass | 69.1% | 65.3% |
| age + sex | 78.7% | 76.6% |
| pclass + sex | 78.7% | 76.5% |
| age + pclass + sex | 79.5% | 75.6% |

---

layout: false
class: inverse, middle, center

# Feature engineering

---

# Titolo

```{r}
combi$name[1]
library(dplyr)
library(stringr)
combi <- combi %>% 
     mutate(title = str_extract(name, "[a-zA-Z]+\\."))
table(combi$title)
combi$title <- factor(combi$title)
```

---

```{r}
plot(survived ~ title, combi[1:n,])
```

---

# Uomo, ragazzo o donna?

.pull-left[
```{r, eval=F}
combi$wbm <- "man"
combi$wbm[grep('Master',combi$name)] <- "boy"
combi$wbm[combi$sex=="female"] <- "woman"
combi$wbm <- factor(combi$wbm)
plot(survived ~ wbm, combi[1:n,])
```
]

.pull-right[
```{r, echo=F}
combi$wbm <- "man"
combi$wbm[grep('Master',combi$name)] <- "boy"
combi$wbm[combi$sex=="female"] <- "woman"
combi$wbm <- factor(combi$wbm)
plot(survived ~ wbm, combi[1:n,])
```
]


---

# Cabina

.pull-left[
```{r}
# the first character of cabin is the deck
table(substr(combi$cabin, 1, 1))
```
]

.pull-right[
![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Titanic_cutaway_diagram.png/322px-Titanic_cutaway_diagram.png)
]

---

# Nucleo familiare

.pull-left[
```{r, eval=F}
combi$familysize <- combi$sibsp + combi$parch + 1
plot(survived ~ familysize, combi[1:n,])
```
]

.pull-right[
```{r, echo=F}
combi$familysize <- combi$sibsp + combi$parch + 1
plot(survived ~ familysize, combi[1:n,])
```
]

---

# Frequenze dei cognomi

.pull-left[
```{r, eval=F}
combi$surname <- substring(combi$name,0,regexpr(",",combi$name)-1)
combi$surnameFreq <- ave(1:nrow(combi),combi$surname,FUN=length)
plot(survived ~ surnameFreq, combi[1:n,])
```
]

.pull-right[
```{r, echo=F}
combi$surname <- substring(combi$name,0,regexpr(",",combi$name)-1)
combi$surnameFreq <- ave(1:nrow(combi),combi$surname,FUN=length)
plot(survived ~ surnameFreq, combi[1:n,])
```
]



---

# Sopravvivenza secondo i cognomi

.pull-left[
```{r, eval=F}
train = combi[1:n,]
plot(survived ~ as.factor(surname), train[train$surnameFreq==5,])
```
]

.pull-right[
```{r, echo=F}
train = combi[1:n,]
plot(survived ~ as.factor(surname), train[train$surnameFreq==5,])
```
]



---

# Modello con genere e famiglia

| Rule | Prediction |
|---|----|
| Man | Death |
| Boy and all females and boys in his family live | Alive |
| Boy and not all females and boys in his family live | Death |
| Female and all females and boys in her family die | Death |
| Female and not all females and boys in her family die | Alive |

---

# Confronto tra modellu

| Model | Acc.Tr | Acc.Te |
|---|----|----|
| All-dead | 61.6% |  62.2% |
| Gender-only | 78.7% | 76.6% |
| Gender surname | 85.5% | 80.1% |




