---
title: "Data Mining"
subtitle: "Bagging e Foreste Casuali"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Combinazione di modelli

In molte situazioni concrete, diversi modelli sembrano adattarsi bene ai dati e non sembra essercene uno di preferibile.

In maniera analoga e forse più evidente, la previsione attraverso metodi più instabili, come ad esempio alberi decisionali, è fortemente influenzata dalla scelta specifica dell'insieme di dati usato per la stima. 

Se tale insieme viene modificato di poco, si può ottenere un modello completamente diverso dall'originale, con circa lo stesso errore di previsione. Molti modelli possono quindi, risultare simili in termini di errore di previsione, ma molto diversi in quanto a forma.

Per tentare di migliorare la capacità previsiva di ciascun modello, una possibilità è quella di combinare le previsioni ottenute da modelli diversi. 

L'apprendimento di insieme (*ensemble learning*) produce un modello che in qualche maniera raccoglie le qualità delle singole componenti e per questo spesso porta a previsioni più accurate.

Per ottenere una combinazione di modelli, ci sono diversi approcci: in questa lezione vedremo il *bagging* e la foresta casuale (*random forest*).

Tuttavia, i metodi di *ensemble* richiedono un maggiore costo computazionale e spesso si perde l'interpretabilità dei risultati.

---

# Instabilità degli alberi decisionali

Uno degli svantaggi principali degli alberi decisionali è che sono piuttosto
instabili (hanno variabilità elevata). Spesso una piccola modifica nei dati di training si traduce nella costruzione di un albero completamente diverso. 

La ragione principale di 
questa instabilità è che se ad un certo livello dell'alberto cambia la regola di suddivisione (*split*), cambiano anche tutte suddivisioni sottostanti, propagando così la variabilità.


---

# Il bootstrap 

Il *bootstrap* è una procedura di ricampionamento. Un campione bootstrap di ampiezza $n$ ottenuto a partire dai dati di training è dato da
$$(\tilde{x}_1, \tilde{y}_1), (\tilde{x}_2, \tilde{y}_2), \ldots, (\tilde{x}_n, \tilde{y}_n)$$
dove ciascuna osservazione $(\tilde{x}_i, \tilde{y}_i)$ si ottiene estraendo con ripetizione le osservazioni dei dati di training $$(x_1, y_1), (x_2, y_2), \ldots, (x_n, x_n)$$ 

Non tutte le osservazioni del training set sono presenti in un campione bootstrap e alcune sono presenti più volte. La probabilità che un'osservazione originale non venga selezionata in una singola estrazione è $1 - \ frac{1}{n}$. Per $n$ elevato, la probabilità che un'osservazione originale non venga estratta in nessuna delle $n$ estrazioni è
$$\lim_{n\rightarrow \infty} \left(1- \frac{1}{n}\right)^n = \frac{1}{e} \approx 0.368$$
Ci possiamo quindi aspettare che circa $\approx 1/3$ delle $n$ osservazioni originali saranno escluse dal campione bootrap (*out-of-bag*, OOB).

---

```{r}
rm(list=ls())
n = 1000
original = 1:n
set.seed(123)
bagged = sample(original, size=n, replace=TRUE)
# proporzione di osservazioni OOB
length(setdiff(original,bagged))/n
# frequenza delle osservazioni originali
table(table(bagged))
```

---

# Bagging 

*Bootstrap AGGregation*, o *bagging*, è una procedura generale per ridurre la variabilità di un modello, e risulta particolarmente utile nel caso di alberi di regressione o di classificazione. 

L'algoritmo prevede di

1. Generare $B$ campioni boostrap dal training set
$$(\tilde{x}_1^b, \tilde{y}_1^b), (\tilde{x}_2^b, \tilde{y}_2^b), \ldots, (\tilde{x}_n^b, \tilde{y}_n^b), \qquad b=1,\ldots,B$$

2. Stimare un albero di regressione $\hat{f}^{b}$ o di classificazione $\hat{c}^{b}$ utilizzando ciascun campione bootstrap;

3. Calcolare la media delle previsioni
$$\bar{f}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{b}(x)$$
per gli alberi di regressione o la moda
$$\bar{c}(x) = \mathrm{Mode}\{\hat{c}^{b}(x),b=1,\ldots,B\}$$
per gli alberi di classificazine (che rappresenta il voto di maggioranza, o *consensus*).

---

.pull-left[
![](images/bootstrap.jpg)
]

.pull-right[
Hastie, Tibshirani and Friedman (2009) [The Elements of Statical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/) (ESL) p. 284: $n = 30$ training data points, $p = 5$
predictors, and $K = 2$ classes. No pruning used in growing trees
]

---

ESL p. 285: Bagging helps decrease the misclassification rate of the classifier (evaluated on a large independent test set)

![](images/improved.jpg)



---

# Bagging per problemi di classificazione

Per un problema di classificazione con $K$ classi, è possibile stimare le probabilità di appartenza a ciascuna classe utilizzando il bagging.

Gli alberi di classificazione fornisco una stima delle probabilità di appartenza a ciascuna classe dato $x$: $\hat{p}_k^b(x), k=1,\ldots,K$. La stima bagging è data dalla semplice media 
$$\bar{p}_k(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{p}_k^{b}(x), \quad k=1,\ldots,K$$
La classificazione finale del bagging corrisponde alla classe con maggiore probabilità.


Questa variante del bagging è preferibile se si desidera ottenere stime delle probabilità di classe.

---

# Breiman (1996) Bagging Predictors. *Machine learning*

Confronto in termini di errore di classificazione tra l'albero decisionale (algoritmo CART, potatura dell'albero mediante convalida incrociata) e il classificatore *bagging* (con $B=50$ campioni bootstrap).


| Data set | CART | Bagging |
|---|---|----|---|
| waveform | 29.1 | 19.3 |
| heart | 4.9 | 2.8 |
| breast cancer | 5.9 | 3.7 |
| ionosphere | 11.2 | 7.9 |
| diabetes | 25.3 | 23.9 |
| glass | 30.4 | 23.6 |
| soybean | 8.6 | 6.8 |

---

# Stima dell'errore di previsione mediante osservazioni *OOB*

Un albero decisionale costruito su un campione boostrap utilizza circa $\approx 2/3$ delle osservazioni originali.

Possiamo quindi prevedere la risposta per l' $i$-sima osservazione utilizzando gli alberi per i quali quell'osservazione risulta OOB.

Questo produce circa $B/3$ previsioni per l'osservazione $i$-sima, che combiniamo calcolando la media (delle previsioni o della probabilità stimate) o la moda. 

Si può quindi stimare l'errore di previsione senza ricorrere a soluzioni come la convalida incrociata.

---

ESL p. 592:

![](images/OOB.jpg)

---
layout: false
class: inverse, middle, center

# Spam data

---

Dear Google User,

You have been selected as a winner for using our free services !!!

Find attached email with more details. 

Its totally free and you won't be sorry !!!

Congratulations !!!

Matt Brittin. CEO Google UK

---

# Dati Spam


4601 email inviate a `George`, un dipendente di Hewlett-Packard

George ha etichettato 1813 e-mail come `spam`, mentre le rimandenti come `good` 

L'obiettivo è di costruire un filtro spam per George, ovvero prevedere se una e-mail è  `spam` oppure `good`

Per ciascuna email sono state registrate le frequenze relative di alcune parole chiave
e.g. `business` , `address` , `free` ,  `George`, etc.
e di certi caratteri: ( , [ , ! , $ , \#. Sono incluse anche tre diverse registrazioni di lettere maiuscole

Si tratta di un dataset accessibile al pubblico, disponibile presso  [UC Irvine data repository](archive.ics.uci.edu/ml/datasets/Spambase).  Ulteriori informazioni su questi dati sono disponibili [qui](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)

Per questo problema non tutti gli errori sono uguali: è preferibile evitare di filtrare
email "buone", mentre lasciare passare lo spam non è desiderabile ma ha conseguenze meno gravi

Per questo motivo valuteremo le previsioni sia in termini di accuratezza che di specificità:
$$\frac{\# \mathrm{\,\,email\,\,buone\,\,previste\,\,correttamente}}{\# \mathrm{\,\,email\,\,buone}}$$

---


| | george | you | your | hp | free | hpl | ! | our | re | edu | remove |
|--|--|--|--|--|--|--|--|--|--|--|--|
| spam | 0.00 | 2.26 | 1.38 | 0.02 | 0.52 | 0.01 | 0.51 | 0.51 | 0.13 | 0.01 | 0.28 |
| email | 1.27 | 1.27 | 0.44 | 0.90 | 0.07 | 0.43 | 0.11 | 0.18 | 0.42 | 0.29 | 0.01 |

ESL, Table 1.1: Average percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email.

---

```{r}
## Caricamento dei dati e suddivisione in training e test
rm(list=ls())
spam <- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test = spam[spam$testid, -2]
n = nrow(train)
m = nrow(test)
```

---


```{r}
# Una funzione per calcolare l'accuratezza e la specificità 
score <- function(phat, truth, name="model") {
  ctable <- table(truth=truth,
                  yhat=(phat>0.5))
  accuracy <- sum(diag(ctable))/sum(ctable)
  specificity <- ctable[1,1]/sum(ctable[1,])
  data.frame(model=name, accuracy=accuracy, specificity=specificity)
}
```

---

# Modello nullo

```{r}
phat.null = rep(mean(train$spam=="spam"),m)
yhat.null = ifelse(phat.null>.5,"spam","email")
## matrice di confusione
table(Predicted=yhat.null, True=test$spam)
## score
score(phat.null, test$spam, name="null")
```

---

# Albero di classificazione

```{r}
library(rpart)
fit.tree= rpart(spam ~ .,train)
phat.tree = predict(fit.tree, newdata=test)[,"spam"]
yhat.tree = ifelse(phat.tree>.5,"spam","email")
score(phat.tree, test$spam, name="classification tree")
```

---

```{r}
library(rpart.plot)
rpart.plot(fit.tree, type=0, extra=1)
```

---

```{r}
phat = numeric(nrow(test))
acc = NULL
spec = NULL
B = 50
for (b in 1:B){
use = sample(n,size=n,rep=T)
fit.tree = rpart(spam ~.,data=train[use,], cp=0.0001)
phat = phat + predict(fit.tree, newdata=test, type="prob")[,"spam"]
yhat = ifelse(phat/b >.5,"spam","email")
acc = c(acc,mean(yhat == test$spam))
spec = c(spec, score(phat/b, test$spam, name="bagging")[3])
}
```


---

```{r}
plot(1:B, acc, xlab="Bagging iterations", ylab="Accuracy", type="l")
```

---

```{r}
plot(1:B, spec, xlab="Bagging iterations", ylab="Specificity", type="l")
```


---

# Foreste casuali

L'idea delle foreste casuali è rendere ancora più diversi gli alberi costruiti sui campioni boostrap.

Da un punto di vista algoritmico:

* Bagging modifica le righe del training set  (campionamento casuale con ripetizione delle osservazioni)

* La foresta casuale (*random forest*) modifica anche le colonne del training set (sottocampione casuale senza reinserimento dei predittori)

---

# Il parametro di *tuning* della foresta casuale

Prima di effettuare la suddivisione in corrispondenza di un certo nodo dell'albero (*split*), seleziona a caso $m$ predittori come possibili candidati per lo *split*, con $1 \leq m \leq p$. 

Quindi, per far crescere l'albero, anziché esplorare per ciascun nodo tutti i possibili $p$ predittori per decidere quale utilizzare per effettuare lo *split*,  vengono esplorati solo $m$ predittori scelti a caso. 

$m$ rappresenta il parametro di *tuning* della foresta casuale. Solitamente 
* $m = \sqrt{p}$ per problemi di classificazione; 
* $m=p/3$ problemi di regressione.

Si noti che la scelta $m=p$ corrisponde al *bagging* come caso particolare di una foresta casuale. 


---

# Perchè le foreste casuali funzionano?

Si consideri per semplicità un problema di regressione. Sia $\hat{T}_D$ un albero di regressione stimato sul training set $D=\{(X_i,Y_i), i=1,\ldots,n\}$. Sia $\bar{T} = \mathbb{E} (\hat{T}_D)$ e supponiamo di voler prevedere l'osservazione $(X^*,Y^*)$ del test set, con $(X^*,Y^*) \stackrel{d}{=}(X_1,Y_1)$.
Possiamo quindi scomporre l'errore di previsione di $\hat{T}_D$ come
$$
\begin{aligned}
\underbrace{\mathbb{E}[\{Y^* - \hat{T}_D(X^*) \}^2]}_{\mathrm{Errore\,\,di\,\,previsione\,\,di\,\,}\hat{T}_D} &= \mathbb{E}[\{Y^* - \underbrace{\bar{T}(X^*)}_{\mathbb{E}(\hat{T}_D(X^*)|X^*)} + \bar{T}(X^*) - \hat{T}_D(X^*) \}^2]\\
&= \mathbb{E}[\{Y^* - \bar{T}(X^*) \}^2] + \mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}\\
&+ \underbrace{2 \mathbb{E}\{ \mathbb{E}[\{Y-\bar{T}(X^*)\}\{\bar{T}(X^*) - \hat{T}_D(X^*)\}| X^*]}_{=0}\}\\
& = \underbrace{\mathbb{E}[\{Y^* - \bar{T}(X^*) \}^2]}_{\mathrm{Errore\,\,di\,\,previsione\,\,di\,\,}\bar{T}} + \mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}
\end{aligned}
$$

La foresta casuale cerca di "stimare" $\bar{T}$.

Se l'albero $\hat{T}_D$ è complesso (non l'abbiamo potato), ci aspettiamo $\mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}$ elevato, ma l'errore di previsione di $\bar{T}$ dovrebbe risultare basso perché $\bar{T}$ riesce ad approssimare bene $x^* \mapsto \mathbb{E}(Y^*|X^*=x^*)$


---

# La foresta casuale riduce la correlazione tra gli alberi

Il sottocampionamento dei predittori serve a rendere gli alberi $\hat{T}_{D^*_{b}}, b=1,\ldots,B$ meno correlati fra di loro (più vicini all'indipendenza). 

Ipotizzando che gli alberi $\hat{T}_{D^*_b}, b=1,\ldots,B$ siano v.c. identicamente distribuite (ma non necessariamente indipendenti) con correlazione $\mathbb{C}\mathrm{orr}(\hat{T}_{D^*_b}, \hat{T}_{D^*_{\tilde{b}}}) = \rho\geq 0$ per $b\neq \tilde{b}$, risulta (vedi la prossima slide)
$$
\begin{aligned}
\mathbb{V}\mathrm{ar}[\bar{T}_{\mathrm{rf}}(x)] &= \frac{1-\rho}{B} \mathbb{V}\mathrm{ar}[\hat{T}_{D^*_b}(x)] + \rho \mathbb{V}\mathrm{ar}[\hat{T}_{D^*_b}(x)]
\end{aligned}
$$
Il primo termine può essere reso piccolo a piacere aumentando $B$, mentre il secondo termine non dipende da $B$, quindi è preferibile avere $\rho$ più piccolo possibile. 

Il sottocampionamento dei predittori previsto nell'algoritmo della foresta casuale serve a diminuire $\rho$.


---

Siano $Z_1,\ldots,Z_B$ v.c. identicamente distribuite (ma non necessariamente indipendenti) $Z_1,\ldots,Z_B$ con correlazione $\mathbb{C}\mathrm{orr}(Z_j, Z_l) = \rho$, media $\mathbb{E}(Z_j) = \mu$ e varianza $\mathbb{V}\mathrm{ar}(Z_j) = \sigma^2$,. Allora
$$\mathbb{V}\mathrm{ar}(\bar{Z}) =  \frac{(1-\rho)}{B} \sigma^2 + \rho \sigma^2$$

*  $\displaystyle \rho = \frac{1}{\sigma^2}[ \mathbb{E}(Z_i Z_j) - \mathbb{E}(Z_i) \mathbb{E}(Z_j)]$

* $\mathbb{E}(Z_i Z_j) = \rho \sigma^2 + \mu^2$ if $i\neq j$ 

* $\mathbb{E}(Z_i^2) = \sigma^2 + \mu^2$

* $\displaystyle \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] = \sum_{i=1}^{B}\sum_{j=1}^B \mathbb{E}(Z_i Z_j)  =  B \mathbb{E}(Z_i^2) + (B^2-B)\mathbb{E}(Z_i Z_j)$

* $\displaystyle \mathbb{E}(\sum_{j=1}^{B}Z_j) = \sum_{j=1}^{B} \mathbb{E}(Z_j)=  B \mu$

* $\displaystyle \mathbb{V}\mathrm{ar}(\bar{Z}) = \frac{1}{B^2} \mathbb{V}\mathrm{ar}(\sum_{j=1}^{B}Z_j) =  \frac{1}{B^2} \{ \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] - [ \mathbb{E}(\sum_{j=1}^{B}Z_j) ]^2  \}$

---

ESL p. 589 Bagging and random forest applied to the
spam data

```{r, echo=FALSE, fig.align = 'center', out.width = '70%', out.height = '70%'}
knitr::include_graphics("images/randomforest.jpg")
```


---

# La funzione randomForest()

```{r, eval=FALSE}
randomForest(formula, 
  data = ,
  ntree = , # default 500
  mtry = , # default sqrt(p) o p/3
  nodesize = , # default 1 o 5
  importance = ) # default: FALSE
```

Per il *bagging* o per le foreste casuali, gli alberi vengono fatti crescere senza potatura, e il numero minimo di osservazioni per nodo - `nodesize` -  è per defualt $=1$ per problemi di classificazione e $=5$ per problemi di regressione

Il numero di predittori selezionati casualmente - `mtry` - è pari a $=\sqrt{p}$ per problemi di classificazione e $=p/3$ per problemi di regressione. Si noti che `mtry` $=p$ corrisponde al *bagging*

---

```{r}
library(randomForest)
fit.rf <- randomForest(spam ~ ., data = train, ntree = B, importance=T)
phat.rf <- predict(fit.rf, newdata=test, type="prob")[,"spam"]
yhat.rf <- predict(fit.rf, newdata=test)
table(pred=yhat.rf, truth=test$spam)
score(phat.rf, test$spam, name="random forest")
```

---

```{r}
# stima dell'errore out-of-bag
plot(fit.rf)
```


---

# Importanza dei predittori

La funzione `randomForests` permette di calcolare delle misure di "importanza" dei predittori (argomenti `importance=TRUE`). 

La misura più interessante corrispondente all'argomento `type=1`, e viene calcolata permutando le osservazioni OOB: 

* Per ciascun albero, viene registrato l'errore di previsione per le osservazioni OOB;

* Poi si fa la stessa operazione però dopo aver permutato i valori del predittore;

* Infine si calcola la differenza tra queste due misure d'errore, considerando tutti gli alberi e normalizzando con la deviazione standard della differenza


---

[Efron and Hastie (2016) Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) (CASI) p. 332

```{r, echo=FALSE, fig.align = 'center', out.width = '70%', out.height = '70%'}
knitr::include_graphics("images/varimp.jpg")
```

---

```{r}
varImpPlot(fit.rf, type=1)
```


---

# Riepilogo sulle foreste casuali

Le foreste casuali sono semplici da usare e spesso efficaci per fare previsioni. I punti di forza sono

* Funzionano con predittori numerici o categoriali
* Tollerano abbastanza bene l'inclusione di predittori irrilevanti
* Tollerano abbastanza bene predittori fortemente correlati
* Gli alberi decisionali possono gestire la presenza di dati mancanti (ad esempio si veda [qui](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) per l'algoritmo CART implementato nella funzione `rpart`). Per le foreste casuali con la funzione `randomForests`, bisogna specificare l'argomento ` na.action = na.roughfix` (che sostituisce i valori mancanti con la mediana o la moda)

---

```{r}
library("caret")
cv <- trainControl(
  method = "cv",
  number = 10,
  ## Estimate class probabilities
  classProbs = TRUE,
  ## Classification metrics
  summaryFunction = twoClassSummary
)
```

---

```{r, message=FALSE, warning=FALSE}
mtryGrid = data.frame(mtry=c(2,29,57))
rf <- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  tuneGrid = mtryGrid,
  localImp = TRUE,
  trControl=cv)
```

---

```{r}
rf
```

---

```{r}
plot(varImp(rf))
```

---

```{r, message=FALSE, warning=FALSE}
bagging <- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  tuneGrid = data.frame(mtry=57),
  localImp = TRUE,
  trControl=cv)

models = list(
  bagging=bagging,
  rf=rf)
resamps = resamples(models)
```

---

```{r, message=FALSE, warning=FALSE}
bwplot(resamps)
```

---

```{r}
phats.all <- extractProb(models, testX = test, testY = test$spam)
phats <- subset(phats.all, dataType == "Test")
phats$model <- phats$object
plotClassProbs(phats)
```

