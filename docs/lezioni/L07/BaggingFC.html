<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Mining</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Mining
## Bagging e Foreste Casuali
### Aldo Solari

---





# Combinazione di modelli

In molte situazioni concrete, diverse modelli sembrano adattarsi bene ai dati e non sembra essercene uno di preferibile.

In maniera analoga e forse più evidente, la previsione attraverso metodi più instabili, come ad esempio alberi decisionali, è fortemente influenzata dalla scelta specifica dell'insieme di dati usato per la stima. 

Se tale insieme viene modificato di poco, si può ottenere un modello completamente diverso dall'originale, con circa lo stesso errore di previsione. Molti modelli possono quindi, risultare simili in termini di errore di previsione, ma molto diversi in quanto a forma.

Per tentare di migliorare la capacità previsiva di ciascun modello, una possibilità è quella di combinare le previsione ottenute da modelli diversi. 

L'apprendimento di insieme (*ensemble learning*) produce un modello che in qualche maniera raccoglie le qualità delle singole componenti e per questo spesso porta a previsioni più accurate.

Per ottenere una combinazione di modelli, ci sono diversi approcci: in questa lezione vedremo il *bagging* e la foresta casuale (*random forest*).

Tuttavia, i metodi di ensemble richiedono un maggiore costo computazionale e spesso si perde l'interpretabilità dei risultati.

---

# Instabilità degli alberi decisionali

Uno degli svantaggi principali degli alberi decisionali è che sono piuttosto
instabili (hanno variabilità elevata ). Spesso una piccola modifica nei dati di training si traduce nella costruzione di un albero completamente diverso. 

La ragione principale di 
questa instabilità è che se ad un certo livello dell'alberto cambia la regola di suddivisione (*split*), cambiano anche tutte suddivisioni sottostanti, propagando così la variabilità.


---

# Il bootstrap 

Il *bootstrap* è una procedura di ricampionamento. Un campione bootstrap di ampiezza `\(n\)` ottenuto a partire dai dati di training è dato da
`$$(\tilde{x}_1, \tilde{y}_1), (\tilde{x}_2, \tilde{y}_2), \ldots, (\tilde{x}_n, \tilde{y}_n)$$`
dove ciascuna osservazione `\((\tilde{x}_i, \tilde{y}_i)\)` si ottiene estraendo con ripetizione le osservazioni dei dati di training `$$(x_1, y_1), (x_2, y_2), \ldots, (x_n, x_n)$$` 

Non tutte le osservazioni del training set sono presenti in un campione bootstrap e alcune sono presenti più volte. La probabilità che un'osservazione originale non venga selezionata in una singola estrazione è `\(1 - \ frac {1} {n}\)`. Per `\(n\)` elevato, la probabilità che un'osservazione originale non venga estratta in nessuna delle `\(n\)` estrazioni è
`$$\lim_{n\rightarrow \infty} \left(1- \frac{1}{n}\right)^n = \frac{1}{e} \approx 0.368$$`
Ci possiamo quindi aspettare che circa `\(\approx 1/3\)` delle `\(n\)` osservazioni originali saranno escluse dal campione bootrap (*out-of-bag*, OOB).

---


```r
rm(list=ls())
n = 1000
original = 1:n
set.seed(123)
bagged = sample(original, size=n, replace=TRUE)
# proportion of OOB observations
length(setdiff(original,bagged))/n
```

```
[1] 0.362
```

```r
# frequency
table(table(bagged))
```

```

  1   2   3   4   5 
379 178  61  18   2 
```

---

# Bagging 

*Bootstrap AGGregation*, o *bagging*, è una procedura generale per ridurre la variabilità di un modello, e risulta particolarmente utile nel caso di alberi di regressione o di classificazione. 

L'algoritmo prevede di

1. Generare `\(B\)` campioni boostrap dal training set
`$$(\tilde{x}_1^b, \tilde{y}_1^b), (\tilde{x}_2^b, \tilde{y}_2^b), \ldots, (\tilde{x}_n^b, \tilde{y}_n^b), \qquad b=1,\ldots,B$$`

2. Stimare un albero di regressione `\(\hat{f}^{b}\)` o di classificazione `\(\hat{c}^{b}\)` utilizzando ciascun campione bootstrap;

3. Calcolare la media delle previsioni
`$$\bar{f}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}^{b}(x)$$`
per gli alberi di regressione o la moda
`$$\bar{c}(x) = \mathrm{Mode}\{\hat{c}^{b}(x),b=1,\ldots,B\}$$`
per gli alberi di classificazine (che rappresenta il voto di maggioranza, o *consensus*).

---

.pull-left[
![](images/bootstrap.jpg)
]

.pull-right[
Hastie, Tibshirani and Friedman (2009) [The Elements of Statical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/) (ESL) p. 284: `\(n = 30\)` training data points, `\(p = 5\)`
predictors, and `\(K = 2\)` classes. No pruning used in growing trees
]

---

ESL p. 285: Bagging helps decrease the misclassification rate of the classifier (evaluated on a large independent test set)

![](images/improved.jpg)



---

# Bagging per problemi di classificazione

Per un problema di classificazione con `\(K\)` classi, è possibile stimare le probabilità di appartenza a ciascuna classe utilizzando il bagging.

Gli alberi di classificazione fornisco una stima delle probabilità di appartenza a ciascuna classe dato `\(x\)`: `\(\hat{p}_k^b(x), k=1,\ldots,K\)`. La stima bagging è data dalla semplice media 
`$$\bar{p}_k(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{p}_k^{b}(x), \quad k=1,\ldots,K$$`
La classificazione finale del bagging corrisponde alla classe con maggiore probabilità.


Questa variante del bagging è preferibile se si desidera ottenere stime delle probabilità di classe.

---

# Breiman (1996) Bagging Predictors. *Machine learning*

Confronto in termini di errore di classificazione tra l'albero decisionale (algoritmo CART, potatura dell'albero mediante convalida incrociata) e il classificatore *bagging* (con `\(B=50\)` campioni bootstrap).


| Data set | CART | Bagging |
|---|---|----|---|
| waveform | 29.1 | 19.3 |
| heart | 4.9 | 2.8 |
| breast cancer | 5.9 | 3.7 |
| ionosphere | 11.2 | 7.9 |
| diabetes | 25.3 | 23.9 |
| glass | 30.4 | 23.6 |
| soybean | 8.6 | 6.8 |

---

# Stima dell'errore di previsione mediante osservazioni *OOB*

Un albero decisionale costruito su un campione boostrap utilizza circa `\(\approx 2/3\)` delle osservazioni originali.

Possiamo quindi prevedere la risposta per l' `\(i\)`-sima osservazione utilizzando gli alberi per i quali quell'osservazione risulta OOB.

Questo produce circa `\(B/3\)` previsioni per l'osservazione `\(i\)`-sima, che combiniamo calcolando la media (delle previsioni o della probabilità stimate) o la moda. 

Si può quindi stimare l'errore di previsione senza ricorrere a soluzioni come la convalida incrociata.

---

ESL p. 592:

![](images/OOB.jpg)

---
layout: false
class: inverse, middle, center

# Spam data

---

Dear Google User,

You have been selected as a winner for using our free services !!!

Find attached email with more details. 

Its totally free and you won't be sorry !!!

Congratulations !!!

Matt Brittin. CEO Google UK

---

# Dati Spam


4601 email inviate a `George`, un dipendente di Hewlett-Packard

George ha etichettato 1813 e-mail come `spam`, mentre le rimandenti come `good` 

L'obiettivo è di costruire un filtro spam per George, ovvero prevedere se una e-mail è  `spam` oppure `good`

Per ciascuna email sono state registrate le frequenze relative di alcune parole chiave
e.g. `business` , `address` , `free` ,  `George`, etc.
e di certi caratteri: ( , [ , ! , $ , \#. Sono incluse anche tre diverse registrazioni di lettere maiuscole

Si tratta di un dataset accessibile al pubblico, disponibile presso  [UC Irvine data repository](archive.ics.uci.edu/ml/datasets/Spambase).  Ulteriori informazioni su questi dati sono disponibili [qui](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)

Per questo problema non tutti gli errori sono uguali: è preferibile evitare di filtrare
email "buone", mentre lasciare passare lo spam non è desiderabile ma ha conseguenze meno gravi

Per questo motivo valuteremo le previsioni sia in termini di accuratezza che di specificità:
`$$\frac{\# \mathrm{\,\,email\,\,buone\,\,previste\,\,correttamente}}{\# \mathrm{\,\,email\,\,buone}}$$`

---


| | george | you | your | hp | free | hpl | ! | our | re | edu | remove |
|--|--|--|--|--|--|--|--|--|--|--|--|
| spam | 0.00 | 2.26 | 1.38 | 0.02 | 0.52 | 0.01 | 0.51 | 0.51 | 0.13 | 0.01 | 0.28 |
| email | 1.27 | 1.27 | 0.44 | 0.90 | 0.07 | 0.43 | 0.11 | 0.18 | 0.42 | 0.29 | 0.01 |

ESL, Table 1.1: Average percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email.

---


```r
## Caricamento dei dati e suddivisione in training e test
rm(list=ls())
spam &lt;- read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/SPAM.csv",header=T)
spam$spam = as.factor(ifelse(spam$spam==T,"spam","email"))
train = spam[!spam$testid, -2]
test = spam[spam$testid, -2]
n = nrow(train)
m = nrow(test)
```

---



```r
# Una funzione per calcolare l'accuratezza e la specificità 
score &lt;- function(phat, truth, name="model") {
  ctable &lt;- table(truth=truth,
                  yhat=(phat&gt;0.5))
  accuracy &lt;- sum(diag(ctable))/sum(ctable)
  specificity &lt;- ctable[1,1]/sum(ctable[1,])
  data.frame(model=name, accuracy=accuracy, specificity=specificity)
}
```

---

# Modello nullo


```r
phat.null = rep(mean(train$spam=="spam"),m)
yhat.null = ifelse(phat.null&gt;.5,"spam","email")
## matrice di confusione
table(Predicted=yhat.null, True=test$spam)
```

```
         True
Predicted email spam
    email   941  595
```

```r
## score
score(phat.null, test$spam, name="null")
```

```
  model  accuracy specificity
1  null 0.6126302           1
```

---

# Albero di classificazione


```r
library(rpart)
fit.tree= rpart(spam ~ .,train)
phat.tree = predict(fit.tree, newdata=test)[,"spam"]
yhat.tree = ifelse(phat.tree&gt;.5,"spam","email")
score(phat.tree, test$spam, name="classification tree")
```

```
                model  accuracy specificity
1 classification tree 0.8977865   0.9330499
```

---


```r
library(rpart.plot)
rpart.plot(fit.tree, type=0, extra=1)
```

![](BaggingFC_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---


```r
phat = numeric(nrow(test))
acc = NULL
spec = NULL
B = 50
for (b in 1:B){
use = sample(n,size=n,rep=T)
fit.tree = rpart(spam ~.,data=train[use,], cp=0.0001)
phat = phat + predict(fit.tree, newdata=test, type="prob")[,"spam"]
yhat = ifelse(phat/b &gt;.5,"spam","email")
acc = c(acc,mean(yhat == test$spam))
spec = c(spec, score(phat/b, test$spam, name="bagging")[3])
}
```


---


```r
plot(1:B, acc, xlab="Bagging iterations", ylab="Accuracy", type="l")
```

![](BaggingFC_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---


```r
plot(1:B, spec, xlab="Bagging iterations", ylab="Specificity", type="l")
```

![](BaggingFC_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;


---

# Foreste casuali

L'idea delle foreste casuali è rendere ancora più diversi gli alberi costruiti sui campioni boostrap.

Da un punto di vista algoritmico:

* Bagging modifica le righe del training set  (campionamento casuale con ripetizione delle osservazioni)

* La foresta casuale (*random forest*) modifica anche le colonne del training set (sottocampione casuale senza reinserimento dei predittori)

---

# Il parametro di *tuning* della foresta casuale

Prima di effettuare la suddivisione in corrispondenza di un certo nodo dell'albero (*split*), seleziona a caso `\(m\)` predittori come possibili candidati per lo *split*, con `\(1 \leq m \leq p\)`. 

Quindi, per far crescere l'albero, anziché esplorare per ciascun nodo tutti i possibili `\(p\)` predittori per decidere quale utilizzare per effettuare lo *split*,  vengono esplorati solo `\(m\)` predittori scelti a caso. 

`\(m\)` rappresenta il parametro di *tuning* della foresta casuale. Solitamente 
* `\(m = \sqrt{p}\)` per problemi di classificazione; 
* `\(m=p/3\)` problemi di regressione.

Si noti che la scelta `\(m=p\)` corrisponde al *bagging* come caso particolare di una foresta casuale. 


---

# Perchè le foreste casuali funzionano?

Si consideri per semplicità un problema di regressione. Sia `\(\hat{T}_D\)` un albero di regressione stimato sul training set `\(D=\{(X_i,Y_i), i=1,\ldots,n\}\)`. Sia `\(\bar{T} = \mathbb{E} (\hat{T}_D)\)` e supponiamo di voler prevedere l'osservazione `\((X^*,Y^*)\)` del test set, con `\((X^*,Y^*) \stackrel{d}{=}(X_1,Y_1)\)`.
Possiamo quindi scomporre l'errore di previsione di `\(\hat{T}_D\)` come
$$
`\begin{aligned}
\underbrace{\mathbb{E}[\{Y^* - \hat{T}_D(X^*) \}^2]}_{\mathrm{Errore\,\,di\,\,previsione\,\,di\,\,}\hat{T}_D} &amp;= \mathbb{E}[\{Y^* - \underbrace{\bar{T}(X^*)}_{\mathbb{E}(\hat{T}_D(X^*)|X^*)} + \bar{T}(X^*) - \hat{T}_D(X^*) \}^2]\\
&amp;= \mathbb{E}[\{Y^* - \bar{T}(X^*) \}^2] + \mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}\\
&amp;+ \underbrace{2 \mathbb{E}\{ \mathbb{E}[\{Y-\bar{T}(X^*)\}\{\bar{T}(X^*) - \hat{T}_D(X^*)\}| X^*]}_{=0}\}\\
&amp; = \underbrace{\mathbb{E}[\{Y^* - \bar{T}(X^*) \}^2]}_{\mathrm{Errore\,\,di\,\,previsione\,\,di\,\,}\bar{T}} + \mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}
\end{aligned}`
$$

La foresta casuale cerca di "stimare" `\(\bar{T}\)`.

Se l'albero `\(\hat{T}_D\)` è complesso (l'abbiamo lasciato crescere), ci aspettiamo `\(\mathbb{E}\{ \mathbb{V}\mathrm{ar}(\hat{T}_D(X^*)|X^*) \}\)` elevato, ma l'errore di previsione di `\(\bar{T}\)` potrebbe risultare basso se `\(\bar{T}\)` riesce ad approssimare bene `\(x^* \mapsto \mathbb{E}(Y^*|X^*=x^*)\)`


---

# La foresta casuale riduce la correlazione tra gli alberi

Il sottocampionamento dei predittori serve a rendere gli alberi `\(\hat{T}_{D^*_{b}}, b=1,\ldots,B\)` meno correlati fra di loro (più vicini all'indipendenza). 

Ipotizzando che gli alberi `\(\hat{T}_{D^*_b}, b=1,\ldots,B\)` siano v.c. identicamente distribuite (ma non necessariamente indipendenti) con correlazione `\(\mathbb{C}\mathrm{orr}(\hat{T}_{D^*_b}, \hat{T}_{D^*_{\tilde{b}}}) = \rho\geq 0\)` per `\(b\neq \tilde{b}\)`, risulta (vedi la prossima slide)
$$
`\begin{aligned}
\mathbb{V}\mathrm{ar}[\bar{T}_{\mathrm{rf}}(x)] &amp;= \frac{1-\rho}{B} \mathbb{V}\mathrm{ar}[\hat{T}_{D^*_b}(x)] + \rho \mathbb{V}\mathrm{ar}[\hat{T}_{D^*_b}(x)]
\end{aligned}`
$$
Il primo termine può essere reso piccolo a piacere aumentando `\(B\)`, mentre il secondo termine non dipende da `\(B\)`, quindi è preferibile avere `\(\rho\)` più piccolo possibile. 

Il sottocampionamento dei predittori previsto nell'algoritmo della foresta casuale serve a diminuire `\(\rho\)`.


---

Siano `\(Z_1,\ldots,Z_B\)` v.c. identicamente distribuite (ma non necessariamente indipendenti) `\(Z_1,\ldots,Z_B\)` con correlazione `\(\mathbb{C}\mathrm{orr}(Z_j, Z_l) = \rho\)`, media `\(\mathbb{E}(Z_j) = \mu\)` e varianza `\(\mathbb{V}\mathrm{ar}(Z_j) = \sigma^2\)`,. Allora
`$$\mathbb{V}\mathrm{ar}(\bar{Z}) =  \frac{(1-\rho)}{B} \sigma^2 + \rho \sigma^2$$`

*  `\(\displaystyle \rho = \frac{1}{\sigma^2}[ \mathbb{E}(Z_i Z_j) - \mathbb{E}(Z_i) \mathbb{E}(Z_j)]\)`

* `\(\mathbb{E}(Z_i Z_j) = \rho \sigma^2 + \mu^2\)` if `\(i\neq j\)` 

* `\(\mathbb{E}(Z_i^2) = \sigma^2 + \mu^2\)`

* `\(\displaystyle \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] = \sum_{i=1}^{B}\sum_{j=1}^B \mathbb{E}(Z_i Z_j)  =  B \mathbb{E}(Z_i^2) + (B^2-B)\mathbb{E}(Z_i Z_j)\)`

* `\(\displaystyle \mathbb{E}(\sum_{j=1}^{B}Z_j) = \sum_{j=1}^{B} \mathbb{E}(Z_j)=  B \mu\)`

* `\(\displaystyle \mathbb{V}\mathrm{ar}(\bar{Z}) = \frac{1}{B^2} \mathbb{V}\mathrm{ar}(\sum_{j=1}^{B}Z_j) =  \frac{1}{B^2} \{ \mathbb{E}[(\sum_{j=1}^{B}Z_j)^2] - [ \mathbb{E}(\sum_{j=1}^{B}Z_j) ]^2  \}\)`

---

ESL p. 589 Bagging and random forest applied to the
spam data

&lt;img src="images/randomforest.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;


---

# La funzione randomForest()


```r
randomForest(formula, 
  data = ,
  ntree = , # default 500
  mtry = , # default sqrt(p) o p/3
  nodesize = , # default 1 o 5
  importance = ) # default: FALSE
```

Per il *bagging* o per le foreste casuali, gli alberi vengono fatti crescere senza potatura, e il numero minimo di osservazioni per nodo - `nodesize` -  è per defualt `\(=1\)` per problemi di classificazione e `\(=5\)` per problemi di regressione

Il numero di predittori selezionati casualmente - `mtry` - è pari a `\(=\sqrt{p}\)` per problemi di classificazione e `\(=p/3\)` per problemi di regressione. Si noti che `mtry` `\(=p\)` corrisponde al *bagging*

---


```r
library(randomForest)
fit.rf &lt;- randomForest(spam ~ ., data = train, ntree = B, importance=T)
phat.rf &lt;- predict(fit.rf, newdata=test, type="prob")[,"spam"]
yhat.rf &lt;- predict(fit.rf, newdata=test)
table(pred=yhat.rf, truth=test$spam)
```

```
       truth
pred    email spam
  email   909   44
  spam     32  551
```

```r
score(phat.rf, test$spam, name="random forest")
```

```
          model  accuracy specificity
1 random forest 0.9511719   0.9670563
```

---


```r
# stima dell'errore out-of-bag
plot(fit.rf)
```

![](BaggingFC_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;


---

# Importanza dei predittori

La funzione `randomForests` permette di calcolare delle misure di "importanza" dei predittori (argomenti `importance=TRUE`). 

La misura più interessante corrispondente all'argomento `type=1`, e viene calcolata permutando le osservazioni OOB: 

* Per ciascun albero, viene registrato l'errore di previsione per le osservazioni OOB;

* Poi si fa la stessa operazione però dopo aver permutato i valori del predittore;

* Infine si calcola la differenza tra queste due misure d'errore, considerando tutti gli alberi e normalizzando con la deviazione standard della differenza


---

[Efron and Hastie (2016) Computer Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) (CASI) p. 332

&lt;img src="images/varimp.jpg" width="70%" height="70%" style="display: block; margin: auto;" /&gt;

---


```r
varImpPlot(fit.rf, type=1)
```

![](BaggingFC_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;


---

# Riepilogo sulle foreste casuali


Le foreste casuali sono semplici da usare e spesso efficaci per fare previsioni. I punti di forza sono

* Funzionano con predittori numerici o categoriali
* Tollerano abbastanza bene l'inclusione di predittori irrilevanti
* Tollerano abbastanza bene predittori fortemente correlati
* Gli alberi decisionali possono gestire la presenza di dati mancanti (ad esempio si veda [qui](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) per l'algoritmo CART implementato nella funzione `rpart`). Per le foreste casuali con la funzione `randomForests`, bisogna specificare l'argomento ` na.action = na.roughfix` (che sostituisce i valori mancanti con la mediana o la moda)

---


```r
library("caret")
cv &lt;- trainControl(
  method = "cv",
  number = 10,
  ## Estimate class probabilities
  classProbs = TRUE,
  ## Classification metrics
  summaryFunction = twoClassSummary
)
```

---


```r
mtryGrid = data.frame(mtry=c(2,29,57))
rf &lt;- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  tuneGrid = mtryGrid,
  localImp = TRUE,
  trControl=cv)
```

---


```r
rf
```

```
Random Forest 

3065 samples
  57 predictor
   2 classes: 'email', 'spam' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2759, 2758, 2758, 2760, 2759, 2758, ... 
Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec     
   2    0.9812146  0.9696798  0.9031297
  29    0.9802617  0.9583079  0.9195434
  57    0.9772993  0.9610106  0.9137786

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.
```

---


```r
plot(varImp(rf))
```

![](BaggingFC_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---


```r
bagging &lt;- train(
  spam~., train,
  ntree = 50,
  method = "rf",
  tuneGrid = data.frame(mtry=57),
  localImp = TRUE,
  trControl=cv)

models = list(
  bagging=bagging,
  rf=rf)
resamps = resamples(models)
```

---


```r
bwplot(resamps)
```

![](BaggingFC_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---


```r
phats.all &lt;- extractProb(models, testX = test, testY = test$spam)
phats &lt;- subset(phats.all, dataType == "Test")
phats$model &lt;- phats$object
plotClassProbs(phats)
```

![](BaggingFC_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
