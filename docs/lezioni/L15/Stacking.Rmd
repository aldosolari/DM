---
title: "Data Mining"
subtitle: "Model stacking"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```


# Model stacking

Stacking is a general method to combine models 

Consider a __library__ of $L$ models $\hat{f}_1,\ldots, \hat{f}_L$ fitted on the training data
$$(x_1,y_1),\ldots, (x_n,y_n)$$


Perhaps by combining their respective efforts, we could get even better prediction than using any
particular one

The issue then how we might combine them for predicting the test set $y^*_1,\ldots,y^*_m$

A linear combination 
$$\hat{y}^{*}_i = \sum_{l=1}^{L}w_l \hat{f}_l(x^*_i)$$
requires to define the the weights $w_1,\ldots,w_L$

---

# Least squares

The method of least squares provides the weights 
$$\hat{w}_1,\ldots,\hat{w}_L=  \underset{w_1,\ldots,w_L}{\arg \min\,\,} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}_l(x_i) \right]^2$$

However, in this way we fail to take into account for model complexity: models with higher complexity get higher weights

For example, consider $L$ predictors and let $\hat{f}_{l}$ be the linear model formed the best subset of predictors of size $l$, $l=1,\ldots,L$, where best is defined as having the smallest $\mathrm{MSE}_{\mathrm{Tr}}$

Then all the weight goes on the largest model, that is, $\hat{w}_L = 1$ and $\hat{w}_l = 0$ for $l< L$

---

# Stacked regression

Wolpert (1992) presented an interesting idea, called __stacked generalizations__.  This proposal was translated in statistical language by Breiman, in 1993

If we exclude $y_i$ in the fitting procedure of the models, then 
 $\hat{f}^{-i}_1(x_i),\ldots, \hat{f}^{-i}_L(x_i)$ do not depend on $y_i$ 

__Stacked regression__ is a particular case of the model stacking algorithm (next) with $\hat{f}_{\mathrm{stack}}=$ linear model and cross-validation with $K=n$

Read [Guide to Model Stacking](https://gormanalysis.com/guide-to-model-stacking-i-e-meta-ensembling/
) by Ben Gorman

---

# Staked regression algorithm

1. Let $\hat{f}^{-i}_l(x_i)$ be the prediction at $x_i$ using model $l$ fitted to the training data with the
$i$th training observation $(x_i,y_i)$ removed

2. Obtain the weights by least squares
$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$

3. Compute the predictions for the test data as
$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$

---

# Boston data

```{r}
rm(list=ls())
library(MASS)
set.seed(123)
istrain = rbinom(n=nrow(Boston),size=1,prob=0.5)>0
train <- Boston[istrain,]
n=nrow(train)
test = Boston[!istrain,-14]
test.y = Boston[!istrain,14]
m=nrow(test)
```

The training and test data are 
$$(x_1,y_1),\ldots,(x_n,y_n),\quad (x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$$
with $n=235$ and $m=271$ for the Boston data set. 

The response variable is `medv`, and the
predictor variables are `crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat` 

---

Fit a library of $L=2$ models $\hat{f}_1$ and $\hat{f}_2$ to the training set.

The first model $\hat{f}_1$ is a linear model with all predictors

```{r}
fit1 = lm(medv ~ ., train)
```

The second model $\hat{f}_1$ is a regression tree with all predictors and default settings

```{r}
library(rpart)
fit2 = rpart(medv ~ ., train)
```

The mean squared error for the test set

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# 2.
z1 = vector()
z2 = z1
for (i in 1:n){
z1[i] = predict( lm(medv ~ (.), train[-i, ]), 
                 newdata = train[i,]
                 )
z2[i] = predict(rpart(medv ~ ., train[-i,]), 
                newdata = train[i,]
                ) 
}
# 3. 
fit = lm(medv ~ 0 + z1 + z2, train)
weights = coef(fit)
# 4. 
yhat1 = predict(fit1, newdata=test)
yhat2 = predict(fit2, newdata=test)
yhat = weights[1]*yhat1 + weights[2]*yhat2
# 5.
cat("MSE stack: ", mean( (test.y - yhat)^2) ) 
cat("MSE lm: ", mean( (test.y - yhat1)^2) ) 
cat("MSE rpart: ", mean( (test.y - yhat2)^2) ) 
```




---

# Model stacking algorithm

1. Partition the training data into $K$ folds $\mathcal{F}_1,\ldots,\mathcal{F}_K$

2. For each test fold $\mathcal{F}_k$, $k=1,\ldots,K$ ,combine the other $K-1$ folds to be used as a training fold 
    - For $l=1,\ldots,L$,  fit the $l$th model to the training fold and make predictions on the test fold $\mathcal{F}_k$. Store these predictions $$z_i = (\hat{f}_1^{-\mathcal{F}_k}(x_i),\ldots,\hat{f}_L^{-\mathcal{F}_k}(x_i)), \quad  i \in \mathcal{F}_k$$

3. Fit the stacking model $\hat{f}_{\mathrm{stack}}$ using 
$$(y_1,z_1),\ldots, (y_n,z_n)$$

4. For $l=1,\ldots,L$,  fit the $l$th model to the full training data and make predictions on the test data. Store these predictions
$$z^*_i = (\hat{f}_1(x^*_i),\ldots,\hat{f}_L(x^*_i)), \quad i=1,\ldots,m$$
5. Make final predictions $\hat{y}^{*}_i = \hat{f}_{\mathrm{stack}}(z^*_i)$, $i=1,\ldots,m$

---


# caretEnsemble

See [A Brief Introduction to caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
) by Zach Mayer

```{r, eval=FALSE}
# K fold CV for regression
KCV <- trainControl(method="cv", 
                    number=K,
                    savePredictions="final",
                    index=createResample(train$y, K)
                    )
# list of models fit1 and fit2
List <- caretList(y ~. ,
                  data=train,
                  trControl=KCV,
                  methodList=c("fit1","fit2")
                  )
# ensemble fit
fit.ensemble <- caretEnsemble(List, metric="RMSE")
summary(fit.ensemble)
```



---

# Boston data

```{r, eval=FALSE}
# libraries
rm(list=ls())
library(MASS)
library(caret)
library(caretEnsemble)

# import data
set.seed(123)
split <- createDataPartition(y=Boston$medv, p = 0.5, list=FALSE)
train <- Boston[split,]
test = Boston[-split,-14]
test.y = Boston[-split,14]
nrow(test)

# cross-validation settings
K = 10
my_control <- trainControl(
  method="cv",
  number=K,
  savePredictions="final",
  index=createResample(train$medv, K)
  )
```

---

# Library of models

```{r, eval=FALSE}
model_list <- caretList(
  medv~., data=train,
  methodList=c("lm","ctree"), 
  tuneList=list(
    rf=caretModelSpec(method="rf", tuneLength=3)
  ),
  trControl=my_control
)

xyplot(resamples(model_list))
modelCor(resamples(model_list))
```

---

# Ensemble

```{r, eval=FALSE}
greedy_ensemble <- caretEnsemble(
  model_list, 
  metric="RMSE"
  )
summary(greedy_ensemble)
```

---

# Test MSE

```{r, eval=FALSE}
yhats <- lapply(model_list, predict, newdata=test)
lapply(yhats, function(yhat) mean((yhat - test.y)^2) )

yhat.en <- predict(greedy_ensemble, newdata=test)
mean((yhat.en - test.y)^2)
```

