---
title: "Data Mining"
subtitle: "Additive Models"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220))
```





# Additive models

The additive model for regression is that the conditional expectation function is a sum of partial response functions, one for each predictor variable
$$\mathbb{E}(Y|X=x) = \beta_0 + \sum_{j=1}^{p}f_j(x_j)$$

This includes the linear model as a special case, where
$$f_j(x_j) = \beta_j x_j$$
but it's
clearly more general, because the $f_j$'s can be arbitrary nonlinear functions

The idea is still that each input feature makes a separate contribution to the response,
and these just add up (hence "partial response function"), but these contributions donâ€™t have to be strictly proportional to the inputs

We do need to add a
restriction to make it identifiable; without loss of generality
$$\mathbb{E}(Y) = \beta_0, \qquad \mathbb{E}(f_j(X_j)) =0$$

How can we fit additive models?

---

Linear models assume
$$\mathbb{E}(Y|X=x) = \sum_{j=0}^{p} \beta_j x_j$$
where $x_0$ is always the constant 1

Suppose we don't condition on all $X=(X_0,\ldots,X_p)'$ but just one component of it, say $X_k$. Then 
$$\mathbb{E}(Y|X_k=x_k) =  \beta_k x_k + \mathbb{E}(\sum_{j\neq k} \beta_j X_j|X_k=x_k)$$

Then
$$\mathbb{E}(Y^{(k)}|X_k=x_k) =  \beta_k x_k$$
where $Y^{(k)} = Y - \sum_{j\neq k} \beta_j X_j$ the $k$th __partial residual__: the difference between $Y$ and what we expect it to be __ignoring__ the contribution from $X_k$

This suggests the following estimation scheme for linear models, known as the
__Gauss-Seidel algorithm__, or more commonly as __back-fitting__

---

* Given: 
    - $n\times p$ matrix $\mathbf{X}$ of $p$ predictors
    - $n\times 1$ response vector $\mathbf{y}$
    - small tolerance $\delta > 0$

* Center $\mathbf{y}$ and each column of $\mathbf{X}$

* Initialize $\hat{\beta}_{j} \leftarrow 0$ for $j=1,\ldots,p$

* Until (all $|\hat{\beta}_j - \gamma_j| \leq \delta$ ){

    for $k=1,\ldots, p$ {
    
      $y_i^{(k)} = y_i - \sum_{j\neq k} \hat{\beta}_jx_{ij}$
    
      $\gamma_k \leftarrow$ regression coefficient of $y^{(k)}$ on $x_k$
    
      $\hat{\beta}_j \leftarrow  \gamma_k$ }}

* $\hat{\beta}_{0} \leftarrow \bar{y} - \sum_{j=1}^{p} \hat{\beta}_j \bar{x}_{j}$ with original data

* Return $\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p$


---

Generate a response $Y$ and predictors $X_1$, $X_2$ and $X_3$ with $n = 100$ as follows:

```{r}
n = 100
set.seed(1)
x1 = rnorm(n)
x2 = rnorm(n)
x3 = rnorm(n)
y = 30 - 10*x1 + 20*x2 + 30*x3 + rnorm(n)
fit = lm(y ~ x1+x2+x3)
X = model.matrix(fit)
```

---

```{r}
# center y and X
yc = y - mean(y)
Xc = apply(X[,-1],2,function(x) x-mean(x))
# initialize 
p = ncol(Xc)
hatbeta = rep(0,p)
delta = 0.1
gamma = rep(2*delta,p)
while ( !all( abs(gamma - hatbeta) <= delta) ){
for (k in 1:p){
  gamma = hatbeta
  yk = yc - Xc[,-k, drop=F] %*% hatbeta[-k, drop=F]
  gammak = coef(lm(yk ~ 0+ Xc[,k]))
  hatbeta[k] <- gammak
}}
hatbeta0<- mean(y) - sum(hatbeta*apply(X[,-1],2,mean))
data.frame( backfitting=c(hatbeta0,hatbeta), leastsquares=coef(fit))
```

---

# Backfitting additive models

* Define the partial residuals by analogy with the linear case, as
$$Y^{(k)} = Y - \left(\beta_0 - \sum_{j\neq k} f_j(x_j) \right)$$

* Then 
$$\mathbb{E}(Y^{(k)}|X_k=x_k) =  f_k(x_k)$$

* We could use nearest neighbors, or splines, or kernels, or local-linear regression, or anything else

---

* Given: 
    - $n\times p$ matrix of $p$ predictors 
    - $n\times 1$ response vector 
    - maxit: maximum number of iterations
    - one-dimensional smoother $s$ 

* Initialize $\hat{\beta}_0 \leftarrow \bar{y}$ and $\hat{f}_{j} \leftarrow 0$ for $j=1,\ldots,p$

* for (i in 1:maxit){

    for $k=1,\ldots, p$ {
    
      $y_i^{(k)} = y_i - \sum_{j\neq k} \hat{f}_j(x_{ij})$
    
      $g_k \leftarrow s( y^{(k)} \sim x_k)$
    
      $g_k \leftarrow g_k - n^{-1} \sum_{i=1}^{n}g_k(x_{ik})$
    
      $\hat{f}_{k} \leftarrow g_k$
      
      }}

* Return $\hat{\beta}_0, \hat{f}_{1}, \ldots, \hat{f}_{p}$

---

```{r}
am_backfit <-
function(X, y, maxit=10L)
{
  p <- ncol(X)
  id <- seq_len(nrow(X))
  alpha <- mean(y)
  f <- matrix(0, ncol = p, nrow = nrow(X))
  models <- vector("list", p + 1L)
  for (i in seq_len(maxit))
  {
    for (j in seq_len(p))
    {
    p_resid <- y - alpha - apply(f[, -j], 1L, sum)
    id <- order(X[,j])
    models[[j]] <- smooth.spline(X[id,j], p_resid[id])
    f[,j] <- predict(models[[j]], X[,j])$y
    }
  alpha <- mean(y - apply(f, 1L, sum))
  }
  models[[p + 1L]] <- alpha
return(models)
}
```

---

```{r}
am_predict <-
function(models, X_new)
{
p <- ncol(X_new)
f <- matrix(0, ncol = p, nrow = nrow(X_new))
for (j in seq_len(p))
{
f[,j] <- predict(models[[j]], X_new[,j])$y
}
y <- apply(f, 1L, sum) + models[[p + 1L]]
list(y=y, f=f)
}
```

---

```{r}
set.seed(123)
n <- 500; p <- 4
X <- matrix(runif(n * p, min = -2, max = 2), ncol = p)
f1 <- cos(X[,1] * 4) + sin(X[,1] * 10) + X[,1]^(2)
f2 <- -1.5 * X[,2]^2 + (X[,2] > 1) * (X[,2]^3 - 1)
f3 <- 0
f4 <- sign(X[,4]) * 1.5
f1 <- f1 - mean(f1); f2 <- f2 - mean(f2)
f3 <- f3 - mean(f3); f4 <- f4 - mean(f4)
y <- 10 + f1 + f2 + f3 + f4 + rnorm(n, sd = 1.2)
```


---

```{r}
models <- am_backfit(X, y, maxit = 1)
yhat <- am_predict(models,X)
plot(X[,1],yhat$f[,1], type="p")
points(X[,1], f1, col=2)
```


---

# mgcv

The `mgcv` R package  is based not on
backfitting, but rather on something called the *Lanczos
algorithm*, a way of efficiently calculating truncated matrix
decompositions that is beyond the scope of this course

The basic syntax is

```{r, eval=FALSE}
fit <- gam(y ~ s(x1) + s(x2), data=train)
```

One can add arguments to the `s()` function, but the default is to use a __natural cubic spline basis__ and to automatically choose the smoothing parameter $\lambda$ via optimization of the __GCV__

---

# Interaction terms

One way to think about additive models, and about (possibly) including interaction terms, is to imagine doing a sort of __Taylor series expansion__ of the true regression function

The zero-th order expansion would be a constant:
$$f(x)\approx \beta_0$$

A purely additive model would correspond to a first-order expansion:
$$f(x)\approx \beta_0 + \sum_{j=1}^{p}f_j(x_j)$$

Two-way interactions come in when we go to a second-order expansion:
$$f(x)\approx \beta_0 + \sum_{j=1}^{p}f_j(x_j) + \sum_{j=1}^{p}\sum_{k=j+1}^{p}f_{jk}(x_j,x_k)$$

For identifiability, we need $\mathbb{E}(f_{jk}(X_j,X_k) ) = 0$

---

# Thin-plate spline

Suppose we have two input variables, $x$ and $z$, and a single response $y$. How could we do a spline fit?

One approach is to generalize the spline optimization problem so that we penalize the curvature of the spline surface (no longer a curve)

The appropriate penalized least-squares objective function to minimize is
$$\sum_{i=1}^{n}(y_i - f(x_i,z_i))^2 + \lambda \int \left[  \left(\frac{\partial^2 f}{\partial x^2} \right)^2 + 2\left(\frac{\partial^2 f}{\partial x \partial z} \right)^2 + \left(\frac{\partial^2 f}{\partial z^2} \right)^2 \right]$$

The solution is called a __thin-plate spline__  ( `s(x,z)` in `mgcv` )

This is appropriate when the two predictors $x$ and $z$ are measured on similar scales

---

# Tensor product spline

An alternative is use the spline basis functions 
$$f(x,z) = \sum_{j=1}^{K_x}\sum_{k=1}^{K_z} \beta_{jk} b_{j}(x) b_{k}(z)$$

Doing all possible multiplications of one set of numbers or functions with another is said to give their outer product or tensor product, so this is known as a __tensor product spline__   ( `te(x,z)` in `mgcv` )

This is appropriate when the measurement scales of $x$ and $z$ are very different

We have to chose the number of terms to include for each variable ( $K_x$ and $K_z$)

---

# Generalized additive models

* A natural step beyond generalized linear models is generalized additive models (GAMs), where instead of making the transformed mean response a linear function of the predictors, we make it an additive function of the predictors 

* For example
$$\mathrm{logit}\{ \mathbb{E}(Y|X=x) \} = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$

---

# Application: California house prices

* The Census Bureau divides the U.S. into geographic regions called __tracts__ of a few thousand people each

*  Data from the 2011 American Community Survey, containing information on the housing stock and economic circumstances of every tract in California

    -  __Median_house_value__ : The median value of the housing units in the tract (response)
    - __POPULATION__, __LATITUDE__, __LONGITUDE__ : The population, latitude and longitude of the tract
    - __Median_household_income__, __Mean_household_income__ : The median and mean income of households (in dollars, from all sources)
    - __Total_units__, __Vacant_units__ : The total number of units and the number of vacant units
    - __Owners__ : The percentage of households which own their home
    - __Median_rooms__ : The median number of rooms per unit
    - __Mean_household_size_owners__, __ Mean_household_size_renters__ The mean number of people per household which owns its home, the mean
number of people per renting household

---

```{r}
library(RCurl)
calif <- read.csv("https://raw.githubusercontent.com/aldosolari/DM/master/docs/lezioni/L15/calif.csv")
library(mgcv)
fit <- gam(log(Median_house_value) ~ s(Median_household_income) +
                   s(Mean_household_income) + 
                   s(POPULATION) + 
                   s(Total_units) + 
                   s(Vacant_units) +
                   s(Owners) + 
                   s(Median_rooms) + 
                   s(Mean_household_size_owners)  + 
                   s(Mean_household_size_renters) + 
                  s(LATITUDE,LONGITUDE), data=calif)
```

---

```{r}
plot(fit, se = 2, shade = TRUE, resid = TRUE, select=1)
```

---

```{r}
plot(fit, select = 10, se = FALSE)
```

---

```{r}
plot(fit, select = 10, phi = 60, scheme = TRUE, ticktype = "detailed", cex.axis = 0.5)
```


