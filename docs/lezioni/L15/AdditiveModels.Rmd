---
title: "Data Mining"
subtitle: "Additive Models"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220))
```





# Additive models

The additive model for regression is that the conditional expectation function is a sum of partial response functions, one for each predictor variable
$$\mathbb{E}(Y|X=x) = \beta_0 + \sum_{j=1}^{p}f_j(x_j)$$

This includes the linear model as a special case, where
$$f_j(x_j) = \beta_j x_j$$
but it's
clearly more general, because the $f_j$'s can be arbitrary nonlinear functions

The idea is still that each input feature makes a separate contribution to the response,
and these just add up (hence "partial response function"), but these contributions donâ€™t have to be strictly proportional to the inputs

We do need to add a
restriction to make it identifiable; without loss of generality
$$\mathbb{E}(Y) = \beta_0, \qquad \mathbb{E}(f_j(X_j)) =0$$

How can we fit additive models?

---

Linear models assume
$$\mathbb{E}(Y|X=x) = \sum_{j=0}^{p} \beta_j x_j$$
where $x_0$ is always the constant 1

Suppose we don't condition on all $X=(X_0,\ldots,X_p)'$ but just one component of it, say $X_k$. Then 
$$\mathbb{E}(Y|X_k=x_k) =  \beta_k x_k + \mathbb{E}(\sum_{j\neq k} \beta_j X_j|X_k=x_k)$$

Then
$$\mathbb{E}(Y^{(k)}|X_k=x_k) =  \beta_k x_k$$
where $Y^{(k)} = Y - \sum_{j\neq k} \beta_j X_j$ the $k$th __partial residual__: the difference between $Y$ and what we expect it to be __ignoring__ the contribution from $X_k$

This suggests the following estimation scheme for linear models, known as the
__Gauss-Seidel algorithm__, or more commonly as __back-fitting__

---

* Given: 
    - $n\times p$ matrix $\mathbf{X}$ of $p$ predictors
    - $n\times 1$ response vector $\mathbf{y}$
    - small tolerance $\delta > 0$

* Center $\mathbf{y}$ and each column of $\mathbf{X}$

* Initialize $\hat{\beta}_{j} \leftarrow 0$ for $j=1,\ldots,p$

* Until (all $|\hat{\beta}_j - \gamma_j| \leq \delta$ ){

    for $k=1,\ldots, p$ {
    
      $y_i^{(k)} = y_i - \sum_{j\neq k} \hat{\beta}_jx_{ij}$
    
      $\gamma_k \leftarrow$ regression coefficient of $y^{(k)}$ on $x_k$
    
      $\hat{\beta}_j \leftarrow  \gamma_k$
      
      }}

* $\hat{\beta}_{0} \leftarrow \bar{y} - \sum_{j=1}^{p} \hat{\beta}_j \bar{x}_{j}$ with original data

* Return $\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p$


---

Generate a response $Y$ and predictors $X_1$, $X_2$ and $X_3$ with $n = 100$ as follows:

```{r}
n = 100
set.seed(1)
x1 = rnorm(n)
x2 = rnorm(n)
x3 = rnorm(n)
y = 30 - 10*x1 + 20*x2 + 30*x3 + rnorm(n)
fit = lm(y ~ x1+x2+x3)
X = model.matrix(fit)
```

---

```{r}
# center y and X
yc = y - mean(y)
Xc = apply(X[,-1],2,function(x) x-mean(x))
# initialize 
p = ncol(Xc)
hatbeta = rep(0,p)
delta = 0.1
gamma = rep(2*delta,p)
while ( !all( abs(gamma - hatbeta) <= delta) ){
for (k in 1:p){
  gamma = hatbeta
  yk = yc - Xc[,-k, drop=F] %*% hatbeta[-k, drop=F]
  gammak = coef(lm(yk ~ 0+ Xc[,k]))
  hatbeta[k] <- gammak
}}
hatbeta0<- mean(y) - sum(hatbeta*apply(X[,-1],2,mean))
data.frame( backfitting=c(hatbeta0,hatbeta), leastsquares=coef(fit))
```

---

# Backfitting additive models

* Define the partial residuals by analogy with the linear case, as
$$Y^{(k)} = Y - \left(\beta_0 - \sum_{j\neq k} f_j(x_j) \right)$$

* Then 
$$\mathbb{E}(Y^{(k)}|X_k=x_k) =  f_k(x_k)$$

* We could use nearest neighbors, or splines, or kernels, or local-linear regression, or anything else

---

* Given: 
    - $n\times p$ matrix of $p$ predictors 
    - $n\times 1$ response vector 
    - small tolerance $\delta > 0$
    - one-dimensional smoother $s$ 

* Initialize $\hat{\beta}_0 \leftarrow \bar{y}$ and $\hat{f}_{j} \leftarrow 0$ for $j=1,\ldots,p$

* until (all $|\hat{f}_j - g_j| \leq \delta$ ){

    for $k=1,\ldots, p$ {
    
      $y_i^{(k)} = y_i - \sum_{j\neq k} \hat{f}_j(x_{ij})$
    
      $g_k \leftarrow s( y^{(k)} \sim x_k)$
    
      $g_k \leftarrow g_k - n^{-1} \sum_{i=1}^{n}g_k(x_{ik})$
    
      $\hat{f}_{k} \leftarrow g_k$
      
      }
      
    }

* Return $\hat{\beta}_0, \hat{f}_{1}, \ldots, \hat{f}_{p}$

---

# mgcv

The `mgcv` R package  is based not on
backfitting, but rather on something called the *Lanczos
algorithm*, a way of efficiently calculating truncated matrix
decompositions that is beyond the scope of this course

The basic syntax is

```{r, eval=FALSE}
fit <- gam(y ~ s(x1) + s(x2), data=train)
```

One can add arguments to the `s()` function, but the default is to use a __natural cubic spline basis__ and to automatically choose the smoothing parameter $\lambda$ via optimization of the __GCV__

---

# Interaction terms

One way to think about additive models, and about (possibly) including interaction terms, is to imagine doing a sort of __Taylor series expansion__ of the true regression function

The zero-th order expansion would be a constant:
$$f(x)\approx \beta_0$$

A purely additive model would correspond to a first-order expansion:
$$f(x)\approx \beta_0 + \sum_{j=1}^{p}f_j(x_j)$$

Two-way interactions come in when we go to a second-order expansion:
$$f(x)\approx \beta_0 + \sum_{j=1}^{p}f_j(x_j) + \sum_{j=1}^{p}\sum_{k=j+1}^{p}f_{jk}(x_j,x_k)$$

For identifiability, we need $\mathbb{E}(f_{jk}(X_j,X_k) ) = 0$

---

# Thin-plate spline

Suppose we have two input variables, $x$ and $z$, and a single response $y$. How could we do a spline fit?

One approach is to generalize the spline optimization problem so that we penalize the curvature of the spline surface (no longer a curve)

The appropriate penalized least-squares objective function to minimize is
$$\sum_{i=1}^{n}(y_i - f(x_i,z_i))^2 + \lambda \int \left[  \left(\frac{\partial^2 f}{\partial x^2} \right)^2 + 2\left(\frac{\partial^2 f}{\partial x \partial z} \right)^2 + \left(\frac{\partial^2 f}{\partial z^2} \right)^2 \right]$$

The solution is called a __thin-plate spline__  ( `s(x,z)` in `mgcv` )

This is appropriate when the two predictors $x$ and $z$ are measured on similar scales

---

# Tensor product spline

An alternative is use the spline basis functions 
$$f(x,z) = \sum_{j=1}^{K_x}\sum_{k=1}^{K_z} \beta_{jk} b_{j}(x) b_{k}(z)$$

Doing all possible multiplications of one set of numbers or functions with another is said to give their outer product or tensor product, so this is known as a __tensor product spline__   ( `te(x,z)` in `mgcv` )

This is appropriate when the measurement scales of $x$ and $z$ are very different

We have to chose the number of terms to include for each variable ( $K_x$ and $K_z$)

---

# Generalized additive models

* A natural step beyond generalized linear models is generalized additive models (GAMs), where instead of making the transformed mean response a linear function of the predictors, we make it an additive function of the predictors 

* For example
$$\mathrm{logit}\{ \mathbb{E}(Y|X=x) \} = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$

---

# Application: NYC flights data


