---
title: "Lasso"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      ratio: '16:9'
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


# Sparse linear regression

![](images/signalsparsity.png)
.center[Martin J. Wainwright, High-Dimensional Statistics]

---

# Another look at best subset

If the signal is sparse, best subsets regression may seem natural. 

Best subset can be seen amounts to finding a set of coefficient estimates such that RSS is as small as possible, subject to the constraint that
no more than $s$ coefficients can be nonzero
    $$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq s$$
where, abusing terminology, we will define the $\ell_0$-"norm" of a vector as a count of the non-zero components it contains:
$$\|\boldsymbol{\beta}\|_0 = \sum_{j=1}^{p} 1\{\beta_j \neq 0\}$$
Unfortunately, the optimization problem is not convex, and solving it requires considering all ${p \choose s} = \frac{p!}{(p-s)! s!}$ models containing $s$ predictors

---

# Another formulation for ridge regression 

Similarly, with the ridge regression objective
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|^2_2$$
we know that $\hat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ will also be the minimiser of
$$\| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq \|\hat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda\|^2_2$$

In other words, ridge regression can be seen as solving the constrained problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq s$$
for a tuning parameter $s\geq 0$. 

The two problems are equivalent: for any $\lambda \geq 0$ and solution $\hat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$, there is a value $s\geq 0$ such that $\hat{\boldsymbol{\beta}}^{\mathrm{R}}_\lambda$ also solve the constrained problem; and, for any $s \geq 0$ and solution $\hat{\boldsymbol{\beta}}^{\mathrm{R}}_s$, there is a value $\lambda\geq 0$ such that $\hat{\boldsymbol{\beta}}^{\mathrm{R}}_s$ also solve the penalized problem

---

.pull-left[
![](images/l2.jpg)
]

.pull-right[ISL, Figure 6.7: Contours of the residual sum of squares
$$\| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2$$
(red ellipses) and constraint $$\beta_1^2 + \beta_2^2 \leq s$$  (blue region) for ridge regression]

---

# Three norms

Let's consider three canonical choices: the $\ell_0$
, $\ell_1$ and $\ell_2$ norms
$$\|\boldsymbol{\beta} \|_0 = \sum_{j=1}^{p}1\{\beta_j\neq 0\}, \quad \|\boldsymbol{\beta} \|_1 = \sum_{j=1}^{p} |\beta_j |, \quad \|\boldsymbol{\beta} \|_2 =\Big( \sum_{j=1}^{p} \beta_j^2 \Big)^{1/2}$$

The three norms are special cases of the $\ell_q$ norm:
$$\| \boldsymbol{\beta} \|_{q} =  (\sum_{j=1}^{p}|\beta_j|^q)^{1/q}$$


(strictly speaking, the $\ell_0$-"norm" does not satisfy the conditions of a proper norm)

---

![](images/four.jpg)


---

# Signal sparsity

Signal sparsity is the assumption that only a "small" number of predictors have an effect, i.e. have $\beta_j \neq 0$

We would like our estimator $\hat{\boldsymbol{\beta}}$ to be sparse, meaning that most $\hat{\beta}_j$ are zero

The ridge regression estimator is not sparse

The best subset selection estimator and the lasso estimator are sparse

Why does the $\ell_1$ norm induces sparsity and not the $\ell_2$ norm? Look at the lasso and ridge constraint sets (ISL, Figure 6.7)

---

![](images/constraints.jpg)


---

# Lasso 

The **L**east **a**bsolute **s**hrinkage and
**s**election **o**perator (Lasso, Tibshirani, 1996) estimates $\boldsymbol{\beta}^0$ by 
$(\hat{\mu}^L,\hat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda)$, where $\hat{\mu}^{L}$ minimise 
$$\frac{1}{2n}\| \mathbf{y} - \mu\mathbf{1} -  \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$
over $(\mu, \boldsymbol{\beta}) \in \mathbb{R}\times \mathbb{R}^p$. Like ridge regression, $\hat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ shrinks the OLS estimates towards the origin, but there is an important difference. 

The $\ell_1$ penalty can force some of the estimated coefficients to be exactly 0. In this way the Lasso can perform simultaneous variable selection and parameter estimation. 

As we did with ridge regression, we can centre and scale the $\mathbf{X}$ matrix, and also centre $\mathbf{y}$ and thus remove $\mu$ from the objective: the Lasso solves the penalized problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \frac{1}{2n} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$
or, equivalently, the constrained problem
$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq s$$


---

# Shrinkage and selection

Shrinkage: $\hat{\boldsymbol{\beta}}^{\mathrm{L}}_\lambda$ is the Lasso shrunken estimate

Selection: $\hat{S}_\lambda = \{k: \hat{\beta}_{\lambda,k}^{\mathrm{L}} \neq 0\}$ is the index set of "active" predictors

---

Figure 2.1 in Hastie, Tibshirani, Wainwright (2015) [Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity/) 

![](images/lassoridge.jpg)

---

# Convexity

The objective function of the $\ell_1$ penalty, unlike the $\ell_0$ penalty, is a continuous function in the regression vector

Not only is the objective function continuous, it is also convex

We say that a function $f:\mathbb{R}^p \mapsto \mathbb{R}$ convex if for any values $b_1$ and $b_2$ and quantity $t \in [0,1]$ we have
$$f(tb_1 + (1-t)b_2) \leq tf(b_1) + (1-t)f(b_2)$$
Replacing the $\leq$ with $<$ for $t\in (0,1)$ yields a definition of strict convexity. 

The $\ell_1$-penalized objective function is in fact convex (but not strictly convex, which makes the solutions non-linear in the $y_i$, and there is no closed form solution as in ridge). 

---

# Convex optimization

A convex function does not have any local minima that are not also global minima. In other words, if the value $b_0$ minimizes $f$ over a neighborhood of $b_0$, it must also minimize $f$ over its entire domain. 

To see this, assume that $b_1$ is any point that is not a global minima but set $b_2$ equal to a global minima of $f$. Then, for any $t\in [0,1)$, we have
$$tf(b_1) + (1-t)f(b_2) < tf(b_1) + (1-t)f(b_1) = f(b_1)$$
and by convexity this implies that
$$f(t b_1 + (1-t)b_2)  < f(b_1)$$
For any neighborhood around $b_1$ we can find $t$ close enough to 1 such that $tb_1 + (1-t)b_2$ is in that neighborhood, and therefore $b_1$ cannot be a local minimum. 

The lack of local optima makes it possible to optimize convex objective functions using e.g. the coordinate descent algorithm (implemented in the glmnet R package). 

---

# The one standard error rule

Usually we choose the tuning parameter $\lambda$ that minimize the CV error.

This rule often ends up selecting models that are larger than desiderable for interpretation purposes.

We can achieve smaller, simpler models with comparable predictive performance by using a simple device called the __one standard error rule__ .

We can compute cross-validation "standard errors" as
$$\mathrm{SE}(\mathrm{CVErr}) = \frac{1}{\sqrt{K}}\mathrm{sd}(\mathrm{Err}^{-1},\ldots,\mathrm{Err}^{-K})$$
where $\mathrm{Err}^{-k}$ denotes the error incurred in predicting the observations in the $k$ hold-out fold, $k=1,\ldots,K$.

The one standard error rule choooses the tuning parameter value corresponding to the simplest model whose CV error is within one standard error of the minimum.

---

![](images/oneserule.jpg)

---

```{r}
library(glmnet); library(hdi); data(riboflavin); X = riboflavin$x; y = riboflavin$y
```

.pull-left[
```{r}
ridge <- cv.glmnet(X,y,alpha=0)
plot(ridge, ylim=c(0,1))
```
]

.pull-right[
```{r}
lasso <- cv.glmnet(X,y,alpha=1)
plot(lasso, ylim=c(0,1))
```
]

---

ELS, Figure 7.2

![](images/biasvar.jpg)

---

# Group Lasso

Suppose we have a partition $G_1,\ldots, G_q$ of $\{1,\ldots,p\}$. The group Lasso penalty (Yuan and Lin, 2006) is given by 
$$\lambda \sum_{j=1}^{q} m_j \|\boldsymbol{\beta}_{G_j}\|_1$$
The multipliers $m_j>0$ serve to balance cases where the groups are of very different sizes; typically we choose $m_j = \sqrt{|G_j|}$. 

This penalty encourages either an entire group $G$ to have $\hat{\boldsymbol{\beta}}_{G}= \mathbf{0}$ or $\hat{\beta}_k \neq 0$ for all $k \in G$. 

Such a property is useful when groups occur through coding for categorical predictors or when expanding predictors using basis functions. 


---

# Reducing the bias of the Lasso

One potential drawback of the Lasso is that the same shrinkage effect that sets many estimated coefficients to zero also shrinks all non-zero estimated coefficients towards zero. 

One possible solution is to take $\hat{S}_\lambda = \{k: \hat{\beta}_{\lambda,k}^{\mathrm{L}} \neq 0\}$ and then re-estimate $\boldsymbol{\beta}^0_{\hat{S}_\lambda}$ by OLS regression on $\mathbf{X}_{\hat{S}_\lambda}$. 

Another option is to re-estimate using the Lasso on $\mathbf{X}_{\hat{S}_\lambda}$; this procedure is known as the *relaxed Lasso* (Meinshausen, 2007). 


The *adaptive Lasso* (Zou, 2006) takes an initial estimate of $\boldsymbol{\beta}^0$, $\hat{\boldsymbol{\beta}}^{\mathrm{init}}$ (e.g. from the Lasso) and then performs weighted Lasso regression: 
$$\hat{\boldsymbol{\beta}}^{\mathrm{adapt}}_\lambda = \arg\min_{\boldsymbol{\beta}: \boldsymbol{\beta}_{\hat{S}^c_{\mathrm{init}}}=\mathbf{0}} \left\{ \frac{1}{2n} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda \sum_{k \in \hat{S}_{\mathrm{init}}} \frac{|\beta_k|}{|\hat{\beta}_k^{\mathrm{init}}|} \right\}$$
where $\hat{S}_{\mathrm{init}} = \{k: \hat{\beta}^{\mathrm{init}}_k\neq 0\}$.

---

# Elastic Net

Define the objective function $f$ for some $\lambda>0$ and $\alpha \in [0,1]$ as
$$f(\boldsymbol{\beta}; \lambda, \alpha) = \frac{1}{2n} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\left( (1-\alpha)\frac{1}{2}\|\boldsymbol{\beta}||_2^2 + \alpha \|\boldsymbol{\beta}\|_1 \right)$$

and the corresponding *elastic net* estimator as 
$$\hat{\boldsymbol{\beta}}^{\mathrm{E}}_{\lambda,\alpha} = \arg\min_{\boldsymbol{\beta}} f(\boldsymbol{\beta}; \lambda, \alpha)$$
Setting $\alpha$ to 1 yields the Lasso regression and setting it to 0 the ridge regression. 

Adding a small $\ell_2$-penalty preserves the variable selection and convexity properties of the $\ell_1$-penaltized regression, while reducing the variance of the model when $\boldsymbol{X}$ contains sets of highly correlated variables. 



