<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lasso and best subset selection</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lasso and best subset selection
### Aldo Solari

---





# Outline

* Three norms

* Lasso

* Best subset selection

* Variable selection and cross-validation


---

# Three norms

Let's consider three canonical choices: the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms
`$$\|\boldsymbol{\beta} \|_0 = \sum_{j=1}^{p}1\{\beta_j\neq 0\}, \quad \|\boldsymbol{\beta} \|_1 = \sum_{j=1}^{p} |\beta_j |, \quad \|\boldsymbol{\beta} \|_2 =\Big( \sum_{j=1}^{p} \beta_j^2 \Big)^{1/2}$$`

The three norms are special cases of the `\(\ell_q\)` norm:
`$$\| \boldsymbol{\beta} \|_{q} =  (\sum_{j=1}^{p}|\beta_j|^q)^{1/q}$$`


Strictly speacking, "the `\(\ell_0\)` norm" is not a norm : it does not satisfy positive homogeneity, i.e. `\(\|a\boldsymbol{\beta} \|_0 \neq a \|\boldsymbol{\beta} \|_0\)` for `\(a&gt;0\)`

---

![](images/four.jpg)

---

# Penalized problems

In penalized form, the use of the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms gives rise to the problems

* Best subset selection
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_0$$`
* Lasso regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$`
*  Ridge regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|^2_2$$`

with  tuning parameter `\(\lambda\geq 0\)` 

---

# Constrained problems

In constrained form, the use of the `\(\ell_0\)`
, `\(\ell_1\)` and `\(\ell_2\)` norms gives rise to the problems

* Best subset selection
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$`
* Lasso regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$`
*  Ridge regression
    `$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|^2_2 \leq t$$`

where `\(k\)` and `\(t\)` are the tuning parameters

Note that it makes sense to restrict `\(k\)` to be an integer: in best subset selection, we want to find the "best" subset of predictors of size `\(k\)`, where "best" is in terms of the achieved training error

---

# Penalized and constrained problems

* The penalized and constrained problems with the `\(\ell_1\)` and `\(\ell_2\)` norms (lasso and ridge) are equivalent: for any `\(t\geq 0\)` and solution `\(\hat{\boldsymbol{\beta}}\)` of the constrained problem, there is a value `\(\lambda\geq 0\)` such that  `\(\hat{\boldsymbol{\beta}}\)` also solves the penalized problem, and vice versa

* The penalized and constrained problems with the  `\(\ell_0\)` norm (best subset selection) are not equivalent: for every value of `\(\lambda \geq 0\)` and solution `\(\hat{\boldsymbol{\beta}}\)` of the penalized problem, there is a value of `\(k\)` such that `\(\hat{\boldsymbol{\beta}}\)` solves the constrained problem, but the converse is not true

---

# Convexity

* The lasso and ridge regression problems have a very important property: they are __convex optimization problems__

* It is convexity that allows to equate the penalized and constrained problems

* It is also convexity that allows to efficiently obtain the lasso and ridge regression solutions

* The lasso penalty term is not strictly convex and it makes the solutions non-linear in the `\(y_i\)`, and there is no closed form expression as in ridge

* Why is a convex optimization problem so special? The short answer: because any local minimizer is a global minimizer

* Best subset selection is not convex. It is known to be __NP-hard__, in some sense the worst kind of nonconvex problem

---


# Sparsity

* Sparsity is the assumption that only a "small" number of predictors have an effect, i.e. have `\(\beta_j \neq 0\)`

*  We would like our estimator `\(\hat{\boldsymbol{\beta}}\)` to be __sparse__, meaning that most `\(\hat{\beta}_j\)` are zero

* The ridge regression estimator is not sparse

* The best subset selection estimator and the lasso estimator are sparse

* Why does the `\(\ell_1\)` norm induces sparsity and not the `\(\ell_2\)` norm? Look at the lasso and ridge constraint sets (ISLR, Figure 6.7)

---

![](images/constraints.jpg)


---

layout: false
class: inverse, middle, center

# Lasso

---


# Lasso regression

* Solve the penalized problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 + \lambda\| \boldsymbol{\beta}\|_1$$`
or, equivalently, the constrained problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_1 \leq t$$`

* The solution is known as LASSO: Least Absolute Shrinkage and
Selection Operator (Tibshirani, 1996)

* Shrinkage: `\(\hat{\boldsymbol{\beta}}(\lambda)\)` is the lasso shrunken estimate

* Selection: `\(\hat{\mathcal{A}}(\lambda) = \{X_j: \hat{\beta}_j(\lambda) \neq 0\}\)` is the set of active predictors

---

Figure 2.1 in Hastie, Tibshirani, Wainwright (2015) [Statistical Learning with Sparsity](https://web.stanford.edu/~hastie/StatLearnSparsity/) 

![](images/lassoridge.jpg)

---

# The one standard error rule

* Usually we choose the tuning parameter `\(\lambda\)` that minimize the CV error

* This rule often ends up selecting models that are larger than desiderable for interpretation purposes

* We can achieve smaller, simpler models with comparable predictive performance by using a simple device called the __one standard error rule__

* We can compute cross-validation "standard errors" as
`$$\mathrm{SE}(\mathrm{CVErr}) = \frac{1}{\sqrt{K}}\mathrm{sd}(\mathrm{Err}^{-1},\ldots,\mathrm{Err}^{-K})$$`
where `\(\mathrm{Err}^{-k}\)` denotes the error incurred in predicting the observations in the `\(k\)` hold-out fold, `\(k=1,\ldots,K\)`

* The one standard error rule choooses the tuning parameter value corresponding to the simplest model whose CV error is within one standard error of the minimum

---

![](images/oneserule.jpg)

# Regularization

![](images/biasvar.jpg)

---

layout: false
class: inverse, middle, center

# Best subset selection

---

# Best subset selection

* Solve the constrained problem
`$$\min_{\boldsymbol{\beta} \in \mathbb{R}^p} \| \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2_2 \mathrm{\,\,subject\,\,to\,\,}\|\boldsymbol{\beta}\|_0 \leq k$$`

* The solution is the best subset of predictors of size `\(k\)`, known as __best subset selection__

* The problem is nonconvex (NP-hard): it requires
searching through all `\({p \choose k}\)` subsets of predictors

---

# Algorithm

Set `\(B_0\)` as the null model (only intercept)

For `\(k=1,\ldots,p\)`:

1. Fit all `\({ p \choose k }\)` models that contain exactly `\(k\)` predictors

2. Pick the "best" among these `\({ p \choose k }\)` models, and call it `\(B_k\)`, where "best" is defined having the smallest residual sum of squares RSS = `\(n \mathrm{MSE}_{\mathrm{Tr}}\)`

Select a single best model from among `\(B_0,B_1,\ldots,B_p\)` using AIC, BIC, Cross-Validation, etc.


---

![](images/bestsubset.jpg)

---

# Backward stepwise selection


* __Greedy algorithm__ sub-optimal to best subset selection but computationally efficient
* Applicable only when `\(n&gt;p\)`

__Algorithm__

Set `\(S_p\)` as the full model (all `\(p\)` predictors)

For `\(k=p,p-1,\ldots,1\)`:

1. Consider all `\(k\)` models that contain all but one of the predictors in `\(S_k\)`, for a total of `\(k-1\)` predictors

2. Choose the "best" among these `\(k\)` models and call it `\(S_{k-1}\)`, where "best" is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,\ldots,S_p\)` using AIC, BIC, cross-validation, etc.

---

# Forward stepwise selection


* Greedy algorithm sub-optimal to best subset selection but computationally efficient
* Applicable also when `\(p&gt;n\)` to construct the sequence `\(S_0, S_1,\ldots,S_{n-1}\)`

__Algorithm__

Set `\(S_0\)` as the null model (only intercept)

For `\(k=0,\ldots,\min(n-1,p-1)\)`:


1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor

2. Choose the "best" among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where "best" is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,S_2, \ldots\)` using AIC, BIC, cross-validation, etc.


---

# Forward with AIC-based stopping rule

Set `\(S_0\)` as the null model and `\(k=0\)`

1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor

2. Choose the "best" among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where "best" is defined having the smallest AIC

3. If AIC( `\(S_{k+1}\)` ) `\(&lt;\)` AIC( `\(S_k\)` ), set `\(k=k+1\)` and go to 1., otherwise STOP

---

# Application to Hitters data


```r
rm(list=ls())
library(ISLR)
data(Hitters)
Hitters = Hitters[complete.cases(Hitters),]
Hitters[,"League"]=(Hitters[,"League"]=="A")*1
Hitters[,"Division"]=(Hitters[,"Division"]=="E")*1
Hitters[,"NewLeague"]=(Hitters[,"NewLeague"]=="A")*1
set.seed(1)
train.id = sample(nrow(Hitters),nrow(Hitters)/2)
train = Hitters[train.id,]
names(train)[19] = "y" 
X = as.matrix(train[,-19])
y = train$y
n = nrow(X)
p = ncol(X)
test = Hitters[-train.id,]
names(test)[19] = "y" 
X.star = as.matrix(test[,-19])
y.star = test$y
m = nrow(X.star)
```

---


```r
# Full model
fit.full = lm(y ~ ., train)
RMSE.full = sqrt(mean( (predict(fit.full, newdata=test) - y.star )^2 ))
RMSE.full
```

```
[1] 410.6011
```

---


```r
# Ridge regression
require(glmnet)
K = 10
set.seed(123)
ridge.cv&lt;-cv.glmnet(X,y,alpha=0, nfolds = K, grouped=FALSE)
hatlambda &lt;-ridge.cv$lambda.min
yhat.ridge = predict(ridge.cv, s=hatlambda, newx=X.star, exact=TRUE)
RMSE.ridge = sqrt(mean( (yhat.ridge - y.star)^2 ))
RMSE.ridge
```

```
[1] 375.3843
```


---


```r
# LASSO
fit.lasso &lt;- glmnet(X, y, alpha=1)
plot(fit.lasso, xvar="lambda", label=T)
```

![](Lasso_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---


```r
# The one-standard error rule
set.seed(123)
cv.lasso &lt;-cv.glmnet(X, y, alpha=1, nfolds = K)
hatlambda&lt;-cv.lasso$lambda.1se
yhat.lasso = predict(fit.lasso, s=hatlambda, newx=X.star, exact=T)
RMSE.lasso = sqrt(mean( (yhat.lasso - y.star)^2 ))
RMSE.lasso
```

```
[1] 405.0077
```

---


```r
# Best subset
library(leaps)
fit.bests &lt;- regsubsets(y~.,train, nvmax=p)
summary.bests&lt;-summary(fit.bests)
summary.bests$outmat
```

```
          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors NewLeague
1  ( 1 )  " "   " "  " "   " "  " " " "   " "   " "    " "   " "    " "   "*"  " "    " "    " "      " "     " "     " "    " "      
2  ( 1 )  " "   " "  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*"  " "    " "    " "      " "     " "     " "    " "      
3  ( 1 )  " "   " "  " "   " "  " " "*"   " "   " "    " "   " "    " "   "*"  " "    " "    "*"      " "     " "     " "    " "      
4  ( 1 )  " "   " "  " "   " "  " " "*"   "*"   " "    " "   " "    " "   "*"  " "    " "    "*"      " "     " "     " "    " "      
5  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      " "     " "     " "    " "      
6  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      "*"     " "     " "    " "      
7  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      "*"     "*"     " "    " "      
8  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    "*"    "*"      "*"     "*"     " "    " "      
9  ( 1 )  "*"   " "  "*"   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      "*"     "*"     " "    " "      
10  ( 1 ) "*"   " "  "*"   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    "*"    "*"      "*"     "*"     " "    " "      
11  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    " "   " "  " "    "*"    "*"      "*"     "*"     " "    " "      
12  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    " "   " "  " "    "*"    "*"      "*"     "*"     " "    "*"      
13  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    " "   " "  " "    "*"    "*"      "*"     "*"     "*"    "*"      
14  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    "*"   " "  " "    "*"    "*"      "*"     "*"     "*"    "*"      
15  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    "*"   " "  "*"    "*"    "*"      "*"     "*"     "*"    "*"      
16  ( 1 ) "*"   " "  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    "*"   "*"  "*"    "*"    "*"      "*"     "*"     "*"    "*"      
17  ( 1 ) "*"   "*"  "*"   " "  " " "*"   "*"   "*"    "*"   "*"    "*"   "*"  "*"    "*"    "*"      "*"     "*"     "*"    "*"      
18  ( 1 ) "*"   "*"  "*"   "*"  " " "*"   "*"   "*"    "*"   "*"    "*"   "*"  "*"    "*"    "*"      "*"     "*"     "*"    "*"      
19  ( 1 ) "*"   "*"  "*"   "*"  "*" "*"   "*"   "*"    "*"   "*"    "*"   "*"  "*"    "*"    "*"      "*"     "*"     "*"    "*"      
```

---


```r
# Best Cp
plot(summary.bests$cp, xlab="k", ylab="Cp", type="b")
```

![](Lasso_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---


```r
# best BIC
plot(fit.bests, scale="bic")
```

![](Lasso_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---


```r
# function predict for regsubsets
predict.regsubsets =function(object ,newdata ,id ,...){
 form=as.formula(object$call[[2]])
 mat=model.matrix(form, newdata)
 coefi =coef(object, id=id)
 xvars =names(coefi)
 mat[,xvars]%*%coefi
}

yhat.bestBIC = predict.regsubsets(fit.bests, newdata=test, id=which.min(summary.bests$bic))
RMSE.BIC = sqrt(mean( (yhat.bestBIC - y.star)^2 ))
RMSE.BIC
```

```
[1] 386.6866
```

---


```r
# Forward with AIC stopping rule
fit.null = lm(y ~ 1, train)
fit.fwdAIC = step(fit.null, scope=list(upper=fit.full), direction="forward", k=2, trace=0)
summary(fit.fwdAIC)$coeff
```

```
               Estimate  Std. Error    t value     Pr(&gt;|t|)
(Intercept)  60.7018707 79.02242248  0.7681601 4.438408e-01
CRBI          1.1715984  0.19517240  6.0028897 1.954214e-08
Walks         4.6382911  1.45923563  3.1785758 1.865637e-03
Division    152.0129546 53.72684433  2.8293669 5.435269e-03
Years       -23.2411908 11.01026538 -2.1108656 3.677610e-02
PutOuts       0.1420064  0.09941987  1.4283501 1.556850e-01
```

```r
yhat.fwdAIC = predict(fit.fwdAIC, newdata=test)
RMSE.fwdAIC = sqrt(mean( (yhat.fwdAIC - y.star)^2 ))
RMSE.fwdAIC
```

```
[1] 382.7557
```

---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

* See ISLR, 6.5.3

* Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error

* If the full data set is used to perform the best subset selection step, the cross-validation errors that we obtain will not be accurate estimates of the test error

* To choose among the models of different sizes using cross-validation, we perform best
subset selection within each iteration of the cross-validation

* Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation

---


```r
# Validation set
set.seed(1)
is.va = sample(c(T,F), n, replace = TRUE)
err.va = vector()
fit = regsubsets(y ~.,data=train[!is.va,],nvmax=p)
for (j in 1:p){
  yhat=predict(fit, train[is.va,], id=j)
  err.va[j] = mean( (train$y[is.va]-yhat)^2 )
}
yhat.bestVa = predict.regsubsets(fit.bests, newdata=test, id=which.min(err.va))
RMSE.bestVa = sqrt(mean( (yhat.bestVa - y.star)^2 ))
RMSE.bestVa
```

```
[1] 379.7315
```

---


```r
# K-fold Cross-Validation
set.seed(123)
folds = sample(1:K, n, replace =TRUE)
KCV = matrix(NA, K, p)
for (k in 1:K){
  fit_k = regsubsets(y ~.,data=train[folds!=k,],nvmax=p)
  for (j in 1:p){
    yhat_k=predict(fit_k, train[folds==k,], id=j)
    KCV[k,j]=mean( (train$y[folds==k]-yhat_k)^2 )
  }
}
```

---


```r
plot(1:p,apply(KCV,2,mean), type="b", ylab="CV error", xlab="k")
```

![](Lasso_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---


```r
yhat.bestCV = predict.regsubsets(fit.bests, newdata=test, id=4)
RMSE.bestCV = sqrt(mean( (yhat.bestCV - y.star)^2 ))
RMSE.bestCV
```

```
[1] 391.3101
```

---


```r
data.frame(
  fit = c("full","ridge","lasso","best bic", "best fwd aic", "best va", "best cv"),
  RMSE = c(RMSE.full,RMSE.ridge,RMSE.lasso, RMSE.BIC, RMSE.fwdAIC, RMSE.bestVa, RMSE.bestCV)
)
```

```
           fit     RMSE
1         full 410.6011
2        ridge 375.3843
3        lasso 405.0077
4     best bic 386.6866
5 best fwd aic 382.7557
6      best va 379.7315
7      best cv 391.3101
```



---

# ESL Chapter 7.10.2

Consider a scenario with `\(n = 50\)` samples in two equal-sized classes, and `\(p = 5000\)` quantitative
predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%. Try the following approach:

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use `\(K\)`-fold CV to estimate the test error of the final model



---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred &lt;- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.02
```

---

Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations. Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into `\(K\)` cross-validation folds at random
2. For each fold `\(k = 1,\ldots,K\)`

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold `\(k\)`
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold `\(k\)`
    
    c. Use the classifier to predict the class labels for the observations in fold `\(k\)`
    
---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred &lt;- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.56
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "R",
"ratio": "16:9",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
