<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Best subset regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Best subset regression
### Aldo Solari

---





# Variable selection

Let us revisit the linear model `\(\mathbf{y} = \mathbf{X} \boldsymbol{\beta}^0 + \boldsymbol{\varepsilon}\)` where `\(\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}\)` and `\(\mathbb{V}\mathrm{ar}(\boldsymbol{\varepsilon})= \sigma^2 \mathbf{I}\)`. In many modern dataset, there are reasons to believe there are many more variables present than are necessary to explain the response. Let `\(S\)` be the set 
`$$S = \{k \in \{1,\ldots,p\}: \beta_k^0 \neq 0\}$$`
and suppose `\(s = |S| \ll p\)`. The mean square prediction error of OLS is
$$
`\begin{aligned}
\frac{1}{n}\mathbb{E} \| \mathbf{X} \boldsymbol{\beta}^0 - \mathbf{X} \hat{\boldsymbol{\beta}}^{\mathrm{OLS}} \|^2_2 &amp;= \frac{1}{n}\mathbb{E}\{(\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}})^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X} (\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}}) \}\\
&amp;= \frac{1}{n}\mathbb{E}[\mathrm{tr}\{(\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}})  (\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}})^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X}\}]\\
&amp;=\frac{1}{n} \mathrm{tr}[\mathbb{E}\{ (\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}})  (\boldsymbol{\beta}^0 - \hat{\boldsymbol{\beta}}^{\mathrm{OLS}})^{\mathsf{T}} \mathbf{X}^{\mathsf{T}}\mathbf{X}\}]\\
&amp;=  \frac{1}{n} \mathrm{tr}[\mathbb{V}\mathrm{ar}(\hat{\boldsymbol{\beta}}^{\mathrm{OLS}})\mathbf{X}^{\mathsf{T}}\mathbf{X}] = \frac{p}{n}\sigma^2
\end{aligned}`
$$
If we could identify `\(S\)` and then fit a linear model using just these variables, we would obtain a mean square prediction error of `\(\sigma^2s/n\)`. Futhermore, it can be shown that parameter estimates from the reduced model are more accurate. The smaller model would also be easier to interpret. 

---

# Best subset selection

A natural approach to finding `\(S\)` is to consider all `\(2^p\)` possible regression models each involving regressing the response on a different sets of predictors `\(\mathbf{X}_M\)` where `\(M\)` is a subset of `\(\{1,\ldots,p\}\)`. 

We can then pick the "best" model using cross-validation (say).

For general design matrices, this involves an exhaustive search over all subsets, so this is not really feasible for `\(p&gt;50\)`. 

**Algorithm**

Set `\(B_0\)` as the null model (only intercept)

For `\(k=1,\ldots,p\)`:

1. Fit all `\({ p \choose k }\)` models that contain exactly `\(k\)` predictors

2. Pick the "best" among these `\({ p \choose k }\)` models, and call it `\(B_k\)`, where "best" is defined having the smallest residual sum of squares RSS = `\(n \mathrm{MSE}_{\mathrm{Tr}}\)`

Select a single best model from among `\(B_0,B_1,\ldots,B_p\)` using AIC, BIC, Cross-Validation, etc.


---

![](images/bestsubset.jpg)


---

# Backward stepwise selection

This can be seen as a greedy way of performing best subset regression. It will be sub-optimal to best subset selection but computationally efficient.


It is applicable only when `\(n&gt;p\)`.

__Algorithm__

Set `\(S_p\)` as the full model (all `\(p\)` predictors)

For `\(k=p,p-1,\ldots,1\)`:

1. Consider all `\(k\)` models that contain all but one of the predictors in `\(S_k\)`, for a total of `\(k-1\)` predictors

2. Choose the "best" among these `\(k\)` models and call it `\(S_{k-1}\)`, where "best" is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,\ldots,S_p\)` using AIC, BIC, cross-validation, etc.

---

# Forward stepwise selection

Greedy algorithm sub-optimal to best subset selection but computationally efficient.

Applicable also when `\(p&gt;n\)` to construct the sequence `\(S_0, S_1,\ldots,S_{n-1}\)`.

__Algorithm__

Set `\(S_0\)` as the null model (only intercept)

For `\(k=0,\ldots,\min(n-1,p-1)\)`:


1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor

2. Choose the "best" among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where "best" is defined having the smallest RSS

Select a single best model from among `\(S_0,S_1,S_2, \ldots\)` using AIC, BIC, cross-validation, etc.


---

# Forward with AIC-based stopping rule

Set `\(S_0\)` as the null model and `\(k=0\)`.

1. Consider all `\(p-k\)` models that augment the predictors in `\(S_k\)` with one additional predictor.

2. Choose the "best" among these `\(p-k\)` models and call it `\(S_{k+1}\)`, where "best" is defined having the smallest AIC.

3. If AIC( `\(S_{k+1}\)` ) `\(&lt;\)` AIC( `\(S_k\)` ), set `\(k=k+1\)` and go to 1., otherwise STOP

---


# Application to Hitters data


```r
rm(list=ls())
library(ISLR)
data(Hitters)
Hitters = Hitters[complete.cases(Hitters),]
Hitters[,"League"]=(Hitters[,"League"]=="A")*1
Hitters[,"Division"]=(Hitters[,"Division"]=="E")*1
Hitters[,"NewLeague"]=(Hitters[,"NewLeague"]=="A")*1
set.seed(123)
n = 163
train.id = sample(nrow(Hitters),n)
train = Hitters[train.id,]
names(train)[19] = "y" 
X = as.matrix(train[,-19])
y = train$y
p = ncol(X)
test = Hitters[-train.id,]
names(test)[19] = "y" 
X.star = as.matrix(test[,-19])
y.star = test$y
m = nrow(X.star)
```

---


```r
# Full model
fit.full = lm(y ~ ., train)
RMSE.full = sqrt(mean( (predict(fit.full, newdata=test) - y.star )^2 ))
RMSE.full
```

```
[1] 382.139
```

---



```r
# Best subset
library(leaps)
fit.bests &lt;- regsubsets(y~.,train, nvmax=p)
summary.bests&lt;-summary(fit.bests)
head(summary.bests$outmat, 10)
```

```
          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors NewLeague
1  ( 1 )  " "   " "  " "   " "  " " " "   " "   " "    " "   " "    "*"   " "  " "    " "    " "      " "     " "     " "    " "      
2  ( 1 )  " "   " "  " "   "*"  " " " "   " "   " "    " "   " "    " "   "*"  " "    " "    " "      " "     " "     " "    " "      
3  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   " "    " "   " "  " "    " "    " "      " "     " "     " "    " "      
4  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    " "      " "     " "     " "    " "      
5  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      " "     " "     " "    " "      
6  ( 1 )  " "   " "  " "   " "  " " "*"   " "   "*"    "*"   "*"    " "   " "  " "    " "    "*"      "*"     " "     " "    " "      
7  ( 1 )  " "   " "  " "   " "  " " "*"   "*"   "*"    "*"   "*"    " "   " "  " "    " "    "*"      "*"     " "     " "    " "      
8  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   " "    " "   "*"    "*"   " "  "*"    " "    "*"      "*"     " "     " "    " "      
9  ( 1 )  "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*"  "*"    " "    "*"      "*"     " "     " "    " "      
10  ( 1 ) "*"   "*"  " "   " "  " " "*"   " "   "*"    " "   " "    "*"   "*"  "*"    " "    "*"      "*"     " "     " "    "*"      
```

---


```r
# Best Cp
plot(summary.bests$cp, xlab="k", ylab="Cp", type="b")
```

![](Bestsubsets_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---


```r
# best BIC
plot(fit.bests, scale="bic")
```

![](Bestsubsets_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---


```r
# function predict for regsubsets
predict.regsubsets =function(object ,newdata ,id ,...){
 form=as.formula(object$call[[2]])
 mat=model.matrix(form, newdata)
 coefi =coef(object, id=id)
 xvars =names(coefi)
 mat[,xvars]%*%coefi
}

yhat.bestCp = predict.regsubsets(fit.bests, newdata=test, id=which.min(summary.bests$cp))
RMSE.Cp = sqrt(mean( (yhat.bestCp - y.star)^2 ))
RMSE.Cp
```

```
[1] 376.2164
```

```r
yhat.bestBIC = predict.regsubsets(fit.bests, newdata=test, id=which.min(summary.bests$bic))
RMSE.BIC = sqrt(mean( (yhat.bestBIC - y.star)^2 ))
RMSE.BIC
```

```
[1] 390.4112
```

---


```r
# Forward with AIC stopping rule
fit.null = lm(y ~ 1, train)
fit.fwdAIC = step(fit.null, scope=list(upper=fit.full), direction="forward", k=2, trace=0)
summary(fit.fwdAIC)$coeff
```

```
               Estimate Std. Error     t value    Pr(&gt;|t|)
(Intercept)  -1.6692807 79.8974398 -0.02089279 0.983358758
CRuns         1.6677992  0.6224828  2.67926958 0.008196715
Hits          5.7253370  2.3637976  2.42209272 0.016616109
CAtBat       -0.2518051  0.1381579 -1.82258980 0.070343783
PutOuts       0.2122898  0.0871604  2.43562174 0.016030585
CHits         0.5730286  0.5442989  1.05278298 0.294122234
Walks         6.4396812  1.9623209  3.28166562 0.001281879
Division    102.6035698 47.0037801  2.18287911 0.030589445
CHmRun        1.4013975  0.5740356  2.44130770 0.015790019
NewLeague   -83.2220435 47.2542908 -1.76115316 0.080236606
AtBat        -1.5947992  0.7309262 -2.18188808 0.030663972
CWalks       -0.7516641  0.3453419 -2.17657968 0.031065857
```

```r
yhat.fwdAIC = predict(fit.fwdAIC, newdata=test)
RMSE.fwdAIC = sqrt(mean( (yhat.fwdAIC - y.star)^2 ))
RMSE.fwdAIC
```

```
[1] 379.884
```

---

layout: false
class: inverse, middle, center

# Variable selection and cross-validation

---

# Variable selection and cross-validation

See ISLR, 6.5.3.

Since variable selection is part of the model building process,  cross-validation should account for the __variability of the selection__ when calculating estimates of the test error.

If the full data set is used to perform the best subset selection step, the cross-validation errors that we obtain will not be accurate estimates of the test error.

To choose among the models of different sizes using cross-validation, we perform best
subset selection within each iteration of the cross-validation.

Note that possibly different subsets of the "best" predictors are generated at each iteration of the cross-validation.

---


```r
# Validation set
set.seed(123)
is.va = sample(c(T,F), n, replace = TRUE)
err.va = vector()
fit = regsubsets(y ~.,data=train[!is.va,],nvmax=p)
for (j in 1:p){
  yhat=predict(fit, train[is.va,], id=j)
  err.va[j] = sqrt(mean( (train$y[is.va]-yhat)^2 ))
}
names(err.va) &lt;- 1:p
err.va
```

```
       1        2        3        4        5        6        7        8        9       10       11       12       13       14       15       16       17       18       19 
438.8491 398.3519 386.5275 404.7401 394.6362 393.6256 393.5128 391.8776 404.3857 411.1286 410.8402 406.1636 409.2453 409.9608 406.5914 407.4136 407.3910 407.4312 406.7116 
```

```r
yhat.bestVa = predict.regsubsets(fit.bests, newdata=test, id=which.min(err.va))
RMSE.bestVa = sqrt(mean( (yhat.bestVa - y.star)^2 ))
RMSE.bestVa
```

```
[1] 409.8903
```

---


```r
# K-fold Cross-Validation
set.seed(123)
K = 10
folds = sample(1:K, n, replace =TRUE)
KCV = matrix(NA, K, p)
for (k in 1:K){
  fit_k = regsubsets(y ~.,data=train[folds!=k,],nvmax=p)
  for (j in 1:p){
    yhat_k=predict(fit_k, train[folds==k,], id=j)
    KCV[k,j]=mean( (train$y[folds==k]-yhat_k)^2 )
  }
}
```

---


```r
plot(1:p,apply(KCV,2,mean), type="b", ylab="CV error", xlab="k")
```

![](Bestsubsets_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---


```r
yhat.bestCV = predict.regsubsets(fit.bests, newdata=test, id=which.min(apply(KCV,2,mean)))
RMSE.bestCV = sqrt(mean( (yhat.bestCV - y.star)^2 ))
RMSE.bestCV
```

```
[1] 376.2164
```

---


```r
data.frame(
  fit = c("full", "best cp", "best bic", "best fwd aic", "best va", "best cv"),
  RMSE = c(RMSE.full, RMSE.Cp, RMSE.BIC, RMSE.fwdAIC, RMSE.bestVa, RMSE.bestCV)
)
```

```
           fit     RMSE
1         full 382.1390
2      best cp 376.2164
3     best bic 390.4112
4 best fwd aic 379.8840
5      best va 409.8903
6      best cv 376.2164
```

---

# ESL Chapter 7.10.2

Consider a scenario with `\(n = 50\)` samples in two equal-sized classes, and `\(p = 5000\)` quantitative
predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%. Try the following approach:

1. choose the 100 predictors having highest correlation
with the class labels

2. use a 1-nearest neighbor classifier, based on just these 100 *selected* predictors. 

3. Use `\(K\)`-fold CV to estimate the test error of the final model



---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
# compute p correlations between each predictor and the response
cors = apply(X,2, function(x) cor(y,x))
# columns of the best 100 predictors
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
# best 100 predictors
Xbest = X[,colbest100]
# CV
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
# predict the held-out samples using k nearest neighbors
pred &lt;- knn(train = Xbest[ -out, ],test = Xbest[out, ], cl = y[-out], k = 1)
# % of misclassified samples
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.02
```

---

Leaving observations out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent
test set, since these predictors have already seen the left out observations. Here is the correct way to carry out cross-validation in this example:

1. Divide the observations into `\(K\)` cross-validation folds at random
2. For each fold `\(k = 1,\ldots,K\)`

    a. Find the best 100 predictors that have the largest (in absolute value) correlation with the class labels, using all of the observations except those in fold `\(k\)`
    
    b. Using just this subset of predictors, fit a 1-nearest neighbor classifier, using all of the observations except those in fold `\(k\)`
    
    c. Use the classifier to predict the class labels for the observations in fold `\(k\)`
    
---


```r
set.seed(123)
n = 50
p = 5000
y = c(rep(0,n/2),rep(1,n/2))
X = matrix(rnorm(p*n), ncol=p)
require(class)
K&lt;-5
set.seed(123)
folds &lt;- sample( rep(1:K,length=n) )
Err.CV = vector()
for (k in 1:K){
out = which(folds==k)
cors = apply(X[-out, ],2, function(x) cor(y[-out],x))
colbest100 = sort(-abs(cors), index.return=T)$ix[1:100]
Xbest = X[,colbest100]
pred &lt;- knn(train = Xbest[-out, ],
            test = Xbest[out, ],
            cl = y[-out], k = 1)
Err.CV[k] = mean( y[out] != pred)
}
mean(Err.CV)
```

```
[1] 0.56
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLanguage": "R",
"ratio": "16:9",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
