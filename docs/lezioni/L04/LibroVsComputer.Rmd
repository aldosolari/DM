---
title: "Data Mining"
subtitle: "Libro di testo vs computer"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      countIncrementalSlides: false
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F)
```

# Libro di testo vs computer

Nei libri di testo viene spesso presentata la soluzione ad un certo problema con una formula matematica. Ad esempio, si consideri la soluzione 
$$\hat{\beta} = (X^\mathsf{T} X)^{-1}Xy$$

Tuttavia la traduzione diretta di questo tipo di formule in codice non è sempre consigliabile perché ci sono molti aspetti problematici dei computer che semplicemente non sono rilevanti quando si scrivono le cose su carta

I potenziali problemi computazionali che possono emergere sono

1. **Overflow** Quando i numeri diventano troppo grandi, non possono essere rappresentati su un computer e quindi spesso vengono prodotte NA

2. **Underflow** Simile all'overflow, i numeri possono diventare troppo piccoli per essere rappresentati dai computer, provocando errori o avvisi o calcoli imprecisi

3. **Dipendenza quasi lineare** Il computer (che ha una precisone finita) può confondere una dipendenza quasi lineare per una dipendenza lineare

---

# La stima di $\beta$

Sia $\underset{n \times 1}{y}$ il vettore delle risposte, $\underset{n \times p}{X}$ la matrice del disegno e $\underset{p\times 1}{\beta}$ il vettore dei parametri in un modello lineare. Lo stimatore OLS (*Ordinary Least Squares*)
$$\hat{\beta} = (X^\mathsf{T} X)^{-1}Xy$$

Questa soluzione può essere tradotta in codice R come
```{r, eval=F}
betahat <- solve(t(X) %*% X) %*% t(X) %*% y
```

Tuttavia non è consigliabile calcolare il valore di $\hat{\beta}$ in questo modo.

La ragione principale è che il calcolo dell'inversa di $X^\mathsf{T} X$ è molto costoso dal punto di vista computazionale ed è un'operazione potenzialmente instabile su un computer quando c'è un'elevata multicollinearità tra i predittori.

Inoltre, per calcolare $\hat{\beta}$ non abbiamo bisogno dell'inversa di
$X^\mathsf{T} X$, quindi perché calcolarla?

---

# Le equazioni normali

Basta infatti considerare le equazioni normali
$$X^\mathsf{T} X \beta = X^\mathsf{T} y$$
e risolverle direttamente:
```{r, eval=F}
solve(crossprod(X), crossprod(X, y))
```

Questo approccio ha il vantaggio di essere più stabile numericamente e di essere molto più veloce

---

# microbenchmark

```{r}
# generiamo dei dati con n = 500 e p = 100
set.seed(123)
n <- 500; p <- 100
X <- matrix(rnorm(n*p), ncol=p)
y <- rnorm(n)

# confrontiamo i due approcci:
library(microbenchmark)
ms <- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, 
            solve(crossprod(X), crossprod(X, y)))
ms
```

---

```{r}
library(ggplot2)
autoplot(ms)
```


---

# Problemi di multicollinearità 

Per ottenere una situazione di multicollinearità, possiamo aggiungere una colonna a $X$ che è molto simile (ma non identica) alla prima colonna di $X$

```{r}
W <- cbind(X, X[, 1] + rnorm(n, sd = 1e-10))
```

L'approccio "diretto" fallisce quando c'è elevata multicollinearità

```{r, error=TRUE}
solve(crossprod(W), crossprod(W, y))
```

Il numero di condizionamento $\kappa$ (*condition number*) nel calcolo numerico rappresenta il grado di condizionamento di un problema. Se $\kappa$ è molto grande, siamo in presenza di un un problema mal condizionato (un problema dove le soluzioni sono molto sensibili a piccole perturbazioni dei dati iniziali)

```{r}
kappa(crossprod(W))
```

---

# solve 

Le equazioni normali 
$$X^\mathsf{T} X \beta = X^\mathsf{T} y$$
rappresentano un caso particolare di un generico sistema di equazioni
$$A x = b$$

La funzione `solve(A, b, ...)` risolve questo sistema di equazioni, dove $A$ è una matrice quadrata e $b$ può essere un vettore o una matrice.

Se $b$ non viene specificato, allora diventa la matrice identità $I$, quindi il problema si traduce in $Ax = I$, ovvero trovare l'inversa di $A$.

---

# La decomposizione di Cholesky

Poichè nel nostro caso la matrice $A= X^\mathsf{T}X$ è simmetrica, e se $X$ è a rango pieno, è anche definita positiva, allora possiamo considerare la decomposizione di Cholesky
$$A = L L^\mathsf{T}$$
dove $L$ è una matrice triangolare inferiore.

Con questa decomposizione possiamo scrivere 
\begin{align}
L L^\mathsf{T} x &= b \\
L z &= b
\end{align}
dove $z = L^\mathsf{T} x$ è la nuova incognita.

Quando nel generico sistema di equazioni $Ax = b$ la matrice $A$ è triangolare inferiore (superiore), si può applicare l'algoritmo di sostituzione in avanti, denominato *forwardsolve* (l'algoritmo di sostituzione in indietro, denominato *backsolve*).

---

# Algoritmo di backsolve

Si consideri la seguente matrice triangolare superiore
$$\left[\begin{array}{ccc}
l_{1,1}  & l_{1,2}  & l_{1,3} \\ 
0  & l_{2,2}  & l_{2,3} \\ 
0  & 0  & l_{3,3} \\ 
\end{array}\right] 
\left[\begin{array}{c}
z_{1}   \\ 
z_{2} \\ 
z_{3} \\ 
\end{array}\right] = \left[\begin{array}{c}
b_{1}   \\ 
b_{2} \\ 
b_{3} \\ 
\end{array}\right]$$

Dall'ultima riga (equazione) risulta
$$z_{3} = \frac{b_3}{l_{3,3}}$$

La seconda riga (equazione) coinvolge solamente $z_2$ e $z_3$, quindi 
$$z_2 = \frac{b_2}{l_{2,2}} - \frac{l_{2,3} z_3}{l_{2,2}}$$

Infine 
$$z_{1} = \frac{b_1}{l_{1,1}} - \frac{l_{1,2} z_2}{l_{1,1}} - \frac{l_{1,3} z_3}{l_{1,1}}$$

---

# Forwardsolve e backsolve

L'algoritmo di *forwardsolve* utilizza sostanzialmente la stessa tecnica per matrici triangolari inferiori. Per il nostro problema, possiamo scrivere
\begin{align}
X^\mathsf{T} X x &= X^\mathsf{T} y \\
L L^\mathsf{T} x &= b \\
L z &= b
\end{align}
dove $\underset{p \times p}{L}$ è una matrice triangolare inferiore
 
Prima si risolve $L z = b$ con
```{r, eval=F}
forwardsolve(L, b)
```

Poi si risolve $L^\mathsf{T}x  = z$ con 
```{r, eval=F}
backsolve(t(L), forwardsolve(L, b))
```

---

# ols_chol

```{r}
# Calcolare la stima OSL con la decomposizione di Cholesky
#
# Argomenti:
# X: la matrice del disegno
# y: il vettore risposta
#
# Ritorna:
# Il vettore hatbeta di lunghezza ncol(X).
ols_chol <-
function(X, y)
{
XtX <- crossprod(X)
Xty <- crossprod(X, y)
L <- t(chol(XtX))
betahat <- backsolve(t(L), forwardsolve(L, Xty))
betahat
}
```

---

```{r}
# generiamo dei dati con n = 1000 e p=4
n <- 1e4; p <-4
X <- matrix(rnorm(n*p), ncol=p)
beta = 1:4
epsilon <- rnorm(n)
y <- X %*% beta + epsilon 

# stimiamo beta
ols_chol(X,y)

# verifichiamo con lm
coef(lm(y ~ X - 1))
```

---

# La decomposizione QR

La funzione `lm` adotta un approccio diverso: utilizza la decomposizione QR, che non è così veloce, ma ha l'ulteriore vantaggio di essere in grado di rilevare e gestire automaticamente le colonne linearmente dipendenti.

La decomposizione QR (ridotta) prevede
$$\underset{n \times p}{X} = \underset{n \times p}{Q}\underset{p \times p}{R}$$
dove $Q$ è una matrice ortogonale tale che $Q^\mathsf{T} Q = I$, ed $R$ è una matrice triangolare superiore, quindi
$$
\begin{aligned}
X^\mathsf{T}X \beta= X^\mathsf{T} y\\
R^\mathsf{T} Q^\mathsf{T} Q R = R^\mathsf{T} Q^\mathsf{T} y\\
R^\mathsf{T} R = R^\mathsf{T} Q^\mathsf{T} y\\
 R =  Q^\mathsf{T} y\\
\end{aligned}
$$

Possiamo quindi risolvere il sistema con l'algoritmo *backsolve* senza dover calcolare $X^\mathsf{T} X$, che potrebbe risultare numericamente instabile.

---

# ols_qr

```{r}
# Calcolare le stime OLS con la decomposizione QR 
#
# Argomenti:
# X: la matrice del disegno
# y: il vettore risposta
#
# Ritorna:
# Il vettore hatbeta di lunghezza ncol(X).
ols_qr <-
function(X, y)
{
qr_obj <- qr(X)
Q <- qr.Q(qr_obj)
R <- qr.R(qr_obj)
Qty <- crossprod(Q, y)
betahat <- backsolve(R, Qty)
betahat
}
```

---

```{r}
# stimiamo beta
ols_qr(X,y)

# verifichiamo con lm
coef(lm(y ~ X - 1))
```

---

# Un semplice esempio

Si consideri il seguente esempio giocattolo con $n=p=2$. Sia 
$$X = \left[\begin{array}{cc}
10^9 & -1 \\
-1 & 10^{-5}\\
\end{array}\right] \quad \beta = \left[\begin{array}{cc}
1 \\
1\\
\end{array}\right]$$
e se definiamo $y=X\beta$, otteniamo
$$y = \left[\begin{array}{cc}
10^9 & -1 \\
-1 & 10^{-5}\\
\end{array}\right] \left[\begin{array}{cc}
1 \\
1\\
\end{array}\right] = \left[\begin{array}{cc}
10^9-1 \\
-0.99999\\
\end{array}\right]$$
Poichè $X$ è una matrice quadrata, possiamo risolvere $X\beta = y$ direttamente con il comando `solve`

---

```{r}
X <- matrix(c(10^9, -1, -1, 10^(-5)), 2, 2)
beta <- c(1,1)
y <- X %*% beta
# visto che in questo esempio X è una matrice quadrata, possiamo risolvere con
solve(X, y)
# il reciproco del numero di condizionamento di X
rcond(X)
```

Il valore è molto piccolo, ma non inferiore alla soglia di tolleranza del computer di $\approx 10^{-16}$
```{r}
.Machine$double.eps
```

---

```{r, error=T}
# se proviamo con le equazioni normali
solve( crossprod(X), crossprod(X, y) )
```
```{r}
rcond(crossprod(X))
```

Ora il valore è inferiore alla soglia di tolleranza $\approx 10^{-16}$, 

```{r}
solve(crossprod(X), crossprod(X, y), tol = 0)
```

La soluzione purtroppo è sbagliata. Se evitiamo di calcolare $X^\mathsf{T}X$, ad esempio con la decomposizione QR, la soluzione risulta corretta

```{r}
ols_qr(X, y)
```

---

```{r}
# generiamo dei dati collineari con n = 500 e p = 100
set.seed(123)
n <- 500; p <- 100
X <- matrix(rnorm(n*p), ncol=p)
y <- rnorm(n)
W <- cbind(X, X[, 1] + rnorm(n, sd = 1e-10))
# confronto tra i tre approcci
ms <- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y,
                    ols_chol(X, y),
                    ols_qr(X, y))
```

---

```{r}
autoplot(ms)
```


---

# lm

La funzione `lm` utilizza la decomposizione QR (programmata in C e richiamata dalla funzione).


Si noti che `lm` gestisce situazioni di colonne (computazionalmente) linearmente dipendenti
```{r}
qr(W)$rank
fit <- lm(y ~ W)
tail(coef(fit))
```
dove l'ultimo elemento di $\hat{\beta}$ è NA perché un coefficiente corrispondente all'ultima colonna di $W$ (la colonna linearmente dipendente). 



Sebbene la decomposizione QR gestisca senza messaggio di errore la matrice $X$ con colonne (computazionalmente o esattamente) linearmente dipendenti, paghiamo un prezzo in termini di velocità, che può essere ridotta utilizzando `lm.fit` e `.lm.fit` se siamo solo interessati alla stima di $\beta$. In alcune situazioni però potrebbe essere più efficente ricorrere ad un approccio alternativo

---

```{r}
autoplot(microbenchmark( lm(y~W), 
                         lm.fit(W,y), 
                         .lm.fit(W,y) 
                        ))
```





