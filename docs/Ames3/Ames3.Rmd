---
title: "Data Mining"
subtitle: "The Ames housing data - part III"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo=T, eval=T, message=F, warning=F, error=F, comment=NA)
```


---

layout: false
class: inverse, middle, center

# 5. Using XGBoost with Tidymodels

---

```{r}
library(tidyverse)
library(tidymodels)
library(modeldata)

data(ames)
ames <- ames %>% mutate(Sale_Price = log10(Sale_Price))

# speed up computation with parallel processing
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
```

---

# Step 1: Initial Data Split

```{r}
set.seed(123)
ames_split <- rsample::initial_split(ames, prop = 0.2, strata = Sale_Price)
```

---

# Step 2: Preprocessing

```{r}
preprocessing_recipe <- 
  recipes::recipe(Sale_Price ~ ., data = training(ames_split)) %>%
  # convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %>%
  # combine low frequency factor levels
  recipes::step_other(all_nominal(), threshold = 0.01) %>%
  # remove no variance predictors which provide no predictive information 
  recipes::step_nzv(all_nominal()) %>%
  prep()
```

---

# Step 3: Splitting for Cross Validation

```{r}
ames_cv_folds <- 
  recipes::bake(
    preprocessing_recipe, 
    new_data = training(ames_split)
  ) %>%  
  rsample::vfold_cv(v = 5)
```

---

# Step 4: XGBoost Model Specification

```{r}
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")
```

---

# Step 5: Grid Specification

```{r}
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )
```

---

```{r}
xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 10
  )
knitr::kable(head(xgboost_grid))
```

---

# Step 6: Define the Workflow

```{r}
xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(Sale_Price ~ .)
```

---

# Step 7: Tune the Model

```{r}
xgboost_tuned <- 
  tune::tune_grid(
  object = xgboost_wf,
  resamples = ames_cv_folds,
  grid = xgboost_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(verbose = TRUE)
)
```

---

```{r}
xgboost_tuned %>%
  tune::show_best(metric = "rmse") %>%
  knitr::kable()
```

---

```{r}
xgboost_best_params <- xgboost_tuned %>%
  tune::select_best("rmse")
knitr::kable(xgboost_best_params)

xgboost_model_final <- xgboost_model %>% 
  finalize_model(xgboost_best_params)
```


---

# Step 8: Evaluate Performance on Test Data

```{r}
train_processed <- bake(preprocessing_recipe,  new_data = training(ames_split))


train_prediction <- xgboost_model_final %>%
  # fit the model on all the training data
  fit(
    formula = Sale_Price ~ ., 
    data    = train_processed
  ) %>%
  # predict the sale prices for the training data
  predict(new_data = train_processed) %>%
  bind_cols(training(ames_split))
```

---


```{r}
xgboost_score_train <- 
  train_prediction %>%
  yardstick::metrics(Sale_Price, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
```

---

```{r}
test_processed  <- bake(preprocessing_recipe, new_data = testing(ames_split))
test_prediction <- xgboost_model_final %>%
  # fit the model on all the training data
  fit(
    formula = Sale_Price ~ ., 
    data    = train_processed
  ) %>%
  # use the training model fit to predict the test data
  predict(new_data = test_processed) %>%
  bind_cols(testing(ames_split))
# measure the accuracy of our model using `yardstick`
xgboost_score <- 
  test_prediction %>%
  yardstick::metrics(Sale_Price, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score)
```


---

```{r}
house_prediction_residual <- test_prediction %>%
  arrange(.pred) %>%
  mutate(residual_pct = (Sale_Price - .pred) / .pred) %>%
  select(.pred, residual_pct)

ggplot(house_prediction_residual, aes(x = .pred, y = residual_pct)) +
  geom_point() +
  xlab("Predicted Sale Price") +
  ylab("Residual (%)") +
  scale_x_continuous(labels = scales::dollar_format()) +
  scale_y_continuous(labels = scales::percent)
```





