---
title: "Data Mining"
subtitle: "Regressione nonparametrica"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```


# Metodi nonparametrici

* I metodi nonparametrici non fanno assunzioni specifiche a proposito della forma funzionale della $f$ (ad esempio, " $f$ è un polinomio "). Si lascia che i dati "parlino da soli"

* Vantaggio: evitando assunzioni sulla forma di $f$, permettiamo qualsiasi forma funzionale per $f$ (anche le più strane e irregolari)

* Svantaggio: poichè il problema della stima di $f$ non si riduce più al problema di stimare $p$ parametri (se $p \ll n$), risulta necessario avere tante osservazioni ( $n$ elevato )

---

# Il metodo dei $k$ vicini più vicini 

* Il metodo dei $k$ vicini più vicini (*k-nearest neighbors*, abbreviato k-NN) è un metodo molto semplice ma efficace. Considereremo questo approccio nel caso di un problema di regressione

* Supponiamo di voler prevedere la risposta $y^*_1$ in corrispondenza ad un certo punto $x^*_1$. Definiamo il "vicinato" (*neighbourhood*) di questo punto con $N_k(x^*_1)$: è l'insieme dei $k$ punti del training set  più "vicini" a $x^*_1$

* Dobbiamo quindi definire una distanza: si può ad esempio considerare la distanza Euclidea tra $x_i$ e $x^*_1$ 
$$\| x_i - x^*_1 \|_2=\sqrt{(x_i - x^*_1)^\mathsf{T}(x_i - x^*_1)} = \sqrt{\sum_{j=1}^{p}(x_{ij} - x^*_{1j})}$$ dove 
$\|\cdot \|_2$ indica la norma Euclidea

* La previsione è definita dalla media delle $k$ risposte $y_i$ del training set che appartengono al vicinato di $x^*_1$, i.e. le $x_i \in N_k(x^*_1)$
$$\hat{f}(x^*_1) = \frac{1}{k}\sum_{i \in N_k(x^*_1)} y_i$$

---

# Parametro di regolazione

* $k$ può assumere valori da $1$ a $n$, e rappresenta il **parametro di regolazione**  (*tuning parameter*) 

* Un $k$ piccolo corrisponde ad una stima più flessibile, vicerversa un $k$ grande ad una stima meno flessibile

* Il caso estremo $k=n$ corrisponde alla media delle risposte del training: $\hat{f}(x^*_1) = \bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$, mentre $k=1$ corrisponde a $\hat{f}(x^*_1) = y_i$ per un certo indice $i$ tale che $x_i$ è il punto più vicino a $x^*_1$

* Operativamente, poichè il metodo utilizza una distanza, spesso i valori dei predittori vengono standardizzati

---

```{r}
library(readr)
df <- read_table2("http://azzalini.stat.unipd.it/Book-DM/yesterday.dat")[-31,]
train <- data.frame(x=df$x, y=df$y.yesterday)
test <- data.frame(x=seq(min(train$x), max(train$x), length=100))
library(kknn)
my_k = 4
fit = kknn(y ~ x, train, test, distance = 2, kernel = "rectangular", k = my_k)
yhat = fit$fitted.values
```

---

```{r}
plot(y ~ x, train, main=paste("k = ", my_k))
lines(test$x, yhat, col=4, type="s")
```

---

# Errore di previsione

Nel Fixed-X setting, la distorsione risulta

$$
\begin{aligned}
\mathbb{E}[\hat{f}(x_i)] - f(x_i) &= \frac{1}{k}\sum_{i \in N_k(x_i)}\mathbb{E}[  y_{i}] - f(x_i)= \frac{1}{k}\sum_{i \in N_k(x_i)}f(x_i) - f(x_i)\\
\end{aligned}
$$
e la varianza risulta
$$
\begin{aligned}
\mathbb{V}\mathrm{ar}(\hat{f}(x_i)) &= \mathbb{E}\{(\hat{f}(x_i) - \mathbb{E}[\hat{f}(x_i)] )^2 \} = \frac{\sigma^2}{k}
\end{aligned}
$$
quindi l'errore di previsione risulta
$$\mathrm{ErrF} = \sigma^2 + \frac{1}{n}\sum_{i=1}^{n}(\frac{1}{k}\sum_{i \in N_k(x_i)}f(x_i) - f(x_i))^2 + \frac{n\sigma^2}{k}$$

Infine si può dimostrare che
$$\mathrm{OptF} = \frac{2}{n}\sum_{i=1}^{n}\mathbb{C}\mathrm{ov}(y_i,\hat{f}(x_i))= \frac{2\sigma^2}{k}$$

