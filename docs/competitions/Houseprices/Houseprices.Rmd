---
title: "House prices"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```

# Outline

* Description
* Data
* Evaluation
* Benchmark
* Timeline


---

# House Prices

* This competition uses the Ames IA housing data. There are 2930 properties in the data. The sale price was recorded along with 81 predictors, including:

    - Location (e.g. neighborhood) and lot information.
    - House components (garage, fireplace, pool, porch, etc.).
    - General assessments such as overall quality and condition.
    - Number of bedrooms, baths, and so on. 

* The training set has $n=1460$, the test set has $m=1470$.

* More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

* See also the Kaggle competition [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)

* Team: No

* Weight: 30%

---


# Data

The training data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers). Hereâ€™s a brief version of what you'll find in the data description file:

[Data documentation](http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/62.txt)

---

# Evaluation

* Submissions are evaluated on __Root Mean Squared Logarithmic Error__ (RMSLE) between the logarithm of the predicted value and the logarithm of the observed sales price

$$\mathrm{RMSLE}= \sqrt{\frac{1}{m}\sum_{i=1}^{m}\left[\log(y^*_i)- \log(\hat{y}_i)\right]^2}$$


* Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally

* During the competition, the leaderboard displays your partial score, which is the RMSLE for $m=735$ (random) houses of the test set. At the end of the contest, the leaderboard will display the final score, which is the RMSLE for the remaining $m=735$ houses of the test set. The final score will determine the final winner. This method prevents users from overfitting to the leaderboard

---

The __points__ will be calculated on the basis of the distribution of the competition' scores

```{r, echo=FALSE}
library(rvest)
library(dplyr)
url <- 'http://www.bee-viva.com/competitions/ames'
webpage <- read_html(url)

tab = webpage %>%
  html_nodes("table") %>%
  .[3] %>%
  html_table(fill = TRUE)

score_text = tab[[1]]$Punteggio
twodig = regmatches(score_text, gregexpr("[[:digit:]]+", score_text))
score = as.numeric(unlist(lapply(twodig, function(x) paste(x,collapse="."))))


plot(ecdf(score), verticals = T, pch=".", main="Partial score distribution", xlab="score", ylab="Cumulative distribution")
fdr = ecdf(score)
bm = 21.32
cat("Benchmark score", bm, "\n",
"Benchmark % of points:", round((1-fdr(bm)+.00001)*100,2), "%")
```



However, I reserve the right to regrade differently (meaning your % of points may be calculated differently)

---

# Benchmark


```{r, eval=F}
train <- read.csv("60.csv", stringsAsFactors=T)
test <- read.csv("61.csv", stringsAsFactors=F)

fit <- lm(log10(SalePrice) ~ Year.Built + Lot.Area + Gr.Liv.Area, data = train)
yhat = 10^predict(fit, newdata=test)

write.table(file="mySubmission.txt", yhat, row.names = FALSE, col.names = FALSE)
```

---

# Timeline

Entry: October 8

Final submission: November 16

The R code used to get the predictions __must be published online__ by November 17. 
Keep only the necessary code (no plots, numeric summaries etc. )





