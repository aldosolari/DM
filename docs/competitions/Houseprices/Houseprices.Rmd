---
title: "House prices"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=T, R.options=list(width=220))
```

# Outline

* Description
* Data
* Evaluation
* Benchmark
* Timeline


---

# House Prices

This competition uses the Ames IA housing data. There are 2930 properties in the data. The sale price was recorded along with 81 predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on. 

The training set has $n=1460$, the test set has $m=1470$.

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

See also the Kaggle competition [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)

---


# Data

The training data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers). Here’s a brief version of what you’ll find in the data description file.

```{r, echo=FALSE}
knitr::kable(head(ames[,rev(1:ncol(ames))]), format = 'html')
```

[Data documentation](http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/62.txt)

---

# Evaluation

* Submissions are evaluated on __Root-Mean-Squared-Error__ (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. 

$$\mathrm{RMSE}= \sqrt{\frac{1}{m}\sum_{i=1}^{m}\left[\log(y^*_i)- \log(\hat{y}_i)\right]^2}$$


* Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally

* During the competition, the leaderboard displays your partial score, which is the RMSE for $m=735$ (random) houses of the test set

* At the end of the contest, the leaderboard will display the final score, which is the RMSE for the remaining $m=735$ houses of the test set

* The final score will determine the final winner. This method prevents users from overfitting to the leaderboard

* The __points__ will be calculated on the basis of the distribution of the competition' scores

---

# Benchmark


```{r, eval=F}
train <- read.csv("60.csv", stringsAsFactors=T)
test <- read.csv("61.csv", stringsAsFactors=F)

fit <- lm(log10(SalePrice) ~ Year.Built + Lot.Area + Gr.Liv.Area, data = train)
yhat = 10^predict(fit, newdata=test)

write.table(file="mySubmission.txt", yhat, row.names = FALSE, col.names = FALSE)
```

---

# Timeline

Entry: October 8

Final submission: November 16

The R code used to get the predictions __must be published online__ by November 17. 
Keep only the necessary code (no plots, numeric summaries etc. )





