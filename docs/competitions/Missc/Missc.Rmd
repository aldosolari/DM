---
title: "Data Mining Competitions"
subtitle: "Netflix Data - Miss Congeniality"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true   
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F)
```

# Outline

* The Netflix Prize
* The problem
* Data
* Rules
* Evaluation
* Grading
* Benchmark
* Timeline


---

# The Netflix Prize

* The Netﬂix Prize was a competition to predict user ratings of movies

* Netﬂix provided ratings of 17770 movie titles by 480189 users, along with the date of each rating

* The task was to predict ratings for 282000 user-movie-date triples that are not in the training set; all the users and movies in this test set appear in the training set

* Netﬂix judged performance by root mean squared error on the test set and offered a $1,000,000 reward to the first team to improve the performance of their current system by more than 10%

* The prize was won in 2009. Details of the Netﬂix Prize are available at: [www.netflixprize.com](www.netflixprize.com)

---

# The problem

* Because the Netﬂix Prize involves a very large data set and a non-standard problem (you could be asked to predict for any movie), the class competition will simplify the problem considerably

* The training data provide the ratings of 10,000 users for 99 movies, along with the dates at which the ratings were made. 
The first 14 of these movies were rated by all users; the remaining 85 may have missing values

* The outcome is the rating that each user gave to a further movie, [Miss Congeniality](https://en.wikipedia.org/wiki/Miss_Congeniality_(film); you are also given the date that this rating was made

* The task is to predict the rating for this movie by a further 2931 users in the test set.

* As with the training set, all users in the test set rated the first 14 movies, while the remaining 85 have missing values. The test set provides the same information as the training set – the dates and ratings of these 99 movies along with the date of the rating for *Miss Congeniality*.

* As with the Netﬂix Prize, performance will be measured by root mean squared error (RMSE) on the test set.

---

# Data

The data for the competition are available as tab-delimited text files. Go to [beeviva](http://www.bee-viva.com/) to download

* train_ratings_all.dat: The ratings that the users in the training data set gave to each of the 99 movies.

* test_ratings_all.dat: Same info for the test set.

* train_dates_all.dat: The date at which each of the ratings above were made.

* test_dates_all.dat: Same info for the test set.

* train_y_rating.dat: The ratings that the users in the training set gave to *Miss Congeniality*

* train_y_date.dat: The dates at which the training set users rated *Miss Congeniality*

* test_y_date.dat: Same info for the test set.

* movie_titles.txt: Names and release dates for the 99 movies, given in the same order as the columns in the data above.

---

# Some notes


* Ratings are from 1 to 5. A value of 0 indicates a missing entry

* For convenience, dates are given as number of days from January 1, 1997

* Missing dates are labeled '0000’

---

# Rules

1. If you attended the classes, you may work in groups of up to three. Each student may be on one team only

2. You may not include information outside the data provided on the class web-site (no reverse engineering of Netflix data please)

3. The competition will be run through an automated system, *BeeViva*

4. The R code used to get the predictions __must be published online__ on GitHub by November 15, 2019

5. TBD

---

# Evaluation

* Submissions are evaluated by RMSE

* During the competition, the leaderboard displays your partial score, which is the RMSE for 1000 (random) observations of the test set

* At the end of the contest, the leaderboard will display the final score, which is the RMSE for all 2931 observations of the test set. The final score will determine the final winner. This method prevents users from overfitting to the leaderboard


---

# Grading

The % of __points__ will be calculated as
$$\min\Big(\frac{0.76-x}{0.015} \cdot 100,100\Big)$$
 where $x$ is your final RMSE score

The winning team, after giving a **brief but informative talk** in class on November 18, 2019 before the exam. The team may receive an additional point bonus in grade

However, I reserve the right to regrade differently (meaning your % of points may be calculated differently)

---

# Benchmark

TBD

---

# Some Points to Ponder

* Can we gain from treating the rankings as categorical? Ordinal?

* How should missing rankings be handled? Dummy variables?

* What would be a good distance measure for k-NN and related methods?

* Efficient k-NN approaches for this big data?

* How should dates be used? Can we use the release year of the movie?

---

# Timeline

Entry: October 21, 2019

Final submission:  November 15, 2019

The R code used to get the predictions __must be published online__ by November 17. 
Keep only the necessary code (no plots, numeric summaries etc. )

