---
title: "Ames house prices"
output: html_document
---

NAME: Aldo Solari 

BADGE: 2575

NICKNAME: solari.aldo 

TEAM: solari.aldo 

ROUND: 1st

### Summary

My strategy was

1. transform SalePrice target to log(x + 1) 
2. transform excessively skewed predictors with log(x + 1)
3. dummy encoding for categorical predictors. For any level of that was NA, set to zero
4. missing values in numeric predictors, impute mean of that predictor
5. lasso regression

### References

* Copy/paste from Kaggle's Kernel [Regularized Linear Models](https://www.kaggle.com/jimthompson/regularized-linear-models-in-r)

* Keep in mind that omitting a reference (online resources, articles, books, other students work, etc.) and/or full copy/paste of R code is forbidden and it implies a ZERO score

### Models used

* Lasso

### Non-standard R packages

* moments (to compute skewness)

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


### R code to reproduce the last submission:

```{r}
# get the required R packages
library(readr)
library(dplyr)
library(caret)
library(moments)
library(glmnet)

# import data
train <- read_csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/60.csv")
test <- read_csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/61.csv")

# combine train and test data for preprocessing
all_data <- rbind(select(train,MS.SubClass:Sale.Condition),
                  select(test,MS.SubClass:Sale.Condition))

# transform SalePrice target to log form
train$SalePrice <- log(train$SalePrice + 1)

# get data type for each feature
feature_classes <- sapply(names(all_data),function(x){class(all_data[[x]])})
numeric_feats <-names(feature_classes[feature_classes != "character"])

# determine skew for each numeric feature
skewed_feats <- sapply(numeric_feats,function(x){skewness(all_data[[x]],na.rm=TRUE)})

# keep only features that exceed a threshold for skewness
skewed_feats <- skewed_feats[skewed_feats > 0.75]

# transform excessively skewed features with log(x + 1)
for(x in names(skewed_feats)) {
  all_data[[x]] <- log(all_data[[x]] + 1)
}

# get names of categorical features
categorical_feats <- names(feature_classes[feature_classes == "character"])

# use caret dummyVars function for hot one encoding for categorical features
dummies <- dummyVars(~.,all_data[categorical_feats])
categorical_1_hot <- predict(dummies,all_data[categorical_feats])
categorical_1_hot[is.na(categorical_1_hot)] <- 0  #for any level that was NA, set to zero

# for any missing values in numeric features, impute mean of that feature
for (x in numeric_feats) {
  mean_value <- mean(train[[x]],na.rm = TRUE)
  all_data[[x]][is.na(all_data[[x]])] <- mean_value
}

# reconstruct all_data with pre-processed data
all_data <- cbind(all_data[numeric_feats],categorical_1_hot)

# create data for training and test
X_train <- all_data[1:nrow(train),]
X_test <- all_data[(nrow(train)+1):nrow(all_data),]
y <- train$SalePrice

# set up caret model training parameters
CARET.TRAIN.CTRL <- trainControl(method="repeatedcv",
                                 number=5,
                                 repeats=5,
                                 verboseIter=FALSE)

# train model
set.seed(123)  # for reproducibility
model_lasso <- train(x=X_train,y=y,
                     method="glmnet",
                     metric="RMSE",
                     maximize=FALSE,
                     trControl=CARET.TRAIN.CTRL,
                     tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                          lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),0.00075,0.0005,0.0001)))

# compute predictions
preds <- exp(predict(model_lasso,newdata=X_test)) - 1

# show first 6 predicted values
head(preds)
```



